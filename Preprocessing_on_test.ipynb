{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Best Combination of Preprocessing Techniques on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HeddaVik/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import validation_and_prediction as VP\n",
    "import tokenizing as TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "global_vectors=HL.get_global_vectors(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:\n",
    "In addition to the acutal corpus, some additional information is needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, nr_pos_tweets, nr_neg_tweets, total_training_tweets=HL.get_corpus(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the neural net\n",
    "At this stage, we want to use the simple neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets=[NN.basic_model_adam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables to apply all preprocessing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing vectors:\n",
    "corpuses=[]\n",
    "corpuses.append(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining names of corpuses: \n",
    "names=['original_corpus','SH_corpus','SHM_corpus','H_corpus','HK_corpus','PS_corpus','NS__corpus','OS_corpus','N_corpus','NM_corpus','ST_corpus','SP_corpus','E_corpus','SN_corpus','RS_corpus','EX_corpus','N-2_corpus','N-3_corpus','N-4_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining inputs to preprocessing function: \n",
    "inputs=[{'hashtag': True, 'segmentation_hash': True},\n",
    "        {'hashtag':True,'segmentation_hash': True,'hashtag_mention':True},\n",
    "        {'hearts':True},\n",
    "        {'hugs_and_kisses':True},\n",
    "        {'pos_smilies':True},\n",
    "        {'neg_smilies':True},\n",
    "        {'other_smilies':True},\n",
    "        {'numbers':True},\n",
    "        {'numbers':True,'number_mention':True},\n",
    "        {'stemming':True},\n",
    "        {'spelling':True},#Warning: When True, it takes app  2.5 h on test set. Recomended to always set to false \n",
    "        {'elongation':True},\n",
    "        {'set_to_not':True},\n",
    "        {'remove_signs':True},\n",
    "        {'exclamation':True}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying all preprocessing techniques to the original corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01f52c14f71e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mcorpuses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/tokenizing.py\u001b[0m in \u001b[0;36mpreprocess_corpus\u001b[0;34m(corpus, stemming, all_smilies, pos_smilies, neg_smilies, other_smilies, hugs_and_kisses, hearts, hashtag, hashtag_mention, numbers, number_mention, exclamation, set_to_not, segmentation_hash, spelling, elongation, remove_signs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msegmentation_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#seg = Segmenter(corpus=\"english\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegmenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"twitter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/anaconda/lib/python3.6/site-packages/ekphrasis/classes/segmenter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, max_split_length)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP2w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_compiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"camel_split\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/anaconda/lib/python3.6/site-packages/ekphrasis/classes/segmenter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, total, unk_func, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for input_ in inputs: \n",
    "        corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "        corpuses.append(corpus)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[2,3,4]\n",
    "for n in ns: \n",
    "    corpus=TO.creating_n_grams_corpus(n,full_corpus)\n",
    "    corpuses.append(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing all preprocessing techniques: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Train on 133332 samples, validate on 66668 samples\n",
      "Epoch 1/100\n",
      "133120/133332 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.7652Epoch 00001: val_loss improved from inf to 0.40993, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 4s 32us/step - loss: 0.4699 - acc: 0.7653 - val_loss: 0.4099 - val_acc: 0.8052\n",
      "Epoch 2/100\n",
      "131072/133332 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8072Epoch 00002: val_loss improved from 0.40993 to 0.39679, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 13us/step - loss: 0.4090 - acc: 0.8072 - val_loss: 0.3968 - val_acc: 0.8118\n",
      "Epoch 3/100\n",
      "129024/133332 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8157Epoch 00003: val_loss improved from 0.39679 to 0.39259, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 13us/step - loss: 0.3957 - acc: 0.8156 - val_loss: 0.3926 - val_acc: 0.8148\n",
      "Epoch 4/100\n",
      "129024/133332 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8202Epoch 00004: val_loss improved from 0.39259 to 0.38873, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 13us/step - loss: 0.3876 - acc: 0.8201 - val_loss: 0.3887 - val_acc: 0.8187\n",
      "Epoch 5/100\n",
      "133120/133332 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8241Epoch 00005: val_loss improved from 0.38873 to 0.38618, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 13us/step - loss: 0.3803 - acc: 0.8241 - val_loss: 0.3862 - val_acc: 0.8182\n",
      "Epoch 6/100\n",
      "133120/133332 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8266Epoch 00006: val_loss improved from 0.38618 to 0.38460, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 12us/step - loss: 0.3745 - acc: 0.8266 - val_loss: 0.3846 - val_acc: 0.8197\n",
      "Epoch 7/100\n",
      "132096/133332 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8298Epoch 00007: val_loss improved from 0.38460 to 0.38364, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 13us/step - loss: 0.3690 - acc: 0.8297 - val_loss: 0.3836 - val_acc: 0.8189\n",
      "Epoch 8/100\n",
      "131072/133332 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8328Epoch 00008: val_loss did not improve\n",
      "133332/133332 [==============================] - 2s 12us/step - loss: 0.3645 - acc: 0.8326 - val_loss: 0.3842 - val_acc: 0.8195\n",
      "Epoch 9/100\n",
      "128000/133332 [===========================>..] - ETA: 0s - loss: 0.3603 - acc: 0.8345Epoch 00009: val_loss improved from 0.38364 to 0.38302, saving model to best_neural_model_save.hdf5\n",
      "133332/133332 [==============================] - 2s 12us/step - loss: 0.3604 - acc: 0.8344 - val_loss: 0.3830 - val_acc: 0.8199\n",
      "Epoch 10/100\n",
      "131072/133332 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8370Epoch 00010: val_loss did not improve\n",
      "133332/133332 [==============================] - 2s 12us/step - loss: 0.3568 - acc: 0.8369 - val_loss: 0.3878 - val_acc: 0.8199\n",
      "Epoch 11/100\n",
      "130048/133332 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8395Epoch 00011: val_loss did not improve\n",
      "133332/133332 [==============================] - 2s 16us/step - loss: 0.3534 - acc: 0.8393 - val_loss: 0.3854 - val_acc: 0.8198\n",
      "Epoch 12/100\n",
      "130048/133332 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8410Epoch 00012: val_loss did not improve\n",
      "133332/133332 [==============================] - 2s 15us/step - loss: 0.3496 - acc: 0.8411 - val_loss: 0.3865 - val_acc: 0.8203\n",
      "Epoch 00012: early stopping\n",
      "66668/66668 [==============================] - 2s 35us/step\n",
      "Train on 133334 samples, validate on 66666 samples\n",
      "Epoch 1/100\n",
      "130048/133334 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.7664Epoch 00001: val_loss improved from inf to 0.42971, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 3s 22us/step - loss: 0.4645 - acc: 0.7672 - val_loss: 0.4297 - val_acc: 0.7939\n",
      "Epoch 2/100\n",
      "132096/133334 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8105Epoch 00002: val_loss improved from 0.42971 to 0.41527, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 12us/step - loss: 0.4027 - acc: 0.8105 - val_loss: 0.4153 - val_acc: 0.8035\n",
      "Epoch 3/100\n",
      "130048/133334 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8165Epoch 00003: val_loss improved from 0.41527 to 0.41077, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 12us/step - loss: 0.3901 - acc: 0.8165 - val_loss: 0.4108 - val_acc: 0.8040\n",
      "Epoch 4/100\n",
      "130048/133334 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8215Epoch 00004: val_loss improved from 0.41077 to 0.40619, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 12us/step - loss: 0.3817 - acc: 0.8214 - val_loss: 0.4062 - val_acc: 0.8084\n",
      "Epoch 5/100\n",
      "133120/133334 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8266Epoch 00005: val_loss improved from 0.40619 to 0.40319, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 11us/step - loss: 0.3749 - acc: 0.8266 - val_loss: 0.4032 - val_acc: 0.8081\n",
      "Epoch 6/100\n",
      "133120/133334 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8294Epoch 00006: val_loss improved from 0.40319 to 0.40289, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 3s 24us/step - loss: 0.3690 - acc: 0.8293 - val_loss: 0.4029 - val_acc: 0.8082\n",
      "Epoch 7/100\n",
      "129024/133334 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8332Epoch 00007: val_loss improved from 0.40289 to 0.40077, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 15us/step - loss: 0.3639 - acc: 0.8331 - val_loss: 0.4008 - val_acc: 0.8061\n",
      "Epoch 8/100\n",
      "133120/133334 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8353Epoch 00008: val_loss did not improve\n",
      "133334/133334 [==============================] - 2s 13us/step - loss: 0.3589 - acc: 0.8353 - val_loss: 0.4015 - val_acc: 0.8065\n",
      "Epoch 9/100\n",
      "130048/133334 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8371Epoch 00009: val_loss did not improve\n",
      "133334/133334 [==============================] - 2s 13us/step - loss: 0.3547 - acc: 0.8374 - val_loss: 0.4017 - val_acc: 0.8135\n",
      "Epoch 10/100\n",
      "131072/133334 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8399Epoch 00010: val_loss did not improve\n",
      "133334/133334 [==============================] - 2s 15us/step - loss: 0.3511 - acc: 0.8397 - val_loss: 0.4014 - val_acc: 0.8085\n",
      "Epoch 00010: early stopping\n",
      "66666/66666 [==============================] - 3s 44us/step\n",
      "Train on 133334 samples, validate on 66666 samples\n",
      "Epoch 1/100\n",
      "131072/133334 [============================>.] - ETA: 0s - loss: 0.4657 - acc: 0.7665Epoch 00001: val_loss improved from inf to 0.42746, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 3s 26us/step - loss: 0.4651 - acc: 0.7670 - val_loss: 0.4275 - val_acc: 0.7951\n",
      "Epoch 2/100\n",
      "133120/133334 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8116Epoch 00002: val_loss improved from 0.42746 to 0.41446, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 13us/step - loss: 0.4011 - acc: 0.8116 - val_loss: 0.4145 - val_acc: 0.8030\n",
      "Epoch 3/100\n",
      "131072/133334 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8189Epoch 00003: val_loss improved from 0.41446 to 0.40798, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 14us/step - loss: 0.3876 - acc: 0.8187 - val_loss: 0.4080 - val_acc: 0.8089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "133120/133334 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8236Epoch 00004: val_loss improved from 0.40798 to 0.40560, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 14us/step - loss: 0.3791 - acc: 0.8237 - val_loss: 0.4056 - val_acc: 0.8102\n",
      "Epoch 5/100\n",
      "128000/133334 [===========================>..] - ETA: 0s - loss: 0.3727 - acc: 0.8272Epoch 00005: val_loss improved from 0.40560 to 0.40470, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 2s 15us/step - loss: 0.3728 - acc: 0.8270 - val_loss: 0.4047 - val_acc: 0.8100\n",
      "Epoch 6/100\n",
      "132096/133334 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8300- ETA: 0s - loss: 0.3670 - Epoch 00006: val_loss improved from 0.40470 to 0.40184, saving model to best_neural_model_save.hdf5\n",
      "133334/133334 [==============================] - 3s 21us/step - loss: 0.3672 - acc: 0.8300 - val_loss: 0.4018 - val_acc: 0.8121\n",
      "Epoch 7/100\n",
      "133120/133334 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8335Epoch 00007: val_loss did not improve\n",
      "133334/133334 [==============================] - 4s 32us/step - loss: 0.3619 - acc: 0.8335 - val_loss: 0.4029 - val_acc: 0.8131\n",
      "Epoch 8/100\n",
      "132096/133334 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8352Epoch 00008: val_loss did not improve\n",
      "133334/133334 [==============================] - 2s 14us/step - loss: 0.3575 - acc: 0.8353 - val_loss: 0.4019 - val_acc: 0.8116\n",
      "Epoch 9/100\n",
      "130048/133334 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8378Epoch 00009: val_loss did not improve\n",
      "133334/133334 [==============================] - 2s 14us/step - loss: 0.3530 - acc: 0.8380 - val_loss: 0.4024 - val_acc: 0.8111\n",
      "Epoch 00009: early stopping\n",
      "66666/66666 [==============================] - 2s 37us/step\n",
      "Model:  basic_model_adam\n",
      "0.81% (+/- 0.01%)\n",
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'add' output (typecode 'O') could not be coerced to provided output parameter (typecode 'd') according to the casting rule ''same_kind''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/glove_module.py\u001b[0m in \u001b[0;36mbuildDocumentVector\u001b[0;34m(document, vec_dimention, word_embedding_model)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mword_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embedding_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_dimention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mdocument_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'so_sad' not in vocabulary\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2f2d6e4db953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpuses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_with_neural_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_nets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_training_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_pos_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/validation_and_prediction.py\u001b[0m in \u001b[0;36mclassify_with_neural_networks\u001b[0;34m(neural_nets_functions, global_vectors, processed_corpus, total_training_tweets, nr_pos_tweets, epochs, n_folds, patience)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets processed: %.0f  of total number of tweets: %.0f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDocumentVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mtrain_document_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/glove_module.py\u001b[0m in \u001b[0;36mbuildDocumentVector\u001b[0;34m(document, vec_dimention, word_embedding_model)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mword_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_word_vec_for_n_gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_dimention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mdocument_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'add' output (typecode 'O') could not be coerced to provided output parameter (typecode 'd') according to the casting rule ''same_kind''"
     ]
    }
   ],
   "source": [
    "accuracies=[]\n",
    "stds=[]\n",
    "\n",
    "for corpus in corpuses: \n",
    "    model_score=VP.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "    accuracies.append(model_score[0][0])\n",
    "    stds.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to determine which preprocessing techniques that improved the accuracy, and keep them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses_1=[]\n",
    "names_1=[]\n",
    "stds_1=[]\n",
    "acc_1=[]\n",
    "print('The original corpus gave accuracy of: ',accuracies[0], 'std:', stds[0],'\\n')\n",
    "for i in range(1,len(accuracies)):\n",
    "    if accuracies[i]>=accuracies[0]:\n",
    "        corpuses_1.append(corpuses[i])\n",
    "        names_1.append(names[i])\n",
    "        stds_1.append(stds[i])\n",
    "        acc_1.append(accuracies[i])\n",
    "        print('IMPROVED:  ',names[i],', score:',accuracies[i],'std:',stds[i])\n",
    "    else:\n",
    "        print('Not better:',names[i],', score:',accuracies[i],'std:',stds[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do not keep SH, as SHM is better. \n",
    "# We do not keep N as NM is better \n",
    "# We do not keep SP due to time  (158.26 min)\n",
    "# We do not keep techniques that imporved less than 0.05 percentage points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply all techniques that contributed positively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_={'hashtag': True, 'segmentation_hash': True, 'hashtag_mention':True,\n",
    "        'hearts':True,'hugs_and_kisses':True,'elongation':True, 'set_to_not':True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying the techniques that we have decided to keep:\n",
    "best_prepr_corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "best_corpus=HL.creating_n_grams_corpus(2,best_prepr_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validating:\n",
    "model_score=VP.classify_with_neural_networks(neural_nets, global_vectors, corpus, \n",
    "                                             total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy:',  model_score[0][0], 'std:',model_score[0][1],'\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mindfs= [0,2,3,5,10,20, 40, 60,100,140,200]\n",
    "maxdfs=[0.8] \n",
    "accuracies_stop=[]\n",
    "stds_stop=[]\n",
    "stop_lens=[]\n",
    "vocabs=[]\n",
    "\n",
    "\n",
    "for max_ in maxdfs:\n",
    "    for min_ in mindfs: \n",
    "        stopwords, vocab= TO.get_dynamic_stopwords(best_corpus, MinDf=min_, MaxDf=max_)\n",
    "        stopword_corpus=TO.remove_stopwords(best_corpus, stopwords)\n",
    "        model_score=VP.classify_with_neural_networks(neural_nets, global_vectors, stopword_corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "        accuracies_stop.append(model_score[0][0])\n",
    "        stds_stop.append(model_score[0][1])\n",
    "        stop_lens.append(len(stopwords))\n",
    "        vocabs.append(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(stop_lens)):\n",
    "    print('min', mindfs[i], 'accuracy', accuracies_stop[i],'std',stds_stop[i],  '\\n')\n",
    "for i in range(len(stop_lens)):\n",
    "    print('number of stop words', stop_lens[i], 'lenght of vocabolary', len(vocabs[i]),'sum for checking',stop_lens[i]+len(vocabs[i]),  '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the best combination with the simple and the complex neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords, vocab= TO.get_dynamic_stopwords(best_corpus, MinDf=5, MaxDf=0.8)\n",
    "final_corpus= TO.remove_stopwords(best_corpus, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_score=VP.classify_with_neural_networks(NN.basic_model_adam, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "\n",
    "print('Applying the best combination of preprocessing, using pre-trained global vectors with 200 dimensions, an cros-validatino accuracy of', model_score[0][0],'+-',model_score[0][1], 'was achieved using the simple neural net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_score=VP.classify_with_neural_networks(NN.complex_model, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "\n",
    "print('Applying the best combination of preprocessing, using pre-trained global vectors with 200 dimensions, an cros-validatino accuracy of', model_score[0][0],'+-',model_score[0][1], 'was achieved using the compelx neural net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
