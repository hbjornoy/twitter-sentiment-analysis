{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove with preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "ERROR:root:Line magic function `%lautoreload` not found.\n",
      "ERROR:root:Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "import tokenizing_ekphrasis as TE\n",
    "\n",
    "%lautoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files used to create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"data/gensim_global_vectors_200dim.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing unprocessed corpus on neural nets to find best neural net: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets = [NN.basic_model, NN.basic_model_adam, NN.wide_model, NN.deep_2_model, NN.deep_HB]\n",
    "\n",
    "GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, epochs=10, n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We decide to keep the deep_HB-model. \n",
    "\n",
    "We now test different combinations of preprocessing to see what has the best results with the chosen neural net model. \n",
    "\n",
    "Preprocessing we're testing: \n",
    "- N-grams \n",
    "- Word cluster\n",
    "- Stemming \n",
    "- Tweet feature creation\n",
    "\n",
    "Different big stuff\n",
    "- Sklearn TfidfVectorizer\n",
    "- ekphrasis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preperation for pipeline testing\n",
    "cluster_file=\"50mpaths2.txt\"\n",
    "cluster_dictionary=CL.create_dictionary(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_corpus = TE.tokenizing_ekphrasis(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets = [NN.deep_HB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTES ( USING 25 DIM DATA )\n",
    "\n",
    "- clustering decreases the acc by ish 10% each time. Always setting it to false\n",
    "\n",
    "COMMON BEST FOUND FACTORS EARLIER RUN: \n",
    "\n",
    "- Specialfeatures:False\n",
    "- Stem:True\n",
    "- N_gram:2\n",
    "\n",
    "BEST SO FAR:\n",
    "\n",
    "Ekphrasis:False\n",
    "N_gram:0\n",
    "Cluster:False\n",
    "Stem:True\n",
    "Special_features:False\n",
    "Model:  deep_HB\n",
    "85.11% (+/- 1.91%)\n",
    "\"\"\"\n",
    "\n",
    "def cross_validate_preprocessing(corpus):\n",
    "    \n",
    "    param_grid = ParameterGrid(param_grid = {\n",
    "    'ekphrasis': [False],\n",
    "    'n_gram': [0],\n",
    "    'cluster': [False],\n",
    "    'stem': [False],\n",
    "    'special_features': [False]\n",
    "    })\n",
    "        \n",
    "    for params in param_grid:\n",
    "        \n",
    "        ekphrasis = params['ekphrasis']\n",
    "        n_gram = params['n_gram']\n",
    "        cluster = params['cluster']\n",
    "        stem = params['stem']\n",
    "        special_features = params['special_features']\n",
    "        \n",
    "        run_pipeline(corpus, ekphrasis, cluster, stem, special_features, n_gram)\n",
    "\n",
    "        \n",
    "def run_pipeline(corpus, ekphrasis, cluster, stem, special_features, n_gram=0):\n",
    "        \n",
    "    if(ekphrasis):\n",
    "        corpus = TE_corpus\n",
    "        print(\"Corpus tokenized!\")\n",
    "        \n",
    "    if(special_features):\n",
    "        \n",
    "        corpus = TO.preprocess_corpus(corpus,\n",
    "                      stemming=stem, all_smilies=False, pos_smilies=True, \n",
    "                      neg_smilies=True, other_smilies=True, hugs_and_kisses=True,\n",
    "                      hearts=True, hashtag=True, hashtag_mention=True, \n",
    "                      numbers=True, number_mention=True, exclamation=True,\n",
    "                      set_to_not=False)\n",
    "        print(\"Special features integrated!\")\n",
    "        \n",
    "    if(n_gram!=0):\n",
    "        corpus = HL.creating_n_grams_cropus(n_gram, corpus)\n",
    "        print(\"N_grams made!\")\n",
    "\n",
    "    if(cluster):\n",
    "        corpus = CL.create_clusterized_corpus(corpus,cluster_dictionary)\n",
    "        print(\"Corpus Clusterized!\")\n",
    "\n",
    "    \n",
    "    print(\"\\nEkphrasis:{}\\n N_gram:{}\\nCluster:{}\\nStem:{}\\nSpecial_features:{}\".format( \n",
    "          ekphrasis, n_gram, cluster, stem, special_features))\n",
    "    GV.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To run the cross val thingy!\n",
    "cross_validate_preprocessing(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using ekphrasis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_corpus=TE.tokenizing_ekphrasis(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, TE_corpus, total_training_tweets, nr_pos_tweets, epochs=5, n_folds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using costumized tokenizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really doing anything, just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding negative and positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_neg_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=True, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_neg_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_all_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=True, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_all_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all hearts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_hearts=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=True,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_hearts, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har kommet på at dette ikke gir så mye mening mer... Hvis vi vil teste dette må vi vel sette hvert ord til et av ordene i clusteren, ikke bare cluster ID? Fordi cluster-ID gir ikke noe mening og dermed gir ikke word2vec vectoren mening.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_file=\"50mpaths2.txt\"\n",
    "cluster_dictionary=CL.create_dictionary(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusteded_corpus=CL.create_clusterized_corpus(best_corpus_so_far,cluster_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, clusteded_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=2\n",
    "n_grams_corpus=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=3\n",
    "n_grams_corpus3=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus3, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=4\n",
    "n_grams_corpus4=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus4, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot graf for n-grams. null poeng i å gjøre før kjøringer gir samme verdi hver gang.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=n_grams_corpus3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "        min_df=11, #5, # removing word that occure less then 10 times \n",
    "        max_df = 1.5, #0.6, # remove words that are too frequent ( more then 0.8 * number of tweets )\n",
    "        sublinear_tf=True, # scale the term frequency in logarithmic scale\n",
    "        use_idf =False #True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dette var åpenbart ikke måten å gjøre det på.. \n",
    "corpus_tf_idf = vectorizer.fit_transform(best_corpus_so_far) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, corpus_tf_idf, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lag plott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Kaggle submission\n",
    "\n",
    "Som før for å lage en keggle! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_TE_10.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.deep_HB, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets,kaggle_name, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
