{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove with preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/HeddaVik/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "import tokenizing_ekphrasis as TE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need the following to get consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "rn.seed(7)\n",
    "tf.set_random_seed(7)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files used to create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY NEED TO THIS THE FIRST TIME ONE IMPORTS THE PRETRAINED GLOVE\n",
    "# Creates a gensim_word2vec_file in the same folder\n",
    "#GV.create_gensim_word2vec_file(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining which neural nets to use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets = [NN.deep_HB]\n",
    "#neural_nets = [NN.basic_model, NN.basic_model_adam, NN.wide_model, NN.deep_2_model, NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing unprocessed corpus: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing again to see if we get the same results\n",
    "GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using ekphrasis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "TE_corpus=TE.tokenizing_ekphrasis(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_HB\n",
      "84.17% (+/- 1.49%)\n",
      "Negative sentiment: 83.69%  Positive sentiment: 84.64%\n",
      "Percentage of positive classifications (should be 50%ish): 50.4779762053\n",
      "Time taken:  1.7471990664800008 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, TE_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using costumized tokenizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really doing anything, just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding negative and positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_neg_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=True, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_neg_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_all_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=True, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_HB\n",
      "83.54% (+/- 1.33%)\n",
      "Negative sentiment: 79.93%  Positive sentiment: 87.16%\n",
      "Percentage of positive classifications (should be 50%ish): 53.6149979157\n",
      "Time taken:  1.8113451202710469 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_all_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all hearts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_hearts=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=True,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_hearts, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har kommet på at dette ikke gir så mye mening mer... Hvis vi vil teste dette må vi vel sette hvert ord til et av ordene i clusteren, ikke bare cluster ID? Fordi cluster-ID gir ikke noe mening og dermed gir ikke word2vec vectoren mening.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_file=\"50mpaths2.txt\"\n",
    "cluster_dictionary=CL.create_dictionary(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusteded_corpus=CL.create_clusterized_corpus(best_corpus_so_far,cluster_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_HB\n",
      "64.49% (+/- 0.26%)\n",
      "Negative sentiment: 55.19%  Positive sentiment: 73.80%\n",
      "Percentage of positive classifications (should be 50%ish): 59.3005004869\n",
      "Time taken:  2.121539612611135 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, clusteded_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=2\n",
    "n_grams_corpus=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_HB\n",
      "84.22% (+/- 1.31%)\n",
      "Negative sentiment: 83.21%  Positive sentiment: 85.23%\n",
      "Percentage of positive classifications (should be 50%ish): 51.0104802854\n",
      "Time taken:  1.880251367886861 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=3\n",
    "n_grams_corpus3=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_HB\n",
      "84.17% (+/- 1.31%)\n",
      "Negative sentiment: 82.74%  Positive sentiment: 85.59%\n",
      "Percentage of positive classifications (should be 50%ish): 51.4260025903\n",
      "Time taken:  1.9127514322598775 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus3, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=4\n",
    "n_grams_corpus4=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_HB\n",
      "83.99% (+/- 1.36%)\n",
      "Negative sentiment: 81.79%  Positive sentiment: 86.18%\n",
      "Percentage of positive classifications (should be 50%ish): 52.1959893505\n",
      "Time taken:  1.883493419488271 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus4, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot graf for n-grams. null poeng i å gjøre før kjøringer gir samme verdi hver gang.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=n_grams_corpus3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "        min_df=11, #5, # removing word that occure less then 10 times \n",
    "        max_df = 1.5, #0.6, # remove words that are too frequent ( more then 0.8 * number of tweets )\n",
    "        sublinear_tf=True, # scale the term frequency in logarithmic scale\n",
    "        use_idf =False #True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tf_idf = vectorizer.transform(best_corpus_so_far) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 194)\t0.203630843509\n",
      "  (0, 487)\t0.203630843509\n",
      "  (0, 1055)\t0.203630843509\n",
      "  (0, 1515)\t0.203630843509\n",
      "  (0, 3056)\t0.203630843509\n",
      "  (0, 3786)\t0.203630843509\n",
      "  (0, 4141)\t0.203630843509\n",
      "  (0, 4439)\t0.254188975973\n",
      "  (0, 4682)\t0.203630843509\n",
      "  (0, 5276)\t0.254188975973\n",
      "  (0, 5418)\t0.203630843509\n",
      "  (0, 6069)\t0.203630843509\n",
      "  (0, 6131)\t0.203630843509\n",
      "  (0, 6455)\t0.203630843509\n",
      "  (0, 6672)\t0.203630843509\n",
      "  (0, 6712)\t0.203630843509\n",
      "  (0, 6828)\t0.203630843509\n",
      "  (0, 6863)\t0.203630843509\n",
      "  (0, 7851)\t0.203630843509\n",
      "  (0, 9704)\t0.203630843509\n",
      "  (0, 10315)\t0.203630843509\n",
      "  (0, 10729)\t0.203630843509\n",
      "  (0, 10963)\t0.203630843509\n",
      "  (1, 1012)\t0.242951161823\n",
      "  (1, 2406)\t0.242951161823\n",
      "  :\t:\n",
      "  (209997, 10067)\t0.218217890236\n",
      "  (209997, 10177)\t0.218217890236\n",
      "  (209997, 10305)\t0.218217890236\n",
      "  (209997, 10748)\t0.218217890236\n",
      "  (209997, 10774)\t0.218217890236\n",
      "  (209998, 1936)\t0.408248290464\n",
      "  (209998, 3945)\t0.408248290464\n",
      "  (209998, 4454)\t0.408248290464\n",
      "  (209998, 4885)\t0.408248290464\n",
      "  (209998, 6712)\t0.408248290464\n",
      "  (209998, 10315)\t0.408248290464\n",
      "  (209999, 478)\t0.267261241912\n",
      "  (209999, 660)\t0.267261241912\n",
      "  (209999, 766)\t0.267261241912\n",
      "  (209999, 2781)\t0.267261241912\n",
      "  (209999, 3909)\t0.267261241912\n",
      "  (209999, 4062)\t0.267261241912\n",
      "  (209999, 4885)\t0.267261241912\n",
      "  (209999, 5560)\t0.267261241912\n",
      "  (209999, 5931)\t0.267261241912\n",
      "  (209999, 6712)\t0.267261241912\n",
      "  (209999, 7615)\t0.267261241912\n",
      "  (209999, 8015)\t0.267261241912\n",
      "  (209999, 9675)\t0.267261241912\n",
      "  (209999, 10305)\t0.267261241912\n"
     ]
    }
   ],
   "source": [
    "print(corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "split not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-bd6e11e3eb3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_with_neural_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_nets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_tf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_training_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_pos_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/glove_module.py\u001b[0m in \u001b[0;36mclassify_with_neural_networks\u001b[0;34m(neural_nets_functions, global_vectors, processed_corpus, total_training_tweets, nr_pos_tweets)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Build a vector of all the words in a tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mtrain_document_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuildWordVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mtrain_document_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_document_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/glove_module.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Build a vector of all the words in a tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mtrain_document_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuildWordVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mtrain_document_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_document_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/EPFL Machine learning/CD-433-Project-2/glove_module.py\u001b[0m in \u001b[0;36mbuildWordVector\u001b[0;34m(tokens, size, model)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HeddaVik/anaconda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: split not found"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, corpus_tf_idf, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lag plott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_TE_n2.csv\"\n",
    "final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.deep_HB, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets,kaggle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
