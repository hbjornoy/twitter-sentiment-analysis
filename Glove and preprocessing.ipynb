{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove with preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "import tokenizing_ekphrasis as TE\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files used to create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1487730303.511215\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"data/gensim_global_vectors_200dim.txt\")\n",
    "print(\"Time: \", (time.time() - start) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1487730299.4267645\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]\n",
    "\n",
    "\n",
    "print(\"Time: \", (time.time() - start) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing unprocessed corpus on neural nets to find best neural net: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets = [NN.basic_model, NN.basic_model_adam, NN.wide_model, NN.deep_2_model, NN.deep_HB]\n",
    "\n",
    "GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, epochs=10, n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We decide to keep the deep_HB-model. \n",
    "\n",
    "We now test different combinations of preprocessing to see what has the best results with the chosen neural net model. \n",
    "\n",
    "Preprocessing we're testing: \n",
    "- N-grams \n",
    "- Word cluster\n",
    "- Stemming \n",
    "- Tweet feature creation\n",
    "\n",
    "Different big stuff\n",
    "- Sklearn TfidfVectorizer\n",
    "- ekphrasis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preperation for pipeline testing\n",
    "cluster_file=\"50mpaths2.txt\"\n",
    "cluster_dictionary=CL.create_dictionary(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_corpus = TE.tokenizing_ekphrasis(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets = [NN.deep_HB_dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTES ( USING 25 DIM DATA )\n",
    "\n",
    "- clustering decreases the acc by ish 10% each time. Always setting it to false\n",
    "\n",
    "COMMON BEST FOUND FACTORS EARLIER RUN: \n",
    "- Specialfeatures:False\n",
    "- Stem:True\n",
    "- N_gram:2\n",
    "\n",
    "BEST SO FAR:\n",
    "\n",
    "Ekphrasis:False\n",
    "N_gram:0\n",
    "Cluster:False\n",
    "Stem:True\n",
    "Special_features:False\n",
    "Model:  deep_HB\n",
    "85.11% (+/- 1.91%)\n",
    "\"\"\"\n",
    "\n",
    "def cross_validate_preprocessing(corpus):\n",
    "    \n",
    "    param_grid = ParameterGrid(param_grid = {\n",
    "    'ekphrasis': [False],\n",
    "    'n_gram': [0],\n",
    "    'cluster': [False],\n",
    "    'stem': [False],\n",
    "    'special_features': [False]\n",
    "    })\n",
    "        \n",
    "    for params in param_grid:\n",
    "        \n",
    "        ekphrasis = params['ekphrasis']\n",
    "        n_gram = params['n_gram']\n",
    "        cluster = params['cluster']\n",
    "        stem = params['stem']\n",
    "        special_features = params['special_features']\n",
    "        \n",
    "        model_scores = run_pipeline(corpus, ekphrasis, cluster, stem, special_features, n_gram)\n",
    "        \n",
    "def run_pipeline(corpus, ekphrasis, cluster, stem, special_features, n_gram=0):\n",
    "        \n",
    "    if(ekphrasis):\n",
    "        corpus = TE_corpus\n",
    "        print(\"Corpus tokenized!\")\n",
    "        \n",
    "    if(special_features):\n",
    "        \n",
    "        corpus = TO.preprocess_corpus(corpus,\n",
    "                      stemming=stem, all_smilies=False, pos_smilies=True, \n",
    "                      neg_smilies=True, other_smilies=True, hugs_and_kisses=True,\n",
    "                      hearts=True, hashtag=True, hashtag_mention=True, \n",
    "                      numbers=True, number_mention=True, exclamation=True,\n",
    "                      set_to_not=False)\n",
    "        print(\"Special features integrated!\")\n",
    "        \n",
    "    if(n_gram!=0):\n",
    "        corpus = HL.creating_n_grams_cropus(n_gram, corpus)\n",
    "        print(\"N_grams made!\")\n",
    "\n",
    "    if(cluster):\n",
    "        corpus = CL.create_clusterized_corpus(corpus,cluster_dictionary)\n",
    "        print(\"Corpus Clusterized!\")\n",
    "    \n",
    "    print(\"\\nEkphrasis:{}\\n N_gram:{}\\nCluster:{}\\nStem:{}\\nSpecial_features:{}\".format( \n",
    "          ekphrasis, n_gram, cluster, stem, special_features))\n",
    "    \n",
    "    model_scores = GV.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets, epochs=10, n_folds=5)\n",
    "    return model_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To run the cross val thingy!\n",
    "cross_validate_preprocessing(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for n_gram in [10,20,30,40,50]:\n",
    "#    score = run_pipeline(full_corpus, False, False, False, False, n_gram=n_gram)[0]\n",
    "#    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Everything below is every \"feature\" run by itself "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using ekphrasis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "TE_corpus=TE.tokenizing_ekphrasis(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 7s 72us/step - loss: 0.6070 - acc: 0.6578\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.4803 - acc: 0.7908\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.4507 - acc: 0.8009\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.4360 - acc: 0.8056\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4243 - acc: 0.8096\n",
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4217 - acc: 0.8091\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4152 - acc: 0.8116\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4123 - acc: 0.8128\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 4s 39us/step - loss: 0.4066 - acc: 0.8150\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.4066 - acc: 0.8152\n",
      "Model:  deep_HB_dropout\n",
      "81.86% (+/- 0.03%)\n",
      "Negative sentiment: 75.71%  Positive sentiment: 88.02%\n",
      "Percentage of positive classifications (should be 50%ish): 56.155\n",
      "Time taken:  1.0371862967809042 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[81.86099999999999]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, TE_corpus, total_training_tweets, nr_pos_tweets, epochs=5, n_folds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 8s 77us/step - loss: 0.6070 - acc: 0.6578\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 4s 42us/step - loss: 0.4803 - acc: 0.7908\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 4s 39us/step - loss: 0.4507 - acc: 0.8009\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 4s 39us/step - loss: 0.4360 - acc: 0.8056\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4243 - acc: 0.8096\n",
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4217 - acc: 0.8091\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 4s 40us/step - loss: 0.4152 - acc: 0.8116\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.4123 - acc: 0.8128\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.4066 - acc: 0.8150\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 5s 54us/step - loss: 0.4066 - acc: 0.8152\n",
      "Model:  deep_HB_dropout\n",
      "81.86% (+/- 0.03%)\n",
      "Negative sentiment: 75.71%  Positive sentiment: 88.02%\n",
      "Percentage of positive classifications (should be 50%ish): 56.155\n",
      "Time taken:  1.057654583454132 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[81.86099999999999]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, TE_corpus, total_training_tweets, nr_pos_tweets, epochs=5, n_folds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using costumized tokenizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really doing anything, just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding negative and positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_neg_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=True, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_neg_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_all_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=True, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_all_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all hearts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_hearts=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=True,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_hearts, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har kommet på at dette ikke gir så mye mening mer... Hvis vi vil teste dette må vi vel sette hvert ord til et av ordene i clusteren, ikke bare cluster ID? Fordi cluster-ID gir ikke noe mening og dermed gir ikke word2vec vectoren mening.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_file=\"50mpaths2.txt\"\n",
    "cluster_dictionary=CL.create_dictionary(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusteded_corpus=CL.create_clusterized_corpus(best_corpus_so_far,cluster_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, clusteded_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=2\n",
    "n_grams_corpus=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=3\n",
    "n_grams_corpus3=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus3, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=4\n",
    "n_grams_corpus4=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus4, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot graf for n-grams. null poeng i å gjøre før kjøringer gir samme verdi hver gang.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Kaggle submission\n",
    "\n",
    "Som før for å lage en keggle! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200000/200000 [==============================] - 9s 44us/step - loss: 0.5302 - acc: 0.7283\n",
      "Epoch 2/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4433 - acc: 0.7978\n",
      "Epoch 3/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4287 - acc: 0.8027\n",
      "Epoch 4/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4213 - acc: 0.8059\n",
      "Epoch 5/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4172 - acc: 0.8084\n",
      "Epoch 6/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4141 - acc: 0.8087\n",
      "Epoch 7/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4097 - acc: 0.8120\n",
      "Epoch 8/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4077 - acc: 0.8121\n",
      "Epoch 9/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4057 - acc: 0.8133\n",
      "Epoch 10/20\n",
      "200000/200000 [==============================] - 6s 32us/step - loss: 0.4038 - acc: 0.8140\n",
      "Epoch 11/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.4001 - acc: 0.8157\n",
      "Epoch 12/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3999 - acc: 0.8165\n",
      "Epoch 13/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3975 - acc: 0.8179\n",
      "Epoch 14/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3965 - acc: 0.8181\n",
      "Epoch 15/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3952 - acc: 0.8184\n",
      "Epoch 16/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3945 - acc: 0.8199\n",
      "Epoch 17/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3938 - acc: 0.8194\n",
      "Epoch 18/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3928 - acc: 0.8202\n",
      "Epoch 19/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3911 - acc: 0.8206\n",
      "Epoch 20/20\n",
      "200000/200000 [==============================] - 6s 31us/step - loss: 0.3910 - acc: 0.8218\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "kaggle_name=\"keggle_glove_TE_dropout.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "\n",
    "delivery_corpus = full_corpus\n",
    "\n",
    "# INSERT PREPROSESSING\n",
    "\n",
    "pred= GV.get_prediction(NN.deep_HB_dropout, global_vectors, delivery_corpus, total_training_tweets, nr_pos_tweets,kaggle_name, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970\n"
     ]
    }
   ],
   "source": [
    "print(sum(pred))\n",
    "\n",
    "#WTF is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
