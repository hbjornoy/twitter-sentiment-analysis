{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove with preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "import tokenizing_ekphrasis as TE\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files used to create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "global_vectors=GV.make_glove(\"data/gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_100dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_200dim.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining which neural nets to use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets = [NN.basic_model]\n",
    "#neural_nets = [NN.basic_model, NN.basic_model_adam, NN.wide_model, NN.deep_2_model, NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing unprocessed corpus: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  basic_model\n",
      "73.42% (+/- 0.43%)\n",
      "Negative sentiment: 70.58%  Positive sentiment: 76.25%\n",
      "Percentage of positive classifications (should be 50%ish): 52.8345\n",
      "Time taken:  0.17774945100148518 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  basic_model\n",
      "73.42% (+/- 0.43%)\n",
      "Negative sentiment: 70.58%  Positive sentiment: 76.25%\n",
      "Percentage of positive classifications (should be 50%ish): 52.8345\n",
      "Time taken:  0.18556700944900512 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using ekphrasis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_corpus=TE.tokenizing_ekphrasis(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, TE_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using costumized tokenizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really doing anything, just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding negative and positive smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_pos_neg_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=True, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_pos_neg_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_all_smile=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=True, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_all_smile, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all hearts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_hearts=TO.preprocess_corpus(TE_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=True,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, tokenized_corpus_hearts, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har kommet på at dette ikke gir så mye mening mer... Hvis vi vil teste dette må vi vel sette hvert ord til et av ordene i clusteren, ikke bare cluster ID? Fordi cluster-ID gir ikke noe mening og dermed gir ikke word2vec vectoren mening.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_file=\"50mpaths2.txt\"\n",
    "cluster_dictionary=CL.create_dictionary(cluster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusteded_corpus=CL.create_clusterized_corpus(best_corpus_so_far,cluster_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, clusteded_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=TE_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=2\n",
    "n_grams_corpus=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=3\n",
    "n_grams_corpus3=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus3, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=4\n",
    "n_grams_corpus4=HL.creating_n_grams_cropus(n_grams,best_corpus_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, n_grams_corpus4, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot graf for n-grams. null poeng i å gjøre før kjøringer gir samme verdi hver gang.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus_so_far=n_grams_corpus3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "        min_df=11, #5, # removing word that occure less then 10 times \n",
    "        max_df = 1.5, #0.6, # remove words that are too frequent ( more then 0.8 * number of tweets )\n",
    "        sublinear_tf=True, # scale the term frequency in logarithmic scale\n",
    "        use_idf =False #True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dette var åpenbart ikke måten å gjøre det på.. \n",
    "corpus_tf_idf = vectorizer.transform(best_corpus_so_far) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(neural_nets, global_vectors, corpus_tf_idf, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lag plott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_TE_n2.csv\"\n",
    "final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.deep_HB, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets,kaggle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
