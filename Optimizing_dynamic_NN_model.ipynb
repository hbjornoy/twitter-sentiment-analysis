{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing dynamic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = \"gensim_data_folder\"\n",
    "DATA_25DIM = DATA_FOLDER + \"/gensim_glove_vectors_25dim.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/gensim_glove_vectors_50dim.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/gensim_glove_vectors_100dim.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/gensim_glove_vectors_200dim.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file Helpers know contains two functions that can pickle BIG files for MACOS\n",
    "\n",
    "#Example of use:\n",
    "\n",
    "file_path = \"testing_pickling_for_BIG_obj.pkl\"\n",
    "pickle_dump(train_document_vecs, file_path)\n",
    "loaded_pickle = pickle_load(file_path)\n",
    "\n",
    "print(type(train_document_vecs))\n",
    "print(type(loaded_pickle))\n",
    "print(np.all(train_document_vecs == loaded_pickle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO-list:\n",
    "    - make dynamic work on exampledata\n",
    "    - be able to save and load weigths\n",
    "    - alter a model instead of creating new\n",
    "    -check out weigth decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TRAININGSET ready for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Choose the corpus\n",
    "processed_corpus = full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::] \n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "train_document_vecs = np.concatenate(vectors)\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make FULLSET ready for Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_corpus = pickle.load( open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )\n",
    "print(len(processed_corpus))\n",
    "\n",
    "nr_pos_tweets = 1250000\n",
    "nr_neg_tweets = 1250000\n",
    "total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Choose the corpus\n",
    "#processed_corpus = processed_full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::]\n",
    "del processed_corpus\n",
    "\n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "del global_vectors\n",
    "del doc\n",
    "print(\"done with making the trainvectors\")\n",
    "\n",
    "train_document_vecz = np.concatenate(vectors)\n",
    "del vectors\n",
    "print(\"done with concatenating the trainvectors\")\n",
    "\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecz)\n",
    "del train_document_vecz\n",
    "print(\"done with scaling the trainvectors\")\n",
    "\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "print(\"done with creating the labels\")\n",
    "\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maybe store trainvectors?\n",
    "pickle.dump([train_document_vecs, labels], open( \"FULL_train_document_vecs_and_labels.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run this this badboy to save time and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_trainvecs = \"AWESOME_train_document_vecs.pkl\"\n",
    "train_document_vecs = HL.pickle_load(file_path_trainvecs)\n",
    "print(train_document_vecs.shape)\n",
    "\n",
    "file_path_labels = \"AWESOME_labels.pkl\"\n",
    "labels = HL.pickle_load(file_path_labels)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a perfect neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = NN.dynamic_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "width = 500\n",
    "depth = 2\n",
    "epochs = 60\n",
    "n_folds = 2\n",
    "split = 0.9\n",
    "dropout_rate=0.3\n",
    "funnel=0.3\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = dd(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu', funnel=funnel)\n",
    "print(model.summary())\n",
    "\n",
    "final_model, cv_scores, histories = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# investigating range of weights in model\n",
    "weights = []\n",
    "for layer in final_model.layers:\n",
    "    w = layer.get_weights()\n",
    "    weights.append(w)\n",
    "    #print(\"mean:\", np.mean(w))\n",
    "    #print(\"std:\", np.std(w))\n",
    "    #print(\"max,min:\", (np.max(w),np.min(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Investigate history and cv_scores\n",
    "print(histories[0].history)\n",
    "print(cv_scores)\n",
    "print(np.mean([cv_score[1] for cv_score in cv_scores]))\n",
    "\n",
    "conf = final_model.get_config()\n",
    "print(type(conf))\n",
    "final_model.to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.arange(9).reshape((3, 3))\n",
    "print(arr)\n",
    "np.random.shuffle(arr)\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new FULL_corpus if Hedda finds a better one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, nr_pos_tweets, nr_neg_tweets, total_training_tweets=HL.get_corpus(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_={'hashtag': True, 'segmentation_hash': True,'hugs_and_kisses':True,'all_smilies':True,\n",
    "        'numbers':True,'number_mention':True,'elongation':True, 'set_to_not':True,'exclamation':True}\n",
    "\n",
    "better_corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "\n",
    "better_corpus_with_ngrams = HL.creating_n_grams_corpus(2,better_corpus)\n",
    "del better_corpus\n",
    "\n",
    "stopwords= TO.get_dynamic_stopwords(full_corpus, MinDf=0.00001, MaxDf=,sublinearTF=True,useIDF=False)\n",
    "\n",
    "stopword_corpus=TO.remove_stopwords(better_corpus_with_ngrams, stopwords)\n",
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(better_corpus_with_ngrams, open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jesus_corpus = pickle.load( open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )\n",
    "print(len(jesus_corpus))\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"should make this to plot the history of epochs and validationscore\n",
    "    maybe even the crossvalidation mean of at each epoch? smoothen out the graph :)\n",
    "    \n",
    "    - make history into dataframe that fits seaborn\n",
    "    - epoch on the x axis\n",
    "    - score on the y axix (0-1)\n",
    "    - plot val_los, val_acc, train_acc and train_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sb.set(style=\"darkgrid\")\n",
    "\n",
    "    # Load the long-form example gammas dataset\n",
    "    gammas = sns.load_dataset(\"gammas\")\n",
    "\n",
    "    # Plot the response with standard error\n",
    "    sb.tsplot(data=gammas, time=\"timepoint\", unit=\"subject\",\n",
    "           condition=\"ROI\", value=\"BOLD signal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of how to save and load model\n",
    "from keras.models import load_model\n",
    "\n",
    "final_model.save('sunday0017_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del final_model  # deletes the existing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "jesus_model = load_model('my_model.h5')## Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading corpus\n",
    "\n",
    "awesome_corpus = pickle.load( open( \"stopword100_corpus_n2_SHM_E_SN_H_HK.pkl\", \"rb\" ) )\n",
    "print(len(awesome_corpus))\n",
    "\n",
    "nr_pos_tweets = 1250000\n",
    "nr_neg_tweets = 1250000\n",
    "total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Choose the corpus\n",
    "processed_corpus = awesome_corpus\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::]\n",
    "del processed_corpus\n",
    "\n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "del global_vectors\n",
    "del doc\n",
    "print(\"done with making the trainvectors\")\n",
    "\n",
    "train_document_vecz = np.concatenate(vectors)\n",
    "del vectors\n",
    "print(\"done with concatenating the trainvectors\")\n",
    "\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecz)\n",
    "del train_document_vecz\n",
    "print(\"done with scaling the trainvectors\")\n",
    "\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "print(\"done with creating the labels\")\n",
    "print(\"time used one the ordeal:\", time.time() - start)\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_document_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe store trainvectors?\n",
    "HL.pickle_dump(train_document_vecs, \"AWESOME_train_document_vecs.pkl\")\n",
    "HL.pickle_dump(labels, \"AWESOME_labels.pkl\")\n",
    "\n",
    "\n",
    "#file_path = \"testing_pickling_for_BIG_obj.pkl\"\n",
    "#pickle_dump(train_document_vecs, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already have made train_document_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_trainvecs = \"AWESOME_train_document_vecs.pkl\"\n",
    "train_document_vecs = HL.pickle_load(file_path_trainvecs)\n",
    "print(train_document_vecs.shape)\n",
    "\n",
    "file_path_labels = \"AWESOME_labels.pkl\"\n",
    "labels = HL.pickle_load(file_path_labels)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defing model :)\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "width = 500\n",
    "depth = 2\n",
    "epochs = 60\n",
    "n_folds = 2\n",
    "split = 0.9\n",
    "dropout_rate=0.4\n",
    "funnel=0.3\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = NN.dynamic_dense(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu', funnel=funnel)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(jesus_model)\n",
    "\n",
    "# further train it\n",
    "#allX = train_document_vecs\n",
    "#allY = labels\n",
    "\n",
    "improved_model, history = GV.train_NN(model, train_document_vecs, labels, epochs=1)\n",
    "# backuppickle\n",
    "improved_model.save('Backup_of_further_training_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(improved_model)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "print(\"Hello world\")\n",
    "pred=model.predict(test_document_vecs)\n",
    "\n",
    "pred_ones=[]\n",
    "for i in pred:\n",
    "    if i> 0.5:\n",
    "        pred_ones.append(1)\n",
    "    else:\n",
    "        pred_ones.append(-1)\n",
    "\n",
    "#CREATING SUBMISSION\n",
    "ids = list(range(1,10000+1))\n",
    "HL.create_csv_submission(ids, pred_ones,\"best_proc_corpus_dynamic_dense.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT RELEVANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, hyperparameters = hyperparameters_improver(dd, train_document_vecs, labels, init_hyperparameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_improver(model, X, Y, init_hyper, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1):\n",
    "    input_dimensions = X.shape[1]\n",
    "    hypers = init_hyper\n",
    "    \n",
    "    time_const = 0.1\n",
    "    loss_const = 2\n",
    "    acc_const = 0.5\n",
    "    \n",
    "    old_state = 999\n",
    "    state_hist = []\n",
    "    acc_hist = []\n",
    "    hyper_hist = [] # list of tuples like [('depth', 1), ('width', 0)...] where 1 is change up, and vice versa\n",
    "    \n",
    "    \n",
    "    curr_hyper, hyper_value = choose_hyperparameter(hypers)\n",
    "    \n",
    "    count = 0\n",
    "    while count < 2:\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model = model(input_dimensions, hypers['width'], hypers['depth'], hypers['dropout_rate'],\n",
    "                      activation=activation)\n",
    "        final_model, cv_scores = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n",
    "        time_used = time.time() - start\n",
    "        \n",
    "        state, time_dominant, acc = score_state(cv_scores, time_used)\n",
    "        state_hist.append(state)\n",
    "        acc_hist.append(acc)\n",
    "        \n",
    "        act_on_move(state_hist, hypers, hyper_hist)\n",
    "        \n",
    "        old_state = state\n",
    "        count +=1\n",
    "        \n",
    "    return model, hyperparameters, stepsizes\n",
    "\n",
    "def choose_hyperparameter(hypers):\n",
    "    \"\"\" choose random \"\"\"\n",
    "    key = random.choice(hypers.keys())\n",
    "    print(\"is this a key:\", key)\n",
    "    return key\n",
    "    \n",
    "def choose_new_value(hypers, hyper_hist):\n",
    "    \"\"\" choose either randomly either up or down, stepsize\"\"\"\n",
    "    key = choose_hyperparameter(hypers)\n",
    "    up_or_down = random.choice([0,1])\n",
    "    \n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "        \n",
    "    return key, \n",
    "\n",
    "def update_value(hypers, key, up_or_down):\n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "\n",
    "def act_on_move(state_hist, hypers, hyper_hist):\n",
    "    \"\"\"if bad move -> half decrease stepsize and go back a bit(half the distance)\n",
    "    if good...really good ---> do again or \n",
    "    barely good --> do other hyperparameter\"\"\"\n",
    "    last_change = state_hist[-1] > - state_hist[-2]\n",
    "    # was the last move a backward move?\n",
    "    #unchanged_hypers = \n",
    "    #regret_move = True if (hyper_hist[-1][0] == \n",
    "    # if the last two moves have been done on the last \n",
    "    #if (hyper_hist[-1][0] == hyper_hist[-1][0]):\n",
    "        \n",
    "        \n",
    "    if (last_change >0.02):\n",
    "        key = hyper_hist[-1][0]\n",
    "        update_value(hypers, key, up_or_down)\n",
    "    else:\n",
    "        choose_new_value(hypers, hyper_hist)\n",
    "\n",
    "def score_state(cv_scores, time_used, ):\n",
    "    \"\"\" low score is good \"\"\"\n",
    "    time_punishment = time_sensitivity*time_punish_constant*time_used\n",
    "    loss_punishment = np.mean(cv[:,0]) * loss_const\n",
    "    acc = np.mean(cv[:,1])\n",
    "    acc_punishment = acc_const/ acc\n",
    "    time_dominant = round(time_punishment/(loss_punishment + acc_punishment)-0.5)\n",
    "    return (loss_punishment + acc_punishment + time_punishment), time_dominant, acc   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural net:\n",
    "dd = NN.dynamic_dense\n",
    "X = train_document_vecs\n",
    "Y = labels\n",
    "\n",
    "init_hyperparameters = {'width':200, 'depth':5, 'dropout_rate':0.2}\n",
    "step_sizes = {'width':10, 'depth':1, 'dropout_rate':0.025}\n",
    "\n",
    "model, hyperparameters, stepsizes = hyperparameter_improver(model, X, Y, init_hyperparameters, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
