{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing dynamic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/havardbjornoy/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = \"gensim_data_folder\"\n",
    "DATA_25DIM = DATA_FOLDER + \"/gensim_glove_vectors_25dim.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/gensim_glove_vectors_50dim.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/gensim_glove_vectors_100dim.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/gensim_glove_vectors_200dim.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO-list:\n",
    "    - make dynamic work on exampledata\n",
    "    - be able to save and load weigths\n",
    "    - alter a model instead of creating new\n",
    "    -check out weigth decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TRAININGSET ready for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n"
     ]
    }
   ],
   "source": [
    "###### Choose the corpus\n",
    "processed_corpus = full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::] \n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "train_document_vecs = np.concatenate(vectors)\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make FULLSET ready for Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = pickle.load( open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )\n",
    "print(len(processed_corpus))\n",
    "\n",
    "nr_pos_tweets = 1250000\n",
    "nr_neg_tweets = 1250000\n",
    "total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Choose the corpus\n",
    "#processed_corpus = processed_full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::]\n",
    "del processed_corpus\n",
    "\n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "del global_vectors\n",
    "del doc\n",
    "print(\"done with making the trainvectors\")\n",
    "\n",
    "train_document_vecz = np.concatenate(vectors)\n",
    "del vectors\n",
    "print(\"done with concatenating the trainvectors\")\n",
    "\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecz)\n",
    "del train_document_vecz\n",
    "print(\"done with scaling the trainvectors\")\n",
    "\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "print(\"done with creating the labels\")\n",
    "\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe store trainvectors?\n",
    "pickle.dump([train_document_vecs, labels], open( \"FULL_train_document_vecs_and_labels.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run this this badboy to save time and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a perfect neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = NN.dynamic_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 113)               17063     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 85)                9690      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 85)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 64)                5504      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 48)                3120      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 27)                999       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 108,518\n",
      "Trainable params: 108,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 90000 samples, validate on 90000 samples\n",
      "Epoch 1/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.6737Epoch 00001: val_loss improved from inf to 0.44443, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.5874 - acc: 0.6748 - val_loss: 0.4444 - val_acc: 0.7878\n",
      "Epoch 2/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.7960Epoch 00002: val_loss improved from 0.44443 to 0.41461, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 89us/step - loss: 0.4483 - acc: 0.7961 - val_loss: 0.4146 - val_acc: 0.8047\n",
      "Epoch 3/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8056Epoch 00003: val_loss improved from 0.41461 to 0.40745, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 88us/step - loss: 0.4283 - acc: 0.8058 - val_loss: 0.4074 - val_acc: 0.8085\n",
      "Epoch 4/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4191 - acc: 0.8089Epoch 00004: val_loss improved from 0.40745 to 0.40168, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 90us/step - loss: 0.4193 - acc: 0.8088 - val_loss: 0.4017 - val_acc: 0.8119\n",
      "Epoch 5/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4117 - acc: 0.8123Epoch 00005: val_loss improved from 0.40168 to 0.40068, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 9s 95us/step - loss: 0.4119 - acc: 0.8121 - val_loss: 0.4007 - val_acc: 0.8134\n",
      "Epoch 6/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8141Epoch 00006: val_loss improved from 0.40068 to 0.39681, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 81us/step - loss: 0.4079 - acc: 0.8139 - val_loss: 0.3968 - val_acc: 0.8145\n",
      "Epoch 7/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8171Epoch 00007: val_loss improved from 0.39681 to 0.39206, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 80us/step - loss: 0.4013 - acc: 0.8169 - val_loss: 0.3921 - val_acc: 0.8157\n",
      "Epoch 8/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8181Epoch 00008: val_loss improved from 0.39206 to 0.39100, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 81us/step - loss: 0.3955 - acc: 0.8179 - val_loss: 0.3910 - val_acc: 0.8147\n",
      "Epoch 9/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8201- ETA: 0s - loss: 0.3926Epoch 00009: val_loss improved from 0.39100 to 0.38957, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 89us/step - loss: 0.3932 - acc: 0.8200 - val_loss: 0.3896 - val_acc: 0.8172\n",
      "Epoch 10/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8208Epoch 00010: val_loss improved from 0.38957 to 0.38925, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 82us/step - loss: 0.3899 - acc: 0.8209 - val_loss: 0.3892 - val_acc: 0.8117\n",
      "Epoch 11/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8218Epoch 00011: val_loss improved from 0.38925 to 0.38802, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 90us/step - loss: 0.3873 - acc: 0.8219 - val_loss: 0.3880 - val_acc: 0.8182\n",
      "Epoch 12/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8238Epoch 00012: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 90us/step - loss: 0.3852 - acc: 0.8237 - val_loss: 0.3901 - val_acc: 0.8181\n",
      "Epoch 13/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8222- ETA: 1s - loss: 0.3833Epoch 00013: val_loss improved from 0.38802 to 0.38747, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 85us/step - loss: 0.3825 - acc: 0.8223 - val_loss: 0.3875 - val_acc: 0.8184\n",
      "Epoch 14/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8257Epoch 00014: val_loss improved from 0.38747 to 0.38729, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 91us/step - loss: 0.3786 - acc: 0.8257 - val_loss: 0.3873 - val_acc: 0.8170\n",
      "Epoch 15/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8271Epoch 00015: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 77us/step - loss: 0.3762 - acc: 0.8271 - val_loss: 0.3873 - val_acc: 0.8181\n",
      "Epoch 16/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8294Epoch 00016: val_loss improved from 0.38729 to 0.38571, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 81us/step - loss: 0.3742 - acc: 0.8292 - val_loss: 0.3857 - val_acc: 0.8187\n",
      "Epoch 17/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8297Epoch 00017: val_loss improved from 0.38571 to 0.38477, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 78us/step - loss: 0.3737 - acc: 0.8297 - val_loss: 0.3848 - val_acc: 0.8183\n",
      "Epoch 18/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8313Epoch 00018: val_loss improved from 0.38477 to 0.38279, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 77us/step - loss: 0.3693 - acc: 0.8313 - val_loss: 0.3828 - val_acc: 0.8198\n",
      "Epoch 19/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8327Epoch 00019: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 76us/step - loss: 0.3676 - acc: 0.8326 - val_loss: 0.3843 - val_acc: 0.8202\n",
      "Epoch 20/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8343Epoch 00020: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 82us/step - loss: 0.3641 - acc: 0.8342 - val_loss: 0.3846 - val_acc: 0.8205\n",
      "Epoch 21/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8336Epoch 00021: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 80us/step - loss: 0.3641 - acc: 0.8336 - val_loss: 0.3860 - val_acc: 0.8199\n",
      "Epoch 22/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8356Epoch 00022: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 88us/step - loss: 0.3616 - acc: 0.8356 - val_loss: 0.3834 - val_acc: 0.8210\n",
      "Epoch 23/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8378Epoch 00023: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 79us/step - loss: 0.3580 - acc: 0.8377 - val_loss: 0.3846 - val_acc: 0.8211\n",
      "Epoch 00023: early stopping\n",
      "Train on 90000 samples, validate on 90000 samples\n",
      "Epoch 1/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.6821Epoch 00001: val_loss improved from inf to 0.51817, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 14s 151us/step - loss: 0.6044 - acc: 0.6830 - val_loss: 0.5182 - val_acc: 0.7872\n",
      "Epoch 2/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4733 - acc: 0.7883Epoch 00002: val_loss improved from 0.51817 to 0.41199, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 80us/step - loss: 0.4727 - acc: 0.7886 - val_loss: 0.4120 - val_acc: 0.8076\n",
      "Epoch 3/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8008Epoch 00003: val_loss improved from 0.41199 to 0.40365, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 82us/step - loss: 0.4386 - acc: 0.8006 - val_loss: 0.4037 - val_acc: 0.8112\n",
      "Epoch 4/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8066Epoch 00004: val_loss improved from 0.40365 to 0.39660, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 86us/step - loss: 0.4252 - acc: 0.8065 - val_loss: 0.3966 - val_acc: 0.8132\n",
      "Epoch 5/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8085Epoch 00005: val_loss improved from 0.39660 to 0.39130, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 78us/step - loss: 0.4186 - acc: 0.8084 - val_loss: 0.3913 - val_acc: 0.8162\n",
      "Epoch 6/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8119Epoch 00006: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 79us/step - loss: 0.4105 - acc: 0.8120 - val_loss: 0.3914 - val_acc: 0.8170\n",
      "Epoch 7/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8140Epoch 00007: val_loss improved from 0.39130 to 0.38583, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 78us/step - loss: 0.4074 - acc: 0.8140 - val_loss: 0.3858 - val_acc: 0.8195\n",
      "Epoch 8/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8155Epoch 00008: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 81us/step - loss: 0.4041 - acc: 0.8155 - val_loss: 0.3885 - val_acc: 0.8184\n",
      "Epoch 9/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8183- ETA: 0s - loss: 0.3977 - acc: 0.Epoch 00009: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 78us/step - loss: 0.3989 - acc: 0.8181 - val_loss: 0.3863 - val_acc: 0.8204\n",
      "Epoch 10/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8195- ETA: 0s - loss: 0.3952 - Epoch 00010: val_loss improved from 0.38583 to 0.38202, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 82us/step - loss: 0.3951 - acc: 0.8194 - val_loss: 0.3820 - val_acc: 0.8221\n",
      "Epoch 11/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8216Epoch 00011: val_loss improved from 0.38202 to 0.38197, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 79us/step - loss: 0.3923 - acc: 0.8216 - val_loss: 0.3820 - val_acc: 0.8222\n",
      "Epoch 12/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8222Epoch 00012: val_loss improved from 0.38197 to 0.38192, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 8s 89us/step - loss: 0.3915 - acc: 0.8223 - val_loss: 0.3819 - val_acc: 0.8205\n",
      "Epoch 13/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8244- ETA:Epoch 00013: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 78us/step - loss: 0.3858 - acc: 0.8244 - val_loss: 0.3848 - val_acc: 0.8210\n",
      "Epoch 14/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8264Epoch 00014: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 86us/step - loss: 0.3840 - acc: 0.8265 - val_loss: 0.3828 - val_acc: 0.8213\n",
      "Epoch 15/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8274Epoch 00015: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 82us/step - loss: 0.3832 - acc: 0.8273 - val_loss: 0.3834 - val_acc: 0.8225\n",
      "Epoch 16/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8271Epoch 00016: val_loss improved from 0.38192 to 0.37753, saving model to best_dd_model.hdf5\n",
      "90000/90000 [==============================] - 7s 79us/step - loss: 0.3799 - acc: 0.8270 - val_loss: 0.3775 - val_acc: 0.8240\n",
      "Epoch 17/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8289Epoch 00017: val_loss did not improve\n",
      "90000/90000 [==============================] - 7s 77us/step - loss: 0.3776 - acc: 0.8289 - val_loss: 0.3789 - val_acc: 0.8221\n",
      "Epoch 18/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8305Epoch 00018: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 85us/step - loss: 0.3765 - acc: 0.8304 - val_loss: 0.3783 - val_acc: 0.8245\n",
      "Epoch 19/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8314Epoch 00019: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 85us/step - loss: 0.3731 - acc: 0.8313 - val_loss: 0.3796 - val_acc: 0.8232\n",
      "Epoch 20/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8316Epoch 00020: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 84us/step - loss: 0.3723 - acc: 0.8317 - val_loss: 0.3790 - val_acc: 0.8228\n",
      "Epoch 21/60\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8332Epoch 00021: val_loss did not improve\n",
      "90000/90000 [==============================] - 8s 84us/step - loss: 0.3696 - acc: 0.8332 - val_loss: 0.3793 - val_acc: 0.8236\n",
      "Epoch 00021: early stopping\n",
      "Val_accuracies: 82.23% (+/- 0.1267)\n",
      "Time taken:  6.066332117716471 \n",
      "\n",
      "180000/180000 [==============================] - 12s 66us/step\n",
      "20000/20000 [==============================] - 1s 70us/step\n",
      "evaluate on train_data: (loss:0.35624 , acc:83.690%):\n",
      "Unseen_accuracies: (loss:0.3811 , acc:82.3550%):\n",
      "Time taken:  6.293135917186737 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "width = 200\n",
    "depth = 8\n",
    "epochs = 60\n",
    "n_folds = 2\n",
    "split = 0.9\n",
    "dropout_rate=0.4\n",
    "funnel=0.75\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = dd(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu', funnel=funnel)\n",
    "print(model.summary())\n",
    "\n",
    "final_model, cv_scores, histories = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# investigating range of weights in model\n",
    "weights = []\n",
    "for layer in final_model.layers:\n",
    "    w = layer.get_weights()\n",
    "    weights.append(w)\n",
    "    #print(\"mean:\", np.mean(w))\n",
    "    #print(\"std:\", np.std(w))\n",
    "    #print(\"max,min:\", (np.max(w),np.min(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Investigate history and cv_scores\n",
    "print(histories[0].history)\n",
    "print(cv_scores)\n",
    "print(np.mean([cv_score[1] for cv_score in cv_scores]))\n",
    "\n",
    "conf = final_model.get_config()\n",
    "print(type(conf))\n",
    "final_model.to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.arange(9).reshape((3, 3))\n",
    "print(arr)\n",
    "np.random.shuffle(arr)\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new FULL_corpus if Hedda finds a better one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, nr_pos_tweets, nr_neg_tweets, total_training_tweets=HL.get_corpus(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_={'hashtag': True, 'segmentation_hash': True,'hugs_and_kisses':True,'all_smilies':True,\n",
    "        'numbers':True,'number_mention':True,'elongation':True, 'set_to_not':True,'exclamation':True}\n",
    "\n",
    "better_corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "\n",
    "better_corpus_with_ngrams = HL.creating_n_grams_corpus(2,better_corpus)\n",
    "del better_corpus\n",
    "\n",
    "stopwords= TO.get_dynamic_stopwords(full_corpus, MinDf=0.00001, MaxDf=,sublinearTF=True,useIDF=False)\n",
    "\n",
    "stopword_corpus=TO.remove_stopwords(better_corpus_with_ngrams, stopwords)\n",
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(better_corpus_with_ngrams, open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jesus_corpus = pickle.load( open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )\n",
    "print(len(jesus_corpus))\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"should make this to plot the history of epochs and validationscore\n",
    "    maybe even the crossvalidation mean of at each epoch? smoothen out the graph :)\n",
    "    \n",
    "    - make history into dataframe that fits seaborn\n",
    "    - epoch on the x axis\n",
    "    - score on the y axix (0-1)\n",
    "    - plot val_los, val_acc, train_acc and train_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sb.set(style=\"darkgrid\")\n",
    "\n",
    "    # Load the long-form example gammas dataset\n",
    "    gammas = sns.load_dataset(\"gammas\")\n",
    "\n",
    "    # Plot the response with standard error\n",
    "    sb.tsplot(data=gammas, time=\"timepoint\", unit=\"subject\",\n",
    "           condition=\"ROI\", value=\"BOLD signal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of how to save and load model\n",
    "from keras.models import load_model\n",
    "\n",
    "final_model.save('sunday0017_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del final_model  # deletes the existing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "jesus_model = load_model('my_model.h5')## Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(jesus_model)\n",
    "\n",
    "# further train it\n",
    "allX = train_document_vecs\n",
    "allY = labels\n",
    "\n",
    "improved_model, history = GV.train_NN(jesus_model, allX, allY, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify the bitches\n",
    "epochs = 4\n",
    "n_folds = 2\n",
    "\n",
    "model_scores= GV.run_k_fold([final_model], train_document_vecs, labels, epochs, n_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x17a113da0>\n"
     ]
    }
   ],
   "source": [
    "print(final_model)\n",
    "model = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "print(\"Hello world\")\n",
    "pred=model.predict(test_document_vecs)\n",
    "\n",
    "pred_ones=[]\n",
    "for i in pred:\n",
    "    if i> 0.5:\n",
    "        pred_ones.append(1)\n",
    "    else:\n",
    "        pred_ones.append(-1)\n",
    "\n",
    "#CREATING SUBMISSION\n",
    "ids = list(range(1,10000+1))\n",
    "HL.create_csv_submission(ids, pred_ones,\"RIGHTONEsunday0017_w200_d6_n3_dr04.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT RELEVANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, hyperparameters = hyperparameters_improver(dd, train_document_vecs, labels, init_hyperparameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_improver(model, X, Y, init_hyper, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1):\n",
    "    input_dimensions = X.shape[1]\n",
    "    hypers = init_hyper\n",
    "    \n",
    "    time_const = 0.1\n",
    "    loss_const = 2\n",
    "    acc_const = 0.5\n",
    "    \n",
    "    old_state = 999\n",
    "    state_hist = []\n",
    "    acc_hist = []\n",
    "    hyper_hist = [] # list of tuples like [('depth', 1), ('width', 0)...] where 1 is change up, and vice versa\n",
    "    \n",
    "    \n",
    "    curr_hyper, hyper_value = choose_hyperparameter(hypers)\n",
    "    \n",
    "    count = 0\n",
    "    while count < 2:\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model = model(input_dimensions, hypers['width'], hypers['depth'], hypers['dropout_rate'],\n",
    "                      activation=activation)\n",
    "        final_model, cv_scores = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n",
    "        time_used = time.time() - start\n",
    "        \n",
    "        state, time_dominant, acc = score_state(cv_scores, time_used)\n",
    "        state_hist.append(state)\n",
    "        acc_hist.append(acc)\n",
    "        \n",
    "        act_on_move(state_hist, hypers, hyper_hist)\n",
    "        \n",
    "        old_state = state\n",
    "        count +=1\n",
    "        \n",
    "    return model, hyperparameters, stepsizes\n",
    "\n",
    "def choose_hyperparameter(hypers):\n",
    "    \"\"\" choose random \"\"\"\n",
    "    key = random.choice(hypers.keys())\n",
    "    print(\"is this a key:\", key)\n",
    "    return key\n",
    "    \n",
    "def choose_new_value(hypers, hyper_hist):\n",
    "    \"\"\" choose either randomly either up or down, stepsize\"\"\"\n",
    "    key = choose_hyperparameter(hypers)\n",
    "    up_or_down = random.choice([0,1])\n",
    "    \n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "        \n",
    "    return key, \n",
    "\n",
    "def update_value(hypers, key, up_or_down):\n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "\n",
    "def act_on_move(state_hist, hypers, hyper_hist):\n",
    "    \"\"\"if bad move -> half decrease stepsize and go back a bit(half the distance)\n",
    "    if good...really good ---> do again or \n",
    "    barely good --> do other hyperparameter\"\"\"\n",
    "    last_change = state_hist[-1] > - state_hist[-2]\n",
    "    # was the last move a backward move?\n",
    "    #unchanged_hypers = \n",
    "    #regret_move = True if (hyper_hist[-1][0] == \n",
    "    # if the last two moves have been done on the last \n",
    "    #if (hyper_hist[-1][0] == hyper_hist[-1][0]):\n",
    "        \n",
    "        \n",
    "    if (last_change >0.02):\n",
    "        key = hyper_hist[-1][0]\n",
    "        update_value(hypers, key, up_or_down)\n",
    "    else:\n",
    "        choose_new_value(hypers, hyper_hist)\n",
    "\n",
    "def score_state(cv_scores, time_used, ):\n",
    "    \"\"\" low score is good \"\"\"\n",
    "    time_punishment = time_sensitivity*time_punish_constant*time_used\n",
    "    loss_punishment = np.mean(cv[:,0]) * loss_const\n",
    "    acc = np.mean(cv[:,1])\n",
    "    acc_punishment = acc_const/ acc\n",
    "    time_dominant = round(time_punishment/(loss_punishment + acc_punishment)-0.5)\n",
    "    return (loss_punishment + acc_punishment + time_punishment), time_dominant, acc   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural net:\n",
    "dd = NN.dynamic_dense\n",
    "X = train_document_vecs\n",
    "Y = labels\n",
    "\n",
    "init_hyperparameters = {'width':200, 'depth':5, 'dropout_rate':0.2}\n",
    "step_sizes = {'width':10, 'depth':1, 'dropout_rate':0.025}\n",
    "\n",
    "model, hyperparameters, stepsizes = hyperparameter_improver(model, X, Y, init_hyperparameters, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
