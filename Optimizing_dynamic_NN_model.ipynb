{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing dynamic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "import random\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = \"gensim_data_folder\"\n",
    "DATA_25DIM = DATA_FOLDER + \"/gensim_glove_vectors_25dim.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/gensim_glove_vectors_50dim.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/gensim_glove_vectors_100dim.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/gensim_glove_vectors_200dim.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO-list:\n",
    "    - make dynamic work on exampledata\n",
    "    - be able to save and load weigths\n",
    "    - alter a model instead of creating new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n"
     ]
    }
   ],
   "source": [
    "###### Choose the corpus\n",
    "processed_corpus = full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::] \n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "train_document_vecs = np.concatenate(vectors)\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a perfect neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_improver(model, X, Y, init_hyper, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1):\n",
    "    input_dimensions = X.shape[1]\n",
    "    hypers = init_hyper\n",
    "    \n",
    "    time_const = 0.1\n",
    "    loss_const = 2\n",
    "    acc_const = 0.5\n",
    "    \n",
    "    old_state = 999\n",
    "    state_hist = []\n",
    "    acc_hist = []\n",
    "    hyper_hist = [] # list of tuples like [('depth', 1), ('width', 0)...] where 1 is change up, and vice versa\n",
    "    \n",
    "    \n",
    "    curr_hyper, hyper_value = choose_hyperparameter(hypers)\n",
    "    \n",
    "    count = 0\n",
    "    while count < 2:\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model = model(input_dimensions, hypers['width'], hypers['depth'], hypers['dropout_rate'],\n",
    "                      activation=activation)\n",
    "        final_model, cv_scores = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n",
    "        time_used = time.time() - start\n",
    "        \n",
    "        state, time_dominant, acc = score_state(cv_scores, time_used)\n",
    "        state_hist.append(state)\n",
    "        acc_hist.append(acc)\n",
    "        \n",
    "        act_on_move(state_hist, hypers, hyper_hist)\n",
    "        \n",
    "        old_state = state\n",
    "        count +=1\n",
    "        \n",
    "    return model, hyperparameters, stepsizes\n",
    "\n",
    "def choose_hyperparameter(hypers):\n",
    "    \"\"\" choose random \"\"\"\n",
    "    key = random.choice(hypers.keys())\n",
    "    print(\"is this a key:\", key)\n",
    "    return key\n",
    "    \n",
    "def choose_new_value(hypers, hyper_hist):\n",
    "    \"\"\" choose either randomly either up or down, stepsize\"\"\"\n",
    "    key = choose_hyperparameter(hypers)\n",
    "    up_or_down = random.choice([0,1])\n",
    "    \n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "        \n",
    "    return key, \n",
    "\n",
    "def update_value(hypers, key, up_or_down):\n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "\n",
    "def act_on_move(state_hist, hypers, hyper_hist):\n",
    "    \"\"\"if bad move -> half decrease stepsize and go back a bit(half the distance)\n",
    "    if good...really good ---> do again or \n",
    "    barely good --> do other hyperparameter\"\"\"\n",
    "    last_change = state_hist[-1] > - state_hist[-2]\n",
    "    # was the last move a backward move?\n",
    "    #unchanged_hypers = \n",
    "    #regret_move = True if (hyper_hist[-1][0] == \n",
    "    # if the last two moves have been done on the last \n",
    "    #if (hyper_hist[-1][0] == hyper_hist[-1][0]):\n",
    "        \n",
    "        \n",
    "    if (last_change >0.02):\n",
    "        key = hyper_hist[-1][0]\n",
    "        update_value(hypers, key, up_or_down)\n",
    "    else:\n",
    "        choose_new_value(hypers, hyper_hist)\n",
    "\n",
    "def score_state(cv_scores, time_used, ):\n",
    "    \"\"\" low score is good \"\"\"\n",
    "    time_punishment = time_sensitivity*time_punish_constant*time_used\n",
    "    loss_punishment = np.mean(cv[:,0]) * loss_const\n",
    "    acc = np.mean(cv[:,1])\n",
    "    acc_punishment = acc_const/ acc\n",
    "    time_dominant = round(time_punishment/(loss_punishment + acc_punishment)-0.5)\n",
    "    return (loss_punishment + acc_punishment + time_punishment), time_dominant, acc   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a26151187963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m model, hyperparameters, stepsizes = hyperparameter_improver(model, X, Y, init_hyperparameters, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n\u001b[0;32m---> 10\u001b[0;31m                             epoch_threshold=2, time_sensitivity=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-95eea9612406>\u001b[0m in \u001b[0;36mhyperparameter_improver\u001b[0;34m(model, X, Y, init_hyper, step_sizes, epochs, n_folds, split, activation, epoch_threshold, time_sensitivity)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcurr_hyper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-95eea9612406>\u001b[0m in \u001b[0;36mchoose_hyperparameter\u001b[0;34m(hypers)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchoose_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m\"\"\" choose random \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is this a key:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object does not support indexing"
     ]
    }
   ],
   "source": [
    "# Neural net:\n",
    "dd = NN.dynamic_dense\n",
    "X = train_document_vecs\n",
    "Y = labels\n",
    "\n",
    "init_hyperparameters = {'width':200, 'depth':5, 'dropout_rate':0.2}\n",
    "step_sizes = {'width':10, 'depth':1, 'dropout_rate':0.025}\n",
    "\n",
    "model, hyperparameters, stepsizes = hyperparameter_improver(model, X, Y, init_hyperparameters, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyperparameters_improver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-62ff01534496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters_improver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_document_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_hyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hyperparameters_improver' is not defined"
     ]
    }
   ],
   "source": [
    "model, hyperparameters = hyperparameters_improver(dd, train_document_vecs, labels, init_hyperparameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 421,801\n",
      "Trainable params: 421,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 70000 samples, validate on 70000 samples\n",
      "Epoch 1/3\n",
      "70000/70000 [==============================] - 22s 319us/step - loss: 0.4134 - acc: 0.8132 - val_loss: 0.3482 - val_acc: 0.8498\n",
      "Epoch 2/3\n",
      "70000/70000 [==============================] - 20s 286us/step - loss: 0.3402 - acc: 0.8544 - val_loss: 0.3369 - val_acc: 0.8547\n",
      "Epoch 3/3\n",
      "70000/70000 [==============================] - 20s 289us/step - loss: 0.3240 - acc: 0.8623 - val_loss: 0.3333 - val_acc: 0.8585\n",
      "70000/70000 [==============================] - 8s 110us/step\n",
      "Train on 70000 samples, validate on 70000 samples\n",
      "Epoch 1/3\n",
      "70000/70000 [==============================] - 20s 291us/step - loss: 0.3343 - acc: 0.8564 - val_loss: 0.3089 - val_acc: 0.8702\n",
      "Epoch 2/3\n",
      "70000/70000 [==============================] - 20s 284us/step - loss: 0.3166 - acc: 0.8645 - val_loss: 0.3079 - val_acc: 0.8692\n",
      "Epoch 3/3\n",
      "70000/70000 [==============================] - 20s 282us/step - loss: 0.3058 - acc: 0.8695 - val_loss: 0.3115 - val_acc: 0.8683\n",
      "70000/70000 [==============================] - 8s 111us/step\n",
      "Accuracies: 0.59% (+/- 0.27%)\n",
      "cv_scores: [[0.33333887838163812, 0.85851428571428567], [0.31152923876760263, 0.86829999999999996]]\n",
      "evaluation score: [0.62878667776584629, 0.66783333333333328]\n",
      "Time taken:  2.5597580671310425 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "init_hyperparameters = {'width':200, 'depth':5, 'dropout_rate':0.1}\n",
    "width = 300\n",
    "depth = 5\n",
    "epochs = 3\n",
    "n_folds = 2\n",
    "split = 0.7\n",
    "dropout_rate=0.1\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = dd(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu')\n",
    "print(model.summary())\n",
    "\n",
    "final_model = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the bitches\n",
    "epochs = 5\n",
    "n_folds = 2\n",
    "\n",
    "model_scores= GV.run_k_fold([model], train_document_vecs, labels, epochs, n_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [\n",
    "                  [4,5,6],\n",
    "                  [7,8,9]]\n",
    "np.random.shuffle(a)\n",
    "a = np.array(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "\n",
    "print(\"Hello world\")\n",
    "pred=model.predict(test_document_vecs)\n",
    "\n",
    "pred_ones=[]\n",
    "for i in pred:\n",
    "    if i> 0.5:\n",
    "        pred_ones.append(1)\n",
    "    else:\n",
    "        pred_ones.append(-1)\n",
    "\n",
    "#CREATING SUBMISSION\n",
    "ids = list(range(1,10000+1))\n",
    "HL.create_csv_submission(ids, pred_ones,\"testing_dd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
