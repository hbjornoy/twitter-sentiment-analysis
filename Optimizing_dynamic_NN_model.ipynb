{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing dynamic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "import random\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = \"gensim_data_folder\"\n",
    "DATA_25DIM = DATA_FOLDER + \"/gensim_glove_vectors_25dim.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/gensim_glove_vectors_50dim.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/gensim_glove_vectors_100dim.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/gensim_glove_vectors_200dim.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO-list:\n",
    "    - make dynamic work on exampledata\n",
    "    - be able to save and load weigths\n",
    "    - alter a model instead of creating new\n",
    "    -check out weigth decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n"
     ]
    }
   ],
   "source": [
    "###### Choose the corpus\n",
    "processed_corpus = full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::] \n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "train_document_vecs = np.concatenate(vectors)\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a perfect neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = NN.dynamic_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 281,601\n",
      "Trainable params: 281,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[     0      1      2 ..., 199997 199998 199999]\n",
      "[ 64404 175630 180009 ...,   9448 188605   3223]\n",
      "Train on 70000 samples, validate on 70000 samples\n",
      "Epoch 1/15\n",
      "70000/70000 [==============================] - 22s 308us/step - loss: 0.5081 - acc: 0.7259 - val_loss: 0.4228 - val_acc: 0.7980\n",
      "Epoch 2/15\n",
      "70000/70000 [==============================] - 13s 181us/step - loss: 0.4146 - acc: 0.8056 - val_loss: 0.4043 - val_acc: 0.8081\n",
      "Epoch 3/15\n",
      "70000/70000 [==============================] - 12s 178us/step - loss: 0.3964 - acc: 0.8141 - val_loss: 0.4002 - val_acc: 0.8117\n",
      "Epoch 4/15\n",
      "70000/70000 [==============================] - 13s 179us/step - loss: 0.3850 - acc: 0.8213 - val_loss: 0.3934 - val_acc: 0.8149\n",
      "Epoch 5/15\n",
      "70000/70000 [==============================] - 14s 197us/step - loss: 0.3779 - acc: 0.8258 - val_loss: 0.3918 - val_acc: 0.8148\n",
      "Epoch 6/15\n",
      "70000/70000 [==============================] - 12s 178us/step - loss: 0.3686 - acc: 0.8291 - val_loss: 0.3878 - val_acc: 0.8174\n",
      "Epoch 7/15\n",
      "70000/70000 [==============================] - 13s 179us/step - loss: 0.3622 - acc: 0.8322 - val_loss: 0.3862 - val_acc: 0.8188\n",
      "Epoch 8/15\n",
      "70000/70000 [==============================] - 13s 180us/step - loss: 0.3550 - acc: 0.8366 - val_loss: 0.3875 - val_acc: 0.8175\n",
      "Epoch 9/15\n",
      "70000/70000 [==============================] - 13s 187us/step - loss: 0.3494 - acc: 0.8405 - val_loss: 0.3901 - val_acc: 0.8167\n",
      "Epoch 10/15\n",
      "70000/70000 [==============================] - 13s 179us/step - loss: 0.3435 - acc: 0.8440 - val_loss: 0.3949 - val_acc: 0.8135\n",
      "Epoch 00010: early stopping\n",
      "Train on 70000 samples, validate on 70000 samples\n",
      "Epoch 1/15\n",
      "70000/70000 [==============================] - 16s 225us/step - loss: 0.5000 - acc: 0.7462 - val_loss: 0.4158 - val_acc: 0.8029\n",
      "Epoch 2/15\n",
      "70000/70000 [==============================] - 13s 180us/step - loss: 0.4207 - acc: 0.8029 - val_loss: 0.4011 - val_acc: 0.8104\n",
      "Epoch 3/15\n",
      "70000/70000 [==============================] - 13s 189us/step - loss: 0.4048 - acc: 0.8101 - val_loss: 0.3964 - val_acc: 0.8128\n",
      "Epoch 4/15\n",
      "70000/70000 [==============================] - 12s 177us/step - loss: 0.3944 - acc: 0.8167 - val_loss: 0.3886 - val_acc: 0.8165\n",
      "Epoch 5/15\n",
      "70000/70000 [==============================] - 12s 177us/step - loss: 0.3833 - acc: 0.8229 - val_loss: 0.3880 - val_acc: 0.8195\n",
      "Epoch 6/15\n",
      "70000/70000 [==============================] - 12s 177us/step - loss: 0.3763 - acc: 0.8262 - val_loss: 0.3829 - val_acc: 0.8205\n",
      "Epoch 7/15\n",
      "70000/70000 [==============================] - 12s 178us/step - loss: 0.3682 - acc: 0.8310 - val_loss: 0.3830 - val_acc: 0.8198\n",
      "Epoch 8/15\n",
      "70000/70000 [==============================] - 13s 183us/step - loss: 0.3620 - acc: 0.8335 - val_loss: 0.3818 - val_acc: 0.8205\n",
      "Epoch 9/15\n",
      "70000/70000 [==============================] - 12s 178us/step - loss: 0.3551 - acc: 0.8382 - val_loss: 0.3822 - val_acc: 0.8185\n",
      "Epoch 10/15\n",
      "70000/70000 [==============================] - 13s 180us/step - loss: 0.3492 - acc: 0.8409 - val_loss: 0.3845 - val_acc: 0.8187\n",
      "Epoch 11/15\n",
      "70000/70000 [==============================] - 13s 181us/step - loss: 0.3430 - acc: 0.8452 - val_loss: 0.3899 - val_acc: 0.8170\n",
      "Epoch 00011: early stopping\n",
      "Val_accuracies: 81.53% (+/- 0.2506%)\n",
      "Time taken:  4.886792318026225 \n",
      "\n",
      "140000/140000 [==============================] - 14s 100us/step\n",
      "60000/60000 [==============================] - 6s 100us/step\n",
      "evaluate on train_data: (loss:0.35% , acc:0.8398%):\n",
      "Unseen_accuracies: (loss:0.39% , acc:0.8163%):\n",
      "metrics in score^: ['loss', 'acc']\n",
      "Time taken:  5.223431519667307 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "width = 200\n",
    "depth = 7\n",
    "epochs = 15\n",
    "n_folds = 2\n",
    "split = 0.7\n",
    "dropout_rate=0.2\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = dd(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu')\n",
    "print(model.summary())\n",
    "\n",
    "final_model, cv_scores, histories = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate history and cv_scores\n",
    "print(histories[0].history)\n",
    "print(cv_scores)\n",
    "print(np.mean([cv_score[1] for cv_score in cv_scores]))\n",
    "\n",
    "conf = final_model.get_config()\n",
    "print(type(conf))\n",
    "final_model.to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[0 1 2]\n",
      " [6 7 8]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(9).reshape((3, 3))\n",
    "print(arr)\n",
    "np.random.shuffle(arr)\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"should make this to plot the history of epochs and validationscore\n",
    "    maybe even the crossvalidation mean of at each epoch? smoothen out the graph :)\n",
    "    \n",
    "    - make history into dataframe that fits seaborn\n",
    "    - epoch on the x axis\n",
    "    - score on the y axix (0-1)\n",
    "    - plot val_los, val_acc, train_acc and train_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sb.set(style=\"darkgrid\")\n",
    "\n",
    "    # Load the long-form example gammas dataset\n",
    "    gammas = sns.load_dataset(\"gammas\")\n",
    "\n",
    "    # Plot the response with standard error\n",
    "    sb.tsplot(data=gammas, time=\"timepoint\", unit=\"subject\",\n",
    "           condition=\"ROI\", value=\"BOLD signal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of how to save and load model\n",
    "from keras.models import load_model\n",
    "\n",
    "final_model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del final_model  # deletes the existing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "jesus_model = load_model('my_model.h5')## Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jesus_model)\n",
    "\n",
    "# further train it\n",
    "allX = train_document_vecs\n",
    "allY = labels\n",
    "\n",
    "improved_model, history = GV.train_NN(jesus_model, allX, allY, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the bitches\n",
    "epochs = 4\n",
    "n_folds = 2\n",
    "\n",
    "model_scores= GV.run_k_fold([final_model], train_document_vecs, labels, epochs, n_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x1749dfef0>\n"
     ]
    }
   ],
   "source": [
    "print(final_model)\n",
    "model = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "# KAGGLE SUBMISSION\n",
    "#test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "#test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "\n",
    "print(\"Hello world\")\n",
    "pred=model.predict(test_document_vecs)\n",
    "\n",
    "pred_ones=[]\n",
    "for i in pred:\n",
    "    if i> 0.5:\n",
    "        pred_ones.append(1)\n",
    "    else:\n",
    "        pred_ones.append(-1)\n",
    "\n",
    "#CREATING SUBMISSION\n",
    "ids = list(range(1,10000+1))\n",
    "HL.create_csv_submission(ids, pred_ones,\"testing_with_unseen_as_val_acc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT RELEVANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, hyperparameters = hyperparameters_improver(dd, train_document_vecs, labels, init_hyperparameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_improver(model, X, Y, init_hyper, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1):\n",
    "    input_dimensions = X.shape[1]\n",
    "    hypers = init_hyper\n",
    "    \n",
    "    time_const = 0.1\n",
    "    loss_const = 2\n",
    "    acc_const = 0.5\n",
    "    \n",
    "    old_state = 999\n",
    "    state_hist = []\n",
    "    acc_hist = []\n",
    "    hyper_hist = [] # list of tuples like [('depth', 1), ('width', 0)...] where 1 is change up, and vice versa\n",
    "    \n",
    "    \n",
    "    curr_hyper, hyper_value = choose_hyperparameter(hypers)\n",
    "    \n",
    "    count = 0\n",
    "    while count < 2:\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model = model(input_dimensions, hypers['width'], hypers['depth'], hypers['dropout_rate'],\n",
    "                      activation=activation)\n",
    "        final_model, cv_scores = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n",
    "        time_used = time.time() - start\n",
    "        \n",
    "        state, time_dominant, acc = score_state(cv_scores, time_used)\n",
    "        state_hist.append(state)\n",
    "        acc_hist.append(acc)\n",
    "        \n",
    "        act_on_move(state_hist, hypers, hyper_hist)\n",
    "        \n",
    "        old_state = state\n",
    "        count +=1\n",
    "        \n",
    "    return model, hyperparameters, stepsizes\n",
    "\n",
    "def choose_hyperparameter(hypers):\n",
    "    \"\"\" choose random \"\"\"\n",
    "    key = random.choice(hypers.keys())\n",
    "    print(\"is this a key:\", key)\n",
    "    return key\n",
    "    \n",
    "def choose_new_value(hypers, hyper_hist):\n",
    "    \"\"\" choose either randomly either up or down, stepsize\"\"\"\n",
    "    key = choose_hyperparameter(hypers)\n",
    "    up_or_down = random.choice([0,1])\n",
    "    \n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "        \n",
    "    return key, \n",
    "\n",
    "def update_value(hypers, key, up_or_down):\n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "\n",
    "def act_on_move(state_hist, hypers, hyper_hist):\n",
    "    \"\"\"if bad move -> half decrease stepsize and go back a bit(half the distance)\n",
    "    if good...really good ---> do again or \n",
    "    barely good --> do other hyperparameter\"\"\"\n",
    "    last_change = state_hist[-1] > - state_hist[-2]\n",
    "    # was the last move a backward move?\n",
    "    #unchanged_hypers = \n",
    "    #regret_move = True if (hyper_hist[-1][0] == \n",
    "    # if the last two moves have been done on the last \n",
    "    #if (hyper_hist[-1][0] == hyper_hist[-1][0]):\n",
    "        \n",
    "        \n",
    "    if (last_change >0.02):\n",
    "        key = hyper_hist[-1][0]\n",
    "        update_value(hypers, key, up_or_down)\n",
    "    else:\n",
    "        choose_new_value(hypers, hyper_hist)\n",
    "\n",
    "def score_state(cv_scores, time_used, ):\n",
    "    \"\"\" low score is good \"\"\"\n",
    "    time_punishment = time_sensitivity*time_punish_constant*time_used\n",
    "    loss_punishment = np.mean(cv[:,0]) * loss_const\n",
    "    acc = np.mean(cv[:,1])\n",
    "    acc_punishment = acc_const/ acc\n",
    "    time_dominant = round(time_punishment/(loss_punishment + acc_punishment)-0.5)\n",
    "    return (loss_punishment + acc_punishment + time_punishment), time_dominant, acc   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural net:\n",
    "dd = NN.dynamic_dense\n",
    "X = train_document_vecs\n",
    "Y = labels\n",
    "\n",
    "init_hyperparameters = {'width':200, 'depth':5, 'dropout_rate':0.2}\n",
    "step_sizes = {'width':10, 'depth':1, 'dropout_rate':0.025}\n",
    "\n",
    "model, hyperparameters, stepsizes = hyperparameter_improver(model, X, Y, init_hyperparameters, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
