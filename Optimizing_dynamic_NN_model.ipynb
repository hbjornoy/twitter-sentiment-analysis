{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing dynamic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/havardbjornoy/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = \"gensim_data_folder\"\n",
    "DATA_25DIM = DATA_FOLDER + \"/gensim_glove_vectors_25dim.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/gensim_glove_vectors_50dim.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/gensim_glove_vectors_100dim.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/gensim_glove_vectors_200dim.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO-list:\n",
    "    - make dynamic work on exampledata\n",
    "    - be able to save and load weigths\n",
    "    - alter a model instead of creating new\n",
    "    -check out weigth decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TRAININGSET ready for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n"
     ]
    }
   ],
   "source": [
    "###### Choose the corpus\n",
    "processed_corpus = full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::] \n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "train_document_vecs = np.concatenate(vectors)\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make FULLSET ready for Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = pickle.load( open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )\n",
    "print(len(processed_corpus))\n",
    "\n",
    "nr_pos_tweets = 1250000\n",
    "nr_neg_tweets = 1250000\n",
    "total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Choose the corpus\n",
    "#processed_corpus = processed_full_corpus\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::]\n",
    "del processed_corpus\n",
    "gc.collect()\n",
    "\n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "del global_vectors\n",
    "del doc\n",
    "gc.collect()\n",
    "print(\"done with making the trainvectors\")\n",
    "\n",
    "train_document_vecz = np.concatenate(vectors)\n",
    "del vectors\n",
    "gc.collect()\n",
    "print(\"done with concatenating the trainvectors\")\n",
    "\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecz)\n",
    "del train_document_vecz\n",
    "gc.collect()\n",
    "print(\"done with scaling the trainvectors\")\n",
    "\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "print(\"done with creating the labels\")\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe store trainvectors?\n",
    "pickle.dump(train_document_vecs, open( \"FULL_train_document_vecs.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del processed_corpus\n",
    "#del global_vectors\n",
    "#del doc\n",
    "#del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run this this badboy to save time and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a perfect neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = NN.dynamic_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 60000 samples\n",
      "Epoch 1/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.7455Epoch 00001: val_loss improved from inf to 0.41976, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 25s 209us/step - loss: 0.4955 - acc: 0.7455 - val_loss: 0.4198 - val_acc: 0.8032\n",
      "Epoch 2/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.8025Epoch 00002: val_loss improved from 0.41976 to 0.40463, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 21s 174us/step - loss: 0.4225 - acc: 0.8025 - val_loss: 0.4046 - val_acc: 0.8098\n",
      "Epoch 3/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8082Epoch 00003: val_loss improved from 0.40463 to 0.39875, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 19s 157us/step - loss: 0.4085 - acc: 0.8082 - val_loss: 0.3987 - val_acc: 0.8138\n",
      "Epoch 4/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8128Epoch 00004: val_loss improved from 0.39875 to 0.39862, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 150us/step - loss: 0.4012 - acc: 0.8128 - val_loss: 0.3986 - val_acc: 0.8169\n",
      "Epoch 5/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8156Epoch 00005: val_loss improved from 0.39862 to 0.39000, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 151us/step - loss: 0.3955 - acc: 0.8157 - val_loss: 0.3900 - val_acc: 0.8179\n",
      "Epoch 6/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8185Epoch 00006: val_loss improved from 0.39000 to 0.38737, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 20s 167us/step - loss: 0.3904 - acc: 0.8185 - val_loss: 0.3874 - val_acc: 0.8193\n",
      "Epoch 7/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8193Epoch 00007: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 146us/step - loss: 0.3869 - acc: 0.8192 - val_loss: 0.3892 - val_acc: 0.8206\n",
      "Epoch 8/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8219Epoch 00008: val_loss did not improve\n",
      "120000/120000 [==============================] - 17s 145us/step - loss: 0.3822 - acc: 0.8219 - val_loss: 0.3898 - val_acc: 0.8188\n",
      "Epoch 9/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8237Epoch 00009: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 148us/step - loss: 0.3795 - acc: 0.8236 - val_loss: 0.3884 - val_acc: 0.8216\n",
      "Epoch 10/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8257Epoch 00010: val_loss improved from 0.38737 to 0.38172, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 149us/step - loss: 0.3763 - acc: 0.8257 - val_loss: 0.3817 - val_acc: 0.8226\n",
      "Epoch 11/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8261Epoch 00011: val_loss did not improve\n",
      "120000/120000 [==============================] - 17s 145us/step - loss: 0.3746 - acc: 0.8262 - val_loss: 0.3842 - val_acc: 0.8226\n",
      "Epoch 12/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8269Epoch 00012: val_loss did not improve\n",
      "120000/120000 [==============================] - 17s 143us/step - loss: 0.3718 - acc: 0.8269 - val_loss: 0.3835 - val_acc: 0.8201\n",
      "Epoch 13/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8284Epoch 00013: val_loss improved from 0.38172 to 0.38013, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.3694 - acc: 0.8283 - val_loss: 0.3801 - val_acc: 0.8222\n",
      "Epoch 14/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8296Epoch 00014: val_loss improved from 0.38013 to 0.37996, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 17s 145us/step - loss: 0.3680 - acc: 0.8296 - val_loss: 0.3800 - val_acc: 0.8239\n",
      "Epoch 15/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8314Epoch 00015: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.3645 - acc: 0.8315 - val_loss: 0.3817 - val_acc: 0.8238\n",
      "Epoch 16/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8319Epoch 00016: val_loss improved from 0.37996 to 0.37848, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 149us/step - loss: 0.3630 - acc: 0.8319 - val_loss: 0.3785 - val_acc: 0.8232\n",
      "Epoch 17/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8327Epoch 00017: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 150us/step - loss: 0.3602 - acc: 0.8326 - val_loss: 0.3802 - val_acc: 0.8230\n",
      "Epoch 18/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8335Epoch 00018: val_loss did not improve\n",
      "120000/120000 [==============================] - 19s 158us/step - loss: 0.3588 - acc: 0.8335 - val_loss: 0.3785 - val_acc: 0.8227\n",
      "Epoch 19/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8357Epoch 00019: val_loss improved from 0.37848 to 0.37652, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 150us/step - loss: 0.3571 - acc: 0.8357 - val_loss: 0.3765 - val_acc: 0.8232\n",
      "Epoch 20/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8358Epoch 00020: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.3561 - acc: 0.8357 - val_loss: 0.3799 - val_acc: 0.8227\n",
      "Epoch 21/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8368Epoch 00021: val_loss improved from 0.37652 to 0.37633, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 151us/step - loss: 0.3536 - acc: 0.8368 - val_loss: 0.3763 - val_acc: 0.8239\n",
      "Epoch 22/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8380Epoch 00022: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.3515 - acc: 0.8381 - val_loss: 0.3790 - val_acc: 0.8248\n",
      "Epoch 23/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8386Epoch 00023: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 150us/step - loss: 0.3488 - acc: 0.8386 - val_loss: 0.3795 - val_acc: 0.8217\n",
      "Epoch 24/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8401Epoch 00024: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 154us/step - loss: 0.3475 - acc: 0.8401 - val_loss: 0.3770 - val_acc: 0.8250\n",
      "Epoch 25/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8396Epoch 00025: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.3479 - acc: 0.8396 - val_loss: 0.3820 - val_acc: 0.8223\n",
      "Epoch 26/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8395Epoch 00026: val_loss did not improve\n",
      "120000/120000 [==============================] - 19s 155us/step - loss: 0.3458 - acc: 0.8394 - val_loss: 0.3767 - val_acc: 0.8248\n",
      "Epoch 00026: early stopping\n",
      "Train on 120000 samples, validate on 60000 samples\n",
      "Epoch 1/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.7376Epoch 00001: val_loss improved from inf to 0.41691, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 20s 164us/step - loss: 0.4993 - acc: 0.7377 - val_loss: 0.4169 - val_acc: 0.8066\n",
      "Epoch 2/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.7999Epoch 00002: val_loss improved from 0.41691 to 0.40864, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 150us/step - loss: 0.4248 - acc: 0.8000 - val_loss: 0.4086 - val_acc: 0.8114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8074Epoch 00003: val_loss improved from 0.40864 to 0.39382, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 151us/step - loss: 0.4118 - acc: 0.8075 - val_loss: 0.3938 - val_acc: 0.8144\n",
      "Epoch 4/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8118Epoch 00004: val_loss improved from 0.39382 to 0.38927, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 152us/step - loss: 0.4031 - acc: 0.8118 - val_loss: 0.3893 - val_acc: 0.8165\n",
      "Epoch 5/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8152Epoch 00005: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 151us/step - loss: 0.3983 - acc: 0.8151 - val_loss: 0.3918 - val_acc: 0.8190\n",
      "Epoch 6/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8167Epoch 00006: val_loss improved from 0.38927 to 0.38821, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 19s 162us/step - loss: 0.3937 - acc: 0.8167 - val_loss: 0.3882 - val_acc: 0.8184\n",
      "Epoch 7/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8179Epoch 00007: val_loss improved from 0.38821 to 0.38581, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 22s 182us/step - loss: 0.3893 - acc: 0.8178 - val_loss: 0.3858 - val_acc: 0.8201\n",
      "Epoch 8/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8204- ETA: 2s - loss: 0.384Epoch 00008: val_loss improved from 0.38581 to 0.38264, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 21s 172us/step - loss: 0.3861 - acc: 0.8205 - val_loss: 0.3826 - val_acc: 0.8231\n",
      "Epoch 9/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8239Epoch 00009: val_loss improved from 0.38264 to 0.38204, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 20s 167us/step - loss: 0.3811 - acc: 0.8239 - val_loss: 0.3820 - val_acc: 0.8220\n",
      "Epoch 10/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8240Epoch 00010: val_loss improved from 0.38204 to 0.38101, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 19s 156us/step - loss: 0.3777 - acc: 0.8239 - val_loss: 0.3810 - val_acc: 0.8216\n",
      "Epoch 11/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8257Epoch 00011: val_loss did not improve\n",
      "120000/120000 [==============================] - 19s 156us/step - loss: 0.3761 - acc: 0.8257 - val_loss: 0.3820 - val_acc: 0.8219\n",
      "Epoch 12/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8274Epoch 00012: val_loss improved from 0.38101 to 0.37908, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 19s 158us/step - loss: 0.3740 - acc: 0.8273 - val_loss: 0.3791 - val_acc: 0.8227\n",
      "Epoch 13/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8289Epoch 00013: val_loss improved from 0.37908 to 0.37804, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 19s 157us/step - loss: 0.3716 - acc: 0.8288 - val_loss: 0.3780 - val_acc: 0.8240\n",
      "Epoch 14/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8296Epoch 00014: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 152us/step - loss: 0.3687 - acc: 0.8295 - val_loss: 0.3785 - val_acc: 0.8217\n",
      "Epoch 15/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8308Epoch 00015: val_loss improved from 0.37804 to 0.37595, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.3674 - acc: 0.8308 - val_loss: 0.3760 - val_acc: 0.8246\n",
      "Epoch 16/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8322Epoch 00016: val_loss improved from 0.37595 to 0.37277, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 19s 154us/step - loss: 0.3647 - acc: 0.8322 - val_loss: 0.3728 - val_acc: 0.8262\n",
      "Epoch 17/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8328Epoch 00017: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 154us/step - loss: 0.3633 - acc: 0.8328 - val_loss: 0.3737 - val_acc: 0.8234\n",
      "Epoch 18/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8350Epoch 00018: val_loss did not improve\n",
      "120000/120000 [==============================] - 18s 151us/step - loss: 0.3608 - acc: 0.8350 - val_loss: 0.3751 - val_acc: 0.8247\n",
      "Epoch 19/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8347Epoch 00019: val_loss did not improve\n",
      "120000/120000 [==============================] - 19s 160us/step - loss: 0.3584 - acc: 0.8347 - val_loss: 0.3738 - val_acc: 0.8267\n",
      "Epoch 20/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8362Epoch 00020: val_loss did not improve\n",
      "120000/120000 [==============================] - 20s 166us/step - loss: 0.3569 - acc: 0.8362 - val_loss: 0.3760 - val_acc: 0.8235\n",
      "Epoch 21/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8374Epoch 00021: val_loss did not improve\n",
      "120000/120000 [==============================] - 21s 172us/step - loss: 0.3555 - acc: 0.8374 - val_loss: 0.3760 - val_acc: 0.8238\n",
      "Epoch 00021: early stopping\n",
      "Train on 120000 samples, validate on 60000 samples\n",
      "Epoch 1/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.7367Epoch 00001: val_loss improved from inf to 0.41743, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 23s 188us/step - loss: 0.4985 - acc: 0.7369 - val_loss: 0.4174 - val_acc: 0.8051\n",
      "Epoch 2/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8007Epoch 00002: val_loss improved from 0.41743 to 0.40200, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 153us/step - loss: 0.4237 - acc: 0.8007 - val_loss: 0.4020 - val_acc: 0.8130\n",
      "Epoch 3/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8081Epoch 00003: val_loss improved from 0.40200 to 0.39745, saving model to best_dd_model.hdf5\n",
      "120000/120000 [==============================] - 18s 150us/step - loss: 0.4116 - acc: 0.8081 - val_loss: 0.3974 - val_acc: 0.8157\n",
      "Epoch 4/60\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8115"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "width = 200\n",
    "depth = 6\n",
    "epochs = 60\n",
    "n_folds = 3\n",
    "split = 0.9\n",
    "dropout_rate=0.4\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = dd(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu')\n",
    "#print(model.summary())\n",
    "\n",
    "final_model, cv_scores, histories = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# investigating range of weights in model\n",
    "weights = []\n",
    "for layer in final_model.layers:\n",
    "    w = layer.get_weights()\n",
    "    weights.append(w)\n",
    "    #print(\"mean:\", np.mean(w))\n",
    "    #print(\"std:\", np.std(w))\n",
    "    #print(\"max,min:\", (np.max(w),np.min(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Investigate history and cv_scores\n",
    "print(histories[0].history)\n",
    "print(cv_scores)\n",
    "print(np.mean([cv_score[1] for cv_score in cv_scores]))\n",
    "\n",
    "conf = final_model.get_config()\n",
    "print(type(conf))\n",
    "final_model.to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.arange(9).reshape((3, 3))\n",
    "print(arr)\n",
    "np.random.shuffle(arr)\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new FULL_corpus if Hedda finds a better one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, nr_pos_tweets, nr_neg_tweets, total_training_tweets=HL.get_corpus(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_={'hashtag': True, 'segmentation_hash': True,'hugs_and_kisses':True,'all_smilies':True,\n",
    "        'numbers':True,'number_mention':True,'elongation':True, 'set_to_not':True,'exclamation':True}\n",
    "\n",
    "better_corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "\n",
    "better_corpus_with_ngrams = HL.creating_n_grams_corpus(2,better_corpus)\n",
    "del better_corpus\n",
    "\n",
    "stopwords= TO.get_dynamic_stopwords(full_corpus, MinDf=0.00001, MaxDf=,sublinearTF=True,useIDF=False)\n",
    "\n",
    "stopword_corpus=TO.remove_stopwords(better_corpus_with_ngrams, stopwords)\n",
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(better_corpus_with_ngrams, open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jesus_corpus = pickle.load( open( \"FULL_so_far_best_corpus.pkl\", \"rb\" ) )\n",
    "print(len(jesus_corpus))\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"should make this to plot the history of epochs and validationscore\n",
    "    maybe even the crossvalidation mean of at each epoch? smoothen out the graph :)\n",
    "    \n",
    "    - make history into dataframe that fits seaborn\n",
    "    - epoch on the x axis\n",
    "    - score on the y axix (0-1)\n",
    "    - plot val_los, val_acc, train_acc and train_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sb.set(style=\"darkgrid\")\n",
    "\n",
    "    # Load the long-form example gammas dataset\n",
    "    gammas = sns.load_dataset(\"gammas\")\n",
    "\n",
    "    # Plot the response with standard error\n",
    "    sb.tsplot(data=gammas, time=\"timepoint\", unit=\"subject\",\n",
    "           condition=\"ROI\", value=\"BOLD signal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of how to save and load model\n",
    "from keras.models import load_model\n",
    "\n",
    "final_model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del final_model  # deletes the existing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "jesus_model = load_model('my_model.h5')## Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(jesus_model)\n",
    "\n",
    "# further train it\n",
    "allX = train_document_vecs\n",
    "allY = labels\n",
    "\n",
    "improved_model, history = GV.train_NN(jesus_model, allX, allY, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify the bitches\n",
    "epochs = 4\n",
    "n_folds = 2\n",
    "\n",
    "model_scores= GV.run_k_fold([final_model], train_document_vecs, labels, epochs, n_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(final_model)\n",
    "model = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "print(\"Hello world\")\n",
    "pred=model.predict(test_document_vecs)\n",
    "\n",
    "pred_ones=[]\n",
    "for i in pred:\n",
    "    if i> 0.5:\n",
    "        pred_ones.append(1)\n",
    "    else:\n",
    "        pred_ones.append(-1)\n",
    "\n",
    "#CREATING SUBMISSION\n",
    "ids = list(range(1,10000+1))\n",
    "HL.create_csv_submission(ids, pred_ones,\"testing_with_unseen_as_val_acc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT RELEVANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, hyperparameters = hyperparameters_improver(dd, train_document_vecs, labels, init_hyperparameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_improver(model, X, Y, init_hyper, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1):\n",
    "    input_dimensions = X.shape[1]\n",
    "    hypers = init_hyper\n",
    "    \n",
    "    time_const = 0.1\n",
    "    loss_const = 2\n",
    "    acc_const = 0.5\n",
    "    \n",
    "    old_state = 999\n",
    "    state_hist = []\n",
    "    acc_hist = []\n",
    "    hyper_hist = [] # list of tuples like [('depth', 1), ('width', 0)...] where 1 is change up, and vice versa\n",
    "    \n",
    "    \n",
    "    curr_hyper, hyper_value = choose_hyperparameter(hypers)\n",
    "    \n",
    "    count = 0\n",
    "    while count < 2:\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model = model(input_dimensions, hypers['width'], hypers['depth'], hypers['dropout_rate'],\n",
    "                      activation=activation)\n",
    "        final_model, cv_scores = GV.testing_for_dd(model, train_document_vecs, labels, epochs, n_folds, split)\n",
    "        time_used = time.time() - start\n",
    "        \n",
    "        state, time_dominant, acc = score_state(cv_scores, time_used)\n",
    "        state_hist.append(state)\n",
    "        acc_hist.append(acc)\n",
    "        \n",
    "        act_on_move(state_hist, hypers, hyper_hist)\n",
    "        \n",
    "        old_state = state\n",
    "        count +=1\n",
    "        \n",
    "    return model, hyperparameters, stepsizes\n",
    "\n",
    "def choose_hyperparameter(hypers):\n",
    "    \"\"\" choose random \"\"\"\n",
    "    key = random.choice(hypers.keys())\n",
    "    print(\"is this a key:\", key)\n",
    "    return key\n",
    "    \n",
    "def choose_new_value(hypers, hyper_hist):\n",
    "    \"\"\" choose either randomly either up or down, stepsize\"\"\"\n",
    "    key = choose_hyperparameter(hypers)\n",
    "    up_or_down = random.choice([0,1])\n",
    "    \n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "        \n",
    "    return key, \n",
    "\n",
    "def update_value(hypers, key, up_or_down):\n",
    "    if up_or_down:\n",
    "        hypers[key] += step_sizes[key]\n",
    "        hyper_hist.append((key, 1))\n",
    "    else:\n",
    "        hypers[key] -= step_sizes[key]\n",
    "        hyper_hist.append((key, 0))\n",
    "\n",
    "def act_on_move(state_hist, hypers, hyper_hist):\n",
    "    \"\"\"if bad move -> half decrease stepsize and go back a bit(half the distance)\n",
    "    if good...really good ---> do again or \n",
    "    barely good --> do other hyperparameter\"\"\"\n",
    "    last_change = state_hist[-1] > - state_hist[-2]\n",
    "    # was the last move a backward move?\n",
    "    #unchanged_hypers = \n",
    "    #regret_move = True if (hyper_hist[-1][0] == \n",
    "    # if the last two moves have been done on the last \n",
    "    #if (hyper_hist[-1][0] == hyper_hist[-1][0]):\n",
    "        \n",
    "        \n",
    "    if (last_change >0.02):\n",
    "        key = hyper_hist[-1][0]\n",
    "        update_value(hypers, key, up_or_down)\n",
    "    else:\n",
    "        choose_new_value(hypers, hyper_hist)\n",
    "\n",
    "def score_state(cv_scores, time_used, ):\n",
    "    \"\"\" low score is good \"\"\"\n",
    "    time_punishment = time_sensitivity*time_punish_constant*time_used\n",
    "    loss_punishment = np.mean(cv[:,0]) * loss_const\n",
    "    acc = np.mean(cv[:,1])\n",
    "    acc_punishment = acc_const/ acc\n",
    "    time_dominant = round(time_punishment/(loss_punishment + acc_punishment)-0.5)\n",
    "    return (loss_punishment + acc_punishment + time_punishment), time_dominant, acc   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural net:\n",
    "dd = NN.dynamic_dense\n",
    "X = train_document_vecs\n",
    "Y = labels\n",
    "\n",
    "init_hyperparameters = {'width':200, 'depth':5, 'dropout_rate':0.2}\n",
    "step_sizes = {'width':10, 'depth':1, 'dropout_rate':0.025}\n",
    "\n",
    "model, hyperparameters, stepsizes = hyperparameter_improver(model, X, Y, init_hyperparameters, step_sizes, epochs=3, n_folds=2, split=0.7, activation='relu',\n",
    "                            epoch_threshold=2, time_sensitivity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
