{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of complex Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = \"gensim_data_folder\"\n",
    "DATA_25DIM = DATA_FOLDER + \"/gensim_glove_vectors_25dim.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/gensim_glove_vectors_50dim.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/gensim_glove_vectors_100dim.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/gensim_glove_vectors_200dim.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading corpus\n",
    "\n",
    "awesome_corpus = pickle.load( open( \"stopword100_corpus_n2_SHM_E_SN_H_HK.pkl\", \"rb\" ) )\n",
    "print(len(awesome_corpus))\n",
    "\n",
    "nr_pos_tweets = 1250000\n",
    "nr_neg_tweets = 1250000\n",
    "total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "###### build vectors of all the tweets ######\n",
    "num_of_dim = global_vectors.syn0.shape[1]\n",
    "# seperate traindata and testdata\n",
    "train_corpus = processed_corpus[:total_training_tweets:] \n",
    "predict_corpus = processed_corpus[total_training_tweets::]\n",
    "del processed_corpus\n",
    "\n",
    "# Build a vector of all the words in a tweet\n",
    "vectors = np.zeros(len(train_corpus), dtype=object)\n",
    "for i, doc in enumerate(train_corpus):\n",
    "    if (i % 50000) == 0:\n",
    "        print(\"tweets processed: %.0f  of total number of tweets: %.0f\" % (i,len(train_corpus)))\n",
    "    vectors[i] = GV.buildWordVector(doc, num_of_dim, global_vectors)\n",
    "del global_vectors\n",
    "del doc\n",
    "print(\"done with making the trainvectors\")\n",
    "\n",
    "train_document_vecz = np.concatenate(vectors)\n",
    "del vectors\n",
    "print(\"done with concatenating the trainvectors\")\n",
    "\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecz)\n",
    "del train_document_vecz\n",
    "print(\"done with scaling the trainvectors\")\n",
    "\n",
    "labels = GV.create_labels(total_training_tweets, nr_pos_tweets)\n",
    "print(\"done with creating the labels\")\n",
    "print(\"time used one the ordeal:\", time.time() - start)\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defing model :)\n",
    "input_dimensions = train_document_vecs.shape[1]\n",
    "width = 500\n",
    "depth = 2\n",
    "epochs = 60\n",
    "n_folds = 2\n",
    "split = 0.9\n",
    "dropout_rate=0.4\n",
    "funnel=0.3\n",
    "\n",
    "#model = NN.deep_HB(input_dimensions)\n",
    "model = NN.dynamic_dense(input_dimensions, width, depth, dropout_rate=dropout_rate, activation='relu', funnel=funnel)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model on dataset\n",
    "This can take a while, it should stop with early stopping(patience=10!), but it can can be stopped prematurely by Interrupting kernel. Then it return the last model it was working on(BUT NOT NECESSARILY THE BEST). Use the \"train_NN_dynamic_model.hdf5\" that is saved by ModelCheckpoint. Maybe try out once before running it for a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "improved_model, history = GV.train_NN(model, train_document_vecs, labels, epochs=1)\n",
    "# backuppickle in case something goes wrong\n",
    "improved_model.save('Backup_of_further_training_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Kaggle when you are happy with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR THE KAGGLE SUBMISSION\n",
    "test_document_vecs = np.concatenate([GV.buildWordVector(doc, num_of_dim, global_vectors) for doc in predict_corpus])\n",
    "test_document_vecs = sk.preprocessing.scale(test_document_vecs)\n",
    "\n",
    "print(\"Hello world\")\n",
    "pred=model.predict(test_document_vecs)\n",
    "\n",
    "pred_ones=[]\n",
    "for i in pred:\n",
    "    if i> 0.5:\n",
    "        pred_ones.append(1)\n",
    "    else:\n",
    "        pred_ones.append(-1)\n",
    "\n",
    "#CREATING SUBMISSION\n",
    "ids = list(range(1,10000+1))\n",
    "HL.create_csv_submission(ids, pred_ones,\"best_proc_corpus_dynamic_dense.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
