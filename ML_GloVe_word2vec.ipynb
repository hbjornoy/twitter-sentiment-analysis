{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideas\n",
    "- We *could* do PCA to vizualize word2vec technology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe with keras classification\n",
    "##### Content:\n",
    "- Import pretrained GloVe vectorspace\n",
    "- Import our own data\n",
    "- classify with keras FFNN(feedforward)\n",
    "\n",
    "\n",
    "##### possible additional steps\n",
    "- clustering\n",
    "- preprossessing\n",
    "- tf-idf\n",
    "- experiment with different neural networks\n",
    "- PCA vizualize vectorspace\n",
    "- visualize end results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\"\n",
    "#gensim_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY NEED TO THIS THE FIRST TIME ONE IMPORTS THE PRETRAINED GLOVE\n",
    "# Creates a gensim_word2vec_file in the same folder\n",
    "\n",
    "GV.create_gensim_word2vec_file(DATA_25DIM)\n",
    "#GV.create_gensim_word2vec_file(DATA_50DIM)\n",
    "#GV.create_gensim_word2vec_file(DATA_100DIM)\n",
    "#GV.create_gensim_word2vec_file(DATA_200DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.method1(\"gensim_global_vectors_100dim.txt\", full_corpus, total_training_tweets, nr_pos_tweets, all_neural_nets=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "global_vectors = GV.make_glove(\"gensim_global_vectors_50dim.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# supposed to show queen, to demonstrate the power of word2vec, but failed #blameTwitterDataset\n",
    "global_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get own data ready for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING COMPLETE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models2 = [NN.basic_model, NN.basic_model_adam, NN.wide_model, NN.deep_2_model, NN.deep_HB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(models2, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing hedda-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tolken\n",
    "tokenized_full_corpus = tolken.replace_words(full_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_corpus = HL.creating_n_grams_cropus(corpus=tokenized_full_corpus, n_gram=2) #2GramsForLife "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labels for the training files. Used to perform validation of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making labels\n",
    "labels = np.zeros(total_training_tweets);\n",
    "labels[0:nr_pos_tweets]=1;\n",
    "labels[nr_pos_tweets:total_training_tweets]=0; \n",
    "\n",
    "print(\"labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the corpus into train and prediction - parts\n",
    "\n",
    "We're done training the word2vec, so all \"common\" operations are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SPLITTING THE W2V model into training and predict\n",
    "train_clusterised_corpus = tokenized_full_corpus[:total_training_tweets:]\n",
    "predict_clusterised_corpus = tokenized_full_corpus[total_training_tweets::]\n",
    "\n",
    "print(len(train_clusterised_corpus))\n",
    "print(len(predict_clusterised_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_document_vecs = np.concatenate([GV.buildWordVector(z, global_vectors.syn0.shape[1], global_vectors) for z in train_clusterised_corpus])\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Train w2v shape:\",train_document_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural net classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TING FOR Å SIKRE REPRODUSERBARHET ( ikke alt er nødv. nødvendig )\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(12345) # NO IDEA WHAT THIS DOES\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some neural net models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossvalidate this bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_k_fold(models, X, Y, epochs, n_folds, seed):\n",
    "    \n",
    "    for neural_model in models:\n",
    "        \n",
    "        model_name = neural_model.__name__\n",
    "        \n",
    "        model = neural_model()\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        kfold = sklearn.model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train, test in kfold.split(X, Y):\n",
    "            \n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            \n",
    "            model.fit(X[train], Y[train], epochs=epochs, batch_size=1024, verbose=0, callbacks=[early_stopping])\n",
    "            \n",
    "            score = model.evaluate(X[test], Y[test], verbose=0)\n",
    "            cv_scores.append(score)\n",
    "        \n",
    "        \n",
    "        # To analyze if it is unbalanced classifying\n",
    "        labels = Y[test]\n",
    "        pred = model.predict(X[test])\n",
    "        pos_right = 0\n",
    "        neg_right = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == 1 and label == pred[i]:\n",
    "                pos_right += 1\n",
    "            elif label == 0 and label == pred[i]:\n",
    "                neg_right += 1\n",
    "        pos_perc = pos_right/(len(labels)*0.5)\n",
    "        neg_perc = neg_right/(len(labels)*0.5)\n",
    "        \n",
    "        print(\"Model: \", model_name)\n",
    "        print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "        print(\"Negative sentiment: %.2f%%  Positive sentiment: %.2f%%\" % (neg_perc, pos_perc))\n",
    "        print(\"Time taken: \", (time.time() - start) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [ basic_model, basic_model_adam, wide_model, deep_1_model, deep_2_model, deep_HB]#, recurrent_model, convolutional_model]\n",
    "\n",
    "run_k_fold(models, train_document_vecs, labels, epochs=10, n_folds=3, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores = []\n",
    "neg_scores = []\n",
    "labs = [0,1,1,1,1,0]\n",
    "prd = [0.32,0.49,0.50,0.6232,0.81,0.9]\n",
    "pos_right = 0\n",
    "neg_right = 0\n",
    "for i, lab in enumerate(labs):\n",
    "    if lab == 1 and prd[i] >=0.5:\n",
    "        pos_right += 1\n",
    "        \n",
    "    elif lab == 0 and prd[i] < 0.5:\n",
    "        neg_right += 1\n",
    "pos_scores.append((pos_right / (len(labs) * 0.5))*100)\n",
    "neg_scores.append((neg_right / (len(labs) * 0.5))*100)\n",
    "print(\"neggies:\", np.mean(neg_scores))\n",
    "print(\"possies:\", np.mean(pos_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_labs = np.array(labs)\n",
    "n_prd = np.array(prd)\n",
    "n_prd = np.round(prd)\n",
    "print(n_prd.astype(int))\n",
    "arg1 = np.argwhere(n_labs == 1)\n",
    "arg0 = np.argwhere(n_labs == 0)\n",
    "neg_\n",
    "\n",
    "print(pos_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Negative sentiment: %.2f%%  Positive sentiment: %.2f%%\" % labels[i==0][np.argmax(pred)]/len(labels))#, pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models2 = [NN.basic_model, NN.basic_model_adam, NN.wide_model, NN.deep_2_model, NN.deep_HB]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GV.classify_with_neural_networks(models, global_vectors, processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
