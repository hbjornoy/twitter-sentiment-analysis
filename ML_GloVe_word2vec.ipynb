{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideas\n",
    "- We *could* do PCA to vizualize word2vec technology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe with keras classification\n",
    "##### Content:\n",
    "- Import pretrained GloVe vectorspace\n",
    "- Import our own data\n",
    "- classify with keras FFNN(feedforward)\n",
    "\n",
    "\n",
    "##### possible additional steps\n",
    "- clustering\n",
    "- preprossessing\n",
    "- tf-idf\n",
    "- experiment with different neural networks\n",
    "- PCA vizualize vectorspace\n",
    "- visualize end results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim as gs\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"..\", \"..\",\"glove_twitter_27B\") #Spesialt for min location..opsops\n",
    "DATA_25DIM = DATA_FOLDER + \"/glove_twitter_27B_25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove_twitter_27B_50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove_twitter_27B_100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove_twitter_27B_200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spits out a .txt-file with the vectors in gensim format\n",
    "glove2word2vec(glove_input_file=DATA_50DIM, word2vec_output_file=\"gensim_glove_vectors_50dim.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = gs.models.KeyedVectors.load_word2vec_format(\"gensim_glove_vectors_50dim.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.similar_by_word(\"racism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supposed to show queen, to demonstrate the power of word2vec, but failed #blameTwitterDataset\n",
    "glove_model.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how one can use a multiple words with the same meaning to predict similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', 0.9476158618927002),\n",
       " (\"n't\", 0.9369821548461914),\n",
       " ('like', 0.9318934679031372),\n",
       " ('just', 0.9307540059089661),\n",
       " ('really', 0.9307530522346497),\n",
       " ('why', 0.9288229942321777),\n",
       " ('hate', 0.9266778826713562),\n",
       " ('know', 0.9264548420906067),\n",
       " ('that', 0.9209468364715576),\n",
       " ('when', 0.9199150204658508)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = create_topic_vector(['i', 'fucking', 'hate', 'trump'])\n",
    "glove_model.similar_by_vector(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing as ONLY wordvectors to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vectors = glove_model.wv\n",
    "del glove_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get own data ready for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens.split():\n",
    "        try:       \n",
    "            word = word.decode('utf-8')\n",
    "            word_vec = model[word].reshape((1, size))             \n",
    "            #idf_weighted_vec = word_vec * tfidf_dict[word]\n",
    "            vec += word_vec\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterableCorpus():\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tweet in self.corpus:\n",
    "            tweet_words = tweet.split()\n",
    "            yield [word.decode('utf-8') for word in tweet_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global variables\n",
    "N_DIMENSIONS = 50 #the global_vector_space is 50dim\n",
    "test_set_tweets = 10000\n",
    "\n",
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing hedda-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tolken\n",
    "tokenized_full_corpus = tolken.replace_words(full_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_corpus = HL.creating_n_grams_cropus(corpus=tokenized_full_corpus, n_gram=4) #2GramsForLife "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labels for the training files. Used to perform validation of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape:  (200000,)\n"
     ]
    }
   ],
   "source": [
    "#Making labels\n",
    "labels = np.zeros(total_training_tweets);\n",
    "labels[0:nr_pos_tweets]=1;\n",
    "labels[nr_pos_tweets:total_training_tweets]=0; \n",
    "\n",
    "print(\"labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the corpus into train and prediction - parts\n",
    "\n",
    "We're done training the word2vec, so all \"common\" operations are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# SPLITTING THE W2V model into training and predict\n",
    "train_clusterised_corpus = tokenized_full_corpus[:total_training_tweets:]\n",
    "predict_clusterised_corpus = tokenized_full_corpus[total_training_tweets::]\n",
    "\n",
    "print(len(train_clusterised_corpus))\n",
    "print(len(predict_clusterised_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_document_vecs = np.concatenate([buildWordVector(z, N_DIMENSIONS, global_vectors) for z in train_clusterised_corpus])\n",
    "train_document_vecs = sk.preprocessing.scale(train_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train w2v shape: (200000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train w2v shape:\",train_document_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural net classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TING FOR Å SIKRE REPRODUSERBARHET ( ikke alt er nødv. nødvendig )\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(12345) # NO IDEA WHAT THIS DOES\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def basic_model_adam():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def wide_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(150, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_1_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_2_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_HB():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(150, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(100, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(80, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(40, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(20, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(40, input_dim=N_DIMENSIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n",
    "def recurrent_model():\n",
    "    # Start neural network\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add an embedding layer\n",
    "    model.add(Embedding(input_dim=N_DIMENSIONS, output_dim=128))\n",
    "\n",
    "    # Add a long short-term memory layer with 128 units\n",
    "    model.add(LSTM(units=128))\n",
    "\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile neural network\n",
    "    model.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                optimizer='Adam', # Adam optimization\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "    return model\n",
    "\n",
    "def convolutional_model():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same', input_shape=(133332, N_DIMENSIONS)))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossvalidate this bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(models, X, Y, epochs, n_folds, seed):\n",
    "    \n",
    "    for neural_model in models:\n",
    "        \n",
    "        model_name = neural_model.__name__\n",
    "        \n",
    "        model = neural_model()\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        kfold = sklearn.model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train, test in kfold.split(X, Y):\n",
    "            \n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            \n",
    "            model.fit(X[train], Y[train], epochs=epochs, batch_size=1024, verbose=0, callbacks=[early_stopping])\n",
    "            \n",
    "            scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "        \n",
    "        \n",
    "        # To analyze if it is unbalanced classifying\n",
    "        labels = Y[test]\n",
    "        pred = model.predict(X[test])\n",
    "        pos_right = 0\n",
    "        neg_right = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == 1 and label == pred[i]:\n",
    "                pos_right += 1\n",
    "            elif label == 0 and label == pred[i]:\n",
    "                neg_right += 1\n",
    "        pos_perc = pos_right/(len(labels)*0.5)\n",
    "        neg_perc = neg_right/(len(labels)*0.5)\n",
    "        print(\"Model: \", model_name)\n",
    "        print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "        print(\"Negative sentiment: %.2f%%  Positive sentiment: %.2f%%\" % (neg_perc, pos_perc))\n",
    "        print(\"Time taken: \", (time.time() - start) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  basic_model\n",
      "nan% (+/- nan%)\n",
      "Negative sentiment: 0.00%  Positive sentiment: 0.00%\n",
      "Time taken:  0.4995209455490112 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/havardbjornoy/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/havardbjornoy/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/havardbjornoy/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/havardbjornoy/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/havardbjornoy/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  basic_model_adam\n",
      "nan% (+/- nan%)\n",
      "Negative sentiment: 0.00%  Positive sentiment: 0.00%\n",
      "Time taken:  0.48916691541671753 \n",
      "\n",
      "Model:  wide_model\n",
      "nan% (+/- nan%)\n",
      "Negative sentiment: 0.00%  Positive sentiment: 0.00%\n",
      "Time taken:  0.545284636815389 \n",
      "\n",
      "Model:  deep_1_model\n",
      "nan% (+/- nan%)\n",
      "Negative sentiment: 0.00%  Positive sentiment: 0.00%\n",
      "Time taken:  0.6302354017893473 \n",
      "\n",
      "Model:  deep_2_model\n",
      "nan% (+/- nan%)\n",
      "Negative sentiment: 0.00%  Positive sentiment: 0.00%\n",
      "Time taken:  0.6828426122665405 \n",
      "\n",
      "Model:  deep_HB\n",
      "nan% (+/- nan%)\n",
      "Negative sentiment: 0.00%  Positive sentiment: 0.00%\n",
      "Time taken:  1.1112324635187785 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [ basic_model, basic_model_adam, wide_model, deep_1_model, deep_2_model, deep_HB]#, recurrent_model, convolutional_model]\n",
    "\n",
    "run_k_fold(models, train_document_vecs, labels, epochs=10, n_folds=3, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negative sentiment: %.2f%%  Positive sentiment: %.2f%%\" % labels[i==0][np.argmax(pred)]/len(labels))#, pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
