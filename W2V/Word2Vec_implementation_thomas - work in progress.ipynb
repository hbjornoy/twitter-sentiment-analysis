{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim \n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "import time\n",
    "\n",
    "import helpers as HL\n",
    "import cleaning as CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['train_pos.txt', 'train_neg.txt'] \n",
    "\n",
    "corpus, file_lengths = HL.create_corpus(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_file_path = '50mpaths2.txt'\n",
    "cluster_dict = CL.create_dictionary(cluster_file_path)\n",
    "\n",
    "clusterised_corpus = CL.create_clusterized_corpus(corpus, cluster_dict)\n",
    "\n",
    "#clusterised_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterableCorpus():\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tweet in self.corpus:\n",
    "            tweet_words = tweet.split()\n",
    "            yield [word.decode('utf-8') for word in tweet_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterable_corpus = IterableCorpus(clusterised_corpus)\n",
    "\n",
    "# SMÃ†KK INN HYPERPARAMETERE HER!!! \n",
    "w2v_model = Word2Vec(iterable_corpus, size=100, window=5, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training-set numbers\n",
    "nr_of_positive_training_examples = 100000\n",
    "nr_of_training_examples = 200000\n",
    "\n",
    "#Making labels\n",
    "labels = np.zeros(nr_of_training_examples);\n",
    "labels[0:nr_of_positive_training_examples]=1;\n",
    "labels[nr_of_positive_training_examples:nr_of_training_examples]=0; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(clusterised_corpus),\n",
    "                                                    np.array(labels), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we have our word2vec model, giving us a representation for each word in the whole corpus. We now need to create a representation of each Tweet in the dataset. \n",
    "\n",
    "We will do this with the following steps: \n",
    "- Creating a TFIDF model for each word\n",
    "- Combining the word2vec-vectors for each word, weighting them by their TFIDF score\n",
    "- Run that shit through a neural network fuck yeaaaaah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a vectorizer to use for Tf-IDF analysis on words\n",
    "vectorizer = TfidfVectorizer(\n",
    "        min_df = 10, # removing word that occure less then 10 times \n",
    "        max_df = 1.5, # remove words that are too frequent ( more then 1.5 * number of tweets )\n",
    "        sublinear_tf=True, # scale the term frequency in logarithmic scale\n",
    "        use_idf =True\n",
    "        #stop_words = custom_stop_words # Removing stop-words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tf_idf = vectorizer.fit_transform(clusterised_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200000x1470 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2741796 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  1470\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "print('Size: ', len(tfidf_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Now we want to create a vector for each tweet, by combining the word vecs weighted by the tfidf-dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens.split():\n",
    "        try:       \n",
    "            word = word.decode('utf-8')\n",
    "            word_vec = w2v_model[word].reshape((1, size))             \n",
    "            idf_weighted_vec = word_vec * tfidf_dict[word]\n",
    "            vec += idf_weighted_vec\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160000/160000 [==============================] - 9s 58us/step - loss: 0.4814 - acc: 0.7555\n",
      "Epoch 2/5\n",
      "160000/160000 [==============================] - 9s 53us/step - loss: 0.4535 - acc: 0.7764: 0s - loss: 0.4535 - acc: 0.\n",
      "Epoch 3/5\n",
      "160000/160000 [==============================] - 9s 55us/step - loss: 0.4447 - acc: 0.7824\n",
      "Epoch 4/5\n",
      "160000/160000 [==============================] - 9s 58us/step - loss: 0.4396 - acc: 0.7865:\n",
      "Epoch 5/5\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.4365 - acc: 0.7883\n",
      "40000/40000 [==============================] - 1s 13us/step\n",
      "[0.44134737029075621, 0.78385000000000005]\n"
     ]
    }
   ],
   "source": [
    "# NEEDS TO BE THE SAME AS WHEN CREATING WORD2VEC\n",
    "n_dim = 100\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in x_train])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in x_test])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "      loss='binary_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
