{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating files for submission to Kaggle  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "# internal imports\n",
    "\n",
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import pickle\n",
    "\n",
    "#FOR THOMAS\n",
    "#import tokenizing as TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, nr_pos_tweets, nr_neg_tweets, total_training_tweets = HL.get_corpus(full=False, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "global_vectors=HL.get_global_vectors(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing unprepocessed full dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/100\n",
      "156672/160000 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.7727Epoch 00001: val_loss improved from inf to 0.41368, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 9us/step - loss: 0.4573 - acc: 0.7733 - val_loss: 0.4137 - val_acc: 0.8027\n",
      "Epoch 2/100\n",
      "155648/160000 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8119Epoch 00002: val_loss improved from 0.41368 to 0.39939, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 8us/step - loss: 0.4010 - acc: 0.8118 - val_loss: 0.3994 - val_acc: 0.8116\n",
      "Epoch 3/100\n",
      "156672/160000 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8190Epoch 00003: val_loss improved from 0.39939 to 0.39421, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 8us/step - loss: 0.3885 - acc: 0.8189 - val_loss: 0.3942 - val_acc: 0.8139\n",
      "Epoch 4/100\n",
      "156672/160000 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8230Epoch 00004: val_loss improved from 0.39421 to 0.39095, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3810 - acc: 0.8229 - val_loss: 0.3910 - val_acc: 0.8153\n",
      "Epoch 5/100\n",
      "158720/160000 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8263Epoch 00005: val_loss improved from 0.39095 to 0.38684, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3742 - acc: 0.8262 - val_loss: 0.3868 - val_acc: 0.8175\n",
      "Epoch 6/100\n",
      "158720/160000 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8300- ETA: 0s - loss: 0.366Epoch 00006: val_loss improved from 0.38684 to 0.38603, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3691 - acc: 0.8301 - val_loss: 0.3860 - val_acc: 0.8185\n",
      "Epoch 7/100\n",
      "153600/160000 [===========================>..] - ETA: 0s - loss: 0.3645 - acc: 0.8325Epoch 00007: val_loss improved from 0.38603 to 0.38495, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3639 - acc: 0.8328 - val_loss: 0.3849 - val_acc: 0.8193\n",
      "Epoch 8/100\n",
      "154624/160000 [===========================>..] - ETA: 0s - loss: 0.3600 - acc: 0.8348Epoch 00008: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3597 - acc: 0.8350 - val_loss: 0.3877 - val_acc: 0.8180\n",
      "Epoch 9/100\n",
      "156672/160000 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8378Epoch 00009: val_loss improved from 0.38495 to 0.38395, saving model to best_neural_model_save.hdf5\n",
      "160000/160000 [==============================] - 1s 8us/step - loss: 0.3562 - acc: 0.8378 - val_loss: 0.3840 - val_acc: 0.8204\n",
      "Epoch 10/100\n",
      "158720/160000 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8391Epoch 00010: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3520 - acc: 0.8391 - val_loss: 0.3868 - val_acc: 0.8185\n",
      "Epoch 11/100\n",
      "154624/160000 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8413Epoch 00011: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3493 - acc: 0.8413 - val_loss: 0.3848 - val_acc: 0.8188\n",
      "Epoch 12/100\n",
      "156672/160000 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8438Epoch 00012: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3455 - acc: 0.8437 - val_loss: 0.3855 - val_acc: 0.8193\n",
      "Epoch 13/100\n",
      "154624/160000 [===========================>..] - ETA: 0s - loss: 0.3428 - acc: 0.8443Epoch 00013: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3429 - acc: 0.8441 - val_loss: 0.3863 - val_acc: 0.8185\n",
      "Epoch 14/100\n",
      "153600/160000 [===========================>..] - ETA: 0s - loss: 0.3402 - acc: 0.8464Epoch 00014: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 8us/step - loss: 0.3403 - acc: 0.8462 - val_loss: 0.3848 - val_acc: 0.8192\n",
      "Epoch 15/100\n",
      "155648/160000 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8471Epoch 00015: val_loss did not improve\n",
      "160000/160000 [==============================] - 2s 10us/step - loss: 0.3377 - acc: 0.8469 - val_loss: 0.3898 - val_acc: 0.8176\n",
      "Epoch 16/100\n",
      "158720/160000 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8480Epoch 00016: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3358 - acc: 0.8480 - val_loss: 0.3886 - val_acc: 0.8189\n",
      "Epoch 17/100\n",
      "157696/160000 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8485Epoch 00017: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3338 - acc: 0.8486 - val_loss: 0.3896 - val_acc: 0.8178\n",
      "Epoch 18/100\n",
      "154624/160000 [===========================>..] - ETA: 0s - loss: 0.3313 - acc: 0.8511Epoch 00018: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3314 - acc: 0.8512 - val_loss: 0.3890 - val_acc: 0.8189\n",
      "Epoch 19/100\n",
      "156672/160000 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8517Epoch 00019: val_loss did not improve\n",
      "160000/160000 [==============================] - 1s 7us/step - loss: 0.3292 - acc: 0.8515 - val_loss: 0.3898 - val_acc: 0.8177\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'neural_model_prediction.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b2b7e5c907b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkaggle_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"keggle_glove_unprepro_simple.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mGV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_model_adam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_training_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_pos_tweets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkaggle_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Thomas\\EPFL\\Machine learning CS-433\\Project 2\\CD-433-Project-2\\glove_module.py\u001b[0m in \u001b[0;36mget_prediction\u001b[1;34m(neural_net, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, kaggle_name, epochs, patience, split)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[1;31m# Loading the best model found during training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'neural_model_prediction.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_document_vecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;31m# instantiate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'neural_model_prediction.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "kaggle_name=\"keggle_glove_unprepro_simple.csv\"\n",
    "pred= GV.get_prediction(NN.basic_model_adam, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets,kaggle_name, patience=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing best preprocessing with full dataset: \n",
    "## Applying the preorocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create on your own:\n",
    "\n",
    "input_=[{'hashtag': True, 'segmentation_hash': True, 'hashtag_mention':True,\n",
    "        'hearts':True,'hugs_and_kisses':True,'elongation':True, 'set_to_not':True}]\n",
    "final_corpus=TO.preprocess_corpus(full_corpus, input_)\n",
    "final_corpus_ngram=HL.creating_n_grams_corpus(2,final_corpus)\n",
    "stopwords=TO.get_dynamic_stopwords(best_corpus, MinDf=100, MaxDf=0.8)\n",
    "final_corpus_ngram_stopwords=TO.remove_stopwords(final_corpus_ngram, stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## or upload from pickle\n",
    "final_corpus_ngram_stopwords=pickle.load( open( \"stopword100_corpus_n2_SHM_E_SN_H_HK.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making kaggle submission using simple neural net: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_best_simple.csv\"\n",
    "pred= GV.get_prediction(NN.basic_model_adam, global_vectors, final_corpus_ngram_stopwords, total_training_tweets, nr_pos_tweets,kaggle_name, patience=10,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making kaggle submission using complex neural net: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_best_complex.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "pred= GV.get_prediction(NN.complex_model, global_vectors, final_corpus_ngram_stopwords, total_training_tweets, nr_pos_tweets,kaggle_name, patience=10,epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
