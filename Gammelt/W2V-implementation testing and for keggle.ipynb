{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens.split():\n",
    "        try:       \n",
    "            word = word.decode('utf-8')\n",
    "            word_vec = model[word].reshape((1, size))             \n",
    "            idf_weighted_vec = word_vec * tfidf_dict[word]\n",
    "            vec += idf_weighted_vec\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterableCorpus():\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tweet in self.corpus:\n",
    "            tweet_words = tweet.split()\n",
    "            yield [word.decode('utf-8') for word in tweet_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim \n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Conv1D, Embedding, LSTM\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import time\n",
    "\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import preprocessing as PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables [ choose sub_set or full_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global variables\n",
    "N_DIMENTIONS = 200 \n",
    "test_set_tweets = 10000\n",
    "\n",
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000\n",
    "\n",
    "#FOR FULL SET\n",
    "#corpus_filenames = ['train_pos_full.txt', 'train_neg_full.txt','test_data.txt'] \n",
    "#nr_pos_tweets = 1250000\n",
    "#nr_neg_tweets = 1250000\n",
    "#total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating clusterizing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len cluster dict: 216856\n"
     ]
    }
   ],
   "source": [
    "cluster_file_path = '50mpaths2.txt'\n",
    "cluster_dict = CL.create_dictionary(cluster_file_path)\n",
    "\n",
    "print(\"Len cluster dict:\", len(cluster_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusterizing corpus \n",
    "# clusterised_full_corpus = CL.create_clusterized_corpus(full_corpus, cluster_dict)\n",
    "\n",
    "corpus = full_corpus\n",
    "\n",
    "#Adding features\n",
    "corpus = PP.add_features(corpus)\n",
    "\n",
    "#Stemming words\n",
    "corpus = PP.stem_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating vectorizer to create idf_weighting of each word in the corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopword_list = ['and','to','the','of','in','there']\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df = 10, # removing word that occure less then 10 times \n",
    "    max_df = 0.3, # remove words that are too frequent ( more then 0.8 * number of files )\n",
    "    sublinear_tf=True, # scale the term frequency in logarithmic scale\n",
    "    max_features = 5000,\n",
    "    use_idf = True, \n",
    "    stop_words = my_stopword_list,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "corpus_tf_idf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Creating word2vec for all tweets, train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2v model: Word2Vec(vocab=10448, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#Making the corpus iterable to be able to feed it to word2vec from gensim\n",
    "iterable_full_corpus = IterableCorpus(corpus)\n",
    "\n",
    "w2v_model = Word2Vec(iterable_full_corpus, size=N_DIMENTIONS, window=5, min_count=10, workers=4)\n",
    "\n",
    "print(\"W2v model:\", w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a different form of the word2vec to use a LOT less RAM when running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labels for the training files. Used to perform validation of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making labels\n",
    "labels = HL.create_labels(nr_pos_tweets, nr_pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the corpus into train and prediction - parts\n",
    "\n",
    "We're done training the word2vec, so all \"common\" operations are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# SPLITTING THE W2V model into training and predict\n",
    "train_clusterised_corpus = corpus[:total_training_tweets:]\n",
    "predict_clusterised_corpus = corpus[total_training_tweets::]\n",
    "\n",
    "print(len(train_clusterised_corpus))\n",
    "print(len(predict_clusterised_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating document wecs for training and test-set\n",
    "\n",
    "In other words a vector for each tweet, representing the content of that tweet. Each word in each tweet is weighted by its idf-score found in the 'tfidf_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_document_vecs = np.concatenate([buildWordVector(z, N_DIMENTIONS, word_vectors) for z in train_clusterised_corpus])\n",
    "train_document_vecs = scale(train_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY FOR KAGGLE\n",
    "test_document_vecs = np.concatenate([buildWordVector(z, N_DIMENTIONS, word_vectors) for z in predict_clusterised_corpus])\n",
    "test_document_vecs = scale(test_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train w2v shape:\",train_document_vecs.shape)\n",
    "print(\"Train w2v shape:\",test_document_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the neural net classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TING FOR Å SIKRE REPRODUSERBARHET ( ikke alt er nødv. nødvendig )\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "rn.seed(12345) # NO IDEA WHAT THIS DOES\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def basic_model_adam():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def wide_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_1_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_2_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_wide():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(150, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(150, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Running some nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_k_fold(models, X, Y, epochs, n_folds, seed):\n",
    "    \n",
    "    for neural_model in models:\n",
    "        \n",
    "        model_name = neural_model.__name__\n",
    "        \n",
    "        model = neural_model()\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train, test in kfold.split(X, Y):\n",
    "            \n",
    "            early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
    "            \n",
    "            model.fit(X[train], Y[train], epochs=epochs, batch_size=1024, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\n",
    "            cv_scores.append(scores[1] * 100)\n",
    "\n",
    "        print(\"Model: \", model_name)\n",
    "        print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "        print(\"Time taken: \", (time.time() - start) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  basic_model\n",
      "79.19% (+/- 0.63%)\n",
      "Time taken:  1.8317458709081014 \n",
      "\n",
      "Model:  basic_model_adam\n",
      "79.23% (+/- 0.45%)\n",
      "Time taken:  1.8012408852577209 \n",
      "\n",
      "Model:  wide_model\n",
      "79.43% (+/- 0.69%)\n",
      "Time taken:  1.6502403140068054 \n",
      "\n",
      "Model:  deep_1_model\n",
      "79.61% (+/- 0.65%)\n",
      "Time taken:  1.7633070906003316 \n",
      "\n",
      "Model:  deep_2_model\n",
      "79.53% (+/- 0.75%)\n",
      "Time taken:  1.899647319316864 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [basic_model, basic_model_adam, wide_model, deep_1_model, deep_2_model]\n",
    "\n",
    "run_k_fold(models, train_document_vecs, labels, epochs=10, n_folds=3, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  deep_wide\n",
      "77.28% (+/- 0.79%)\n",
      "Time taken:  2.010883013407389 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models=[deep_wide]\n",
    "run_k_fold(models, train_document_vecs, labels, epochs=30, n_folds=2, seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff for making keggle predictions! choose the best model to be 'model', then run .predict! :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_vecs_w2v)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounded = [-1 if round(x[0])==0 else 1 for x in predictions]\n",
    "\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = list(range(1,test_set_tweets+1))\n",
    "y_pred = rounded\n",
    "name = \"keggle_submission_neural.csv\"\n",
    "\n",
    "HL.create_csv_submission(ids, y_pred, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF, PROBABLY NOT TO BE USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KAN BRUKES TIL NÅR MAN SKAL GJØRE EN MODEL FIT, NÅR MAN HAR FUNNET RIKTIG MODELL Å BRUKE OVER\n",
    "\n",
    "# Smaller batch equalls longer run times, but less epochs for convergence \n",
    "# DENNE GA 80% PÅ KAGGLE\n",
    "\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "#checkpointer = ModelCheckpoint(filepath='tmp/saved_weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "#model_4 = Sequential()\n",
    "#model_4.add(Dropout(0.05, input_shape=(N_DIMENTIONS,)))\n",
    "#model_4.add(Dense(100, activation='relu', input_dim=N_DIMENTIONS))\n",
    "#model_4.add(Dense(60, activation='relu'))\n",
    "#model_4.add(Dense(30, activation='relu'))\n",
    "#model_4.add(Dropout(0, input_shape=(N_DIMENTIONS,)))\n",
    "#model_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "#model_4.compile(optimizer='rmsprop',\n",
    "#      loss='binary_crossentropy',\n",
    "#      metrics=['accuracy'])\n",
    "\n",
    "#model_4.fit(training_vecs_w2v, labels, epochs=30, batch_size=1024, verbose=1, callbacks=[early_stopping, checkpointer], validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#best_model = load_model('tmp/saved_weights.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
