{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining best combination of preprocessing techniques  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "#import tokenizing as TO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "global_vectors=HL.get_global_vectors(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:\n",
    "In addition to the acutal corpus, some additional information is needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus, nr_pos_tweets, nr_neg_tweets, total_training_tweets=HL.get_corpus(full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets=[NN.basic_model_adam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables to apply all preprocessing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing vectors:\n",
    "corpuses=[]\n",
    "corpuses.append(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining names of corpuses: \n",
    "names=['original_corpus','SH_corpus','SHM_corpus','H_corpus','HK_corpus','PS_corpus','NS__corpus','OS_corpus','N_corpus','NM_corpus','ST_corpus','SP_corpus','E_corpus','SN_corpus','RS_corpus','EX_corpus','N-2_corpus','N-3_corpus','N-4_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining inputs to preprocessing function: \n",
    "inputs=[{'hashtag': True, 'segmentation_hash': True},\n",
    "        {'hashtag':True,'segmentation_hash': True,'hashtag_mention':True},\n",
    "        {'hearts':True},\n",
    "        {'hugs_and_kisses':True},\n",
    "        {'pos_smilies':True},\n",
    "        {'neg_smilies':True},\n",
    "        {'other_smilies':True},\n",
    "        {'numbers':True},\n",
    "        {'numbers':True,'number_mention':True},\n",
    "        {'stemming':True},\n",
    "        {'spelling':False},#Warning: When True, it takes app 149 minutes on test set. Recomended to always set to false \n",
    "        {'elongation':True},\n",
    "        {'set_to_not':True},\n",
    "        {'remove_signs':True},\n",
    "        {'exclamation':True}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying all preprocessing techniques to the original corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for input_ in inputs: \n",
    "        corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "        corpuses.append(corpus)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[2,3,4]\n",
    "for n in ns: \n",
    "    corpus=HL.creating_n_grams_corpus(n,full_corpus)\n",
    "    corpuses.append(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1: Testing all preprocessing techniques: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "stds=[]\n",
    "\n",
    "for corpus in corpuses: \n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "    accuracies.append(model_score[0][0])\n",
    "    stds.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to determine which preprocessing techniques that improved the accuracy, and keep them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses_1=[]\n",
    "names_1=[]\n",
    "stds_1=[]\n",
    "acc_1=[]\n",
    "print('The original corpus gave accuracy of: ',accuracies[0], 'std:', stds[0],'\\n')\n",
    "for i in range(1,len(accuracies)):\n",
    "    if accuracies[i]>=accuracies[0]:\n",
    "        corpuses_1.append(corpuses[i])\n",
    "        names_1.append(names[i])\n",
    "        stds_1.append(stds[i])\n",
    "        acc_1.append(accuracies[i])\n",
    "        print('IMPROVED:  ',names[i],', score:',accuracies[i],'std:',stds[i])\n",
    "    else:\n",
    "        print('Not better:',names[i],', score:',accuracies[i],'std:',stds[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(names_1)\n",
    "\n",
    "\"\"\"\n",
    "IMPROVED:   SH_corpus , score: 0.825355 std: 0.000791580697087\n",
    "IMPROVED:   SHM_corpus , score: 0.825075 std: 0.00108085614214\n",
    "Not better: H_corpus , score: 0.82204 std: 0.00155889383859\n",
    "IMPROVED:   HK_corpus , score: 0.82262 std: 0.00126653069446\n",
    "IMPROVED:   PS_corpus , score: 0.82248 std: 0.00136009190866\n",
    "IMPROVED:   NS__corpus , score: 0.822385 std: 0.00153407952858\n",
    "IMPROVED:   OS_corpus , score: 0.822515 std: 0.00137109445335\n",
    "IMPROVED:   N_corpus , score: 0.822245 std: 0.00134651401775\n",
    "IMPROVED:   NM_corpus , score: 0.822745 std: 0.00184183332579\n",
    "Not better: ST_corpus , score: 0.812625 std: 0.000972239682383\n",
    "IMPROVED:   SP_corpus , score: 0.822245 std: 0.00134651401775\n",
    "IMPROVED:   E_corpus , score: 0.82334 std: 0.00197917912277\n",
    "IMPROVED:   SN_corpus , score: 0.825595 std: 0.00118473625757\n",
    "Not better: RS_corpus , score: 0.814735 std: 0.00153155150093\n",
    "IMPROVED:   EX_corpus , score: 0.82229 std: 0.00142571034926\n",
    "IMPROVED:   N-2_corpus , score: 0.823265 std: 0.00156073700539\n",
    "Not better: N-3_corpus , score: 0.82214 std: 0.00145487112831\n",
    "Not better: N-4_corpus , score: 0.816695 std: 0.00155212757208\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do not keep SHM, as SH is better. \n",
    "# We do not keep N as NM is better \n",
    "# We do not keep SP due to time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2: Apply all techniques that contributed positively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_={'hashtag': True, 'segmentation_hash': True,'hugs_and_kisses':True,'all_smilies':True,\n",
    "        'numbers':True,'number_mention':True,'elongation':True, 'set_to_not':True,'exclamation':True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prepr_corpus=TO.preprocess_corpus(full_corpus, **input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses=[]\n",
    "corpuses.append(all_prepr_corpus)\n",
    "ns=[2]\n",
    "for n in ns: \n",
    "    corpus=HL.creating_n_grams_corpus(n,all_prepr_corpus)\n",
    "    #corpuses.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies2=[]\n",
    "stds2=[]\n",
    "\n",
    "for corpus in corpuses: \n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "    accuracies2.append(model_score[0][0])\n",
    "    stds2.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(corpuses)):\n",
    "    print('Accuracy:',  accuracies2[i], 'std:',stds2[i],'\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_corpus=corpuses[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mindfs= [2,3,5,10,20]\n",
    "maxdfs=[0.8, 0.999] \n",
    "accuracies_stop=[]\n",
    "stds_stop=[]\n",
    "stop_lens=[]\n",
    "vocabs=[]\n",
    "\n",
    "\n",
    "for max_ in maxdfs:\n",
    "    for min_ in mindfs: \n",
    "        stopwords, vocab= TO.get_dynamic_stopwords(best_corpus, MinDf=min_, MaxDf=max_,sublinearTF=True,useIDF=False)\n",
    "        stopword_corpus=TO.remove_stopwords(best_corpus, stopwords)\n",
    "        model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, stopword_corpus, total_training_tweets, nr_pos_tweets, epochs=100, n_folds=3)\n",
    "        accuracies_stop.append(model_score[0][0])\n",
    "        stds_stop.append(model_score[0][1])\n",
    "        stop_lens.append(len(stopwords))\n",
    "        #vocabs.append(vocab)\n",
    "\n",
    "        print('This is min',min_, 'this is max', max_, 'this is len stopwords', len(stopwords), 'this is acc: ', model_score[0][0],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(stop_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(stds_stop)):\n",
    "    print('Accuracy:',  accuracies_stop[i], 'std:',stds_stop[i],'stopwords:',stop_lens[i],'\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing \"best preprocessing\" with full dataset: \n",
    "\n",
    "Som før for å lage en keggle! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_corpus=TO.preprocess_corpus(full_corpus, segmentation_hash=True, hashtag=True, hashtag_mention=True, set_to_not=True,elongation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets, epochs=6, n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making kaggle submission: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_test_adam.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.basic_model_adam, global_vectors, all_prepr_corpus, total_training_tweets, nr_pos_tweets,kaggle_name, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = pickle.load(open('full_corpus_prepro_with_n2_and_all_other.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500000/2500000 [==============================] - 24s 9us/step - loss: 0.3812 - acc: 0.8222\n",
      "Epoch 2/100\n",
      "   7168/2500000 [..............................] - ETA: 53s - loss: 0.3485 - acc: 0.8436 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:402: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3609 - acc: 0.8342\n",
      "Epoch 3/100\n",
      "2500000/2500000 [==============================] - 23s 9us/step - loss: 0.3557 - acc: 0.8370\n",
      "Epoch 4/100\n",
      "2500000/2500000 [==============================] - 21s 8us/step - loss: 0.3527 - acc: 0.8384\n",
      "Epoch 5/100\n",
      "2500000/2500000 [==============================] - 21s 8us/step - loss: 0.3506 - acc: 0.8397\n",
      "Epoch 6/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3489 - acc: 0.8406\n",
      "Epoch 7/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3477 - acc: 0.8410\n",
      "Epoch 8/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3467 - acc: 0.8417\n",
      "Epoch 9/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3457 - acc: 0.8423\n",
      "Epoch 10/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3450 - acc: 0.8425\n",
      "Epoch 11/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3443 - acc: 0.8429\n",
      "Epoch 12/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3436 - acc: 0.8432\n",
      "Epoch 13/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3431 - acc: 0.8436\n",
      "Epoch 14/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3425 - acc: 0.8437\n",
      "Epoch 15/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3421 - acc: 0.8441\n",
      "Epoch 16/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3418 - acc: 0.8443\n",
      "Epoch 17/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3414 - acc: 0.8446\n",
      "Epoch 18/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3410 - acc: 0.8444\n",
      "Epoch 19/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3406 - acc: 0.8447\n",
      "Epoch 20/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3404 - acc: 0.8449\n",
      "Epoch 21/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3402 - acc: 0.8450\n",
      "Epoch 22/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3399 - acc: 0.8452\n",
      "Epoch 23/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3397 - acc: 0.8452\n",
      "Epoch 24/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3394 - acc: 0.8454\n",
      "Epoch 25/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3393 - acc: 0.8456\n",
      "Epoch 26/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3391 - acc: 0.8456\n",
      "Epoch 27/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3390 - acc: 0.8458\n",
      "Epoch 28/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3387 - acc: 0.8458\n",
      "Epoch 29/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3386 - acc: 0.8459\n",
      "Epoch 30/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3384 - acc: 0.8459\n",
      "Epoch 31/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3382 - acc: 0.8461\n",
      "Epoch 32/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3381 - acc: 0.8461\n",
      "Epoch 33/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3380 - acc: 0.8461\n",
      "Epoch 34/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3378 - acc: 0.8461\n",
      "Epoch 35/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3377 - acc: 0.8464\n",
      "Epoch 36/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3376 - acc: 0.8464\n",
      "Epoch 37/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3375 - acc: 0.8464\n",
      "Epoch 38/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3374 - acc: 0.8464\n",
      "Epoch 39/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3373 - acc: 0.8465\n",
      "Epoch 40/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3372 - acc: 0.8466\n",
      "Epoch 41/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3371 - acc: 0.8466\n",
      "Epoch 42/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3370 - acc: 0.8467\n",
      "Epoch 43/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3369 - acc: 0.8468\n",
      "Epoch 44/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3368 - acc: 0.8468\n",
      "Epoch 45/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3367 - acc: 0.8467\n",
      "Epoch 46/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3366 - acc: 0.8469\n",
      "Epoch 47/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3366 - acc: 0.8469\n",
      "Epoch 48/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3365 - acc: 0.8470\n",
      "Epoch 49/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3365 - acc: 0.8469\n",
      "Epoch 50/100\n",
      "2500000/2500000 [==============================] - 22s 9us/step - loss: 0.3364 - acc: 0.8470\n",
      "Epoch 51/100\n",
      "2500000/2500000 [==============================] - 21s 9us/step - loss: 0.3362 - acc: 0.8473\n",
      "Epoch 52/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3362 - acc: 0.8471\n",
      "Epoch 53/100\n",
      "2500000/2500000 [==============================] - 19s 7us/step - loss: 0.3361 - acc: 0.8472\n",
      "Epoch 54/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3361 - acc: 0.8472\n",
      "Epoch 55/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3360 - acc: 0.8474\n",
      "Epoch 56/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3360 - acc: 0.8473\n",
      "Epoch 57/100\n",
      "2500000/2500000 [==============================] - 19s 7us/step - loss: 0.3359 - acc: 0.8473\n",
      "Epoch 58/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3359 - acc: 0.8473\n",
      "Epoch 59/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3358 - acc: 0.8474\n",
      "Epoch 60/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3358 - acc: 0.8474\n",
      "Epoch 61/100\n",
      "2500000/2500000 [==============================] - 19s 7us/step - loss: 0.3357 - acc: 0.8474\n",
      "Epoch 62/100\n",
      "2500000/2500000 [==============================] - 19s 7us/step - loss: 0.3357 - acc: 0.8475\n",
      "Epoch 63/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3355 - acc: 0.8475\n",
      "Epoch 64/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3356 - acc: 0.8475\n",
      "Epoch 65/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3355 - acc: 0.8477\n",
      "Epoch 66/100\n",
      "2500000/2500000 [==============================] - 19s 8us/step - loss: 0.3355 - acc: 0.8475\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'best_neural_model_prediction_model.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b9043ac28228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#final_corpus=n_grams_corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mGV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_model_adam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_training_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_pos_tweets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkaggle_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Thomas\\EPFL\\Machine learning CS-433\\Project 2\\CD-433-Project-2\\glove_module.py\u001b[0m in \u001b[0;36mget_prediction\u001b[1;34m(neural_net, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, kaggle_name, epochs)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_document_vecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_neural_model_prediction_model.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_document_vecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;31m# instantiate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'best_neural_model_prediction_model.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "kaggle_name=\"keggle_glove_test_adam_full_set_thomas.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.basic_model_adam, global_vectors, corpus, total_training_tweets, nr_pos_tweets,kaggle_name, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
