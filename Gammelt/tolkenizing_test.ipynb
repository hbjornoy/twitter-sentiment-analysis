{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports and stuff\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "#Libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#Powerpuff-stuff\n",
    "import helpers as HL\n",
    "import dataanalysis as DA\n",
    "import cleaning as CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUNNING ON TRAINING SET WITH CROSS VALIDATION\n",
    "\n",
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_full = \"train_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputfiles=[training_set_pos, training_set_neg]\n",
    "\n",
    "original_corpus, file_lengths=HL.create_corpus(inputfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_pos_lines=file_lengths[0]\n",
    "nr_neg_lines=file_lengths[1]\n",
    "\n",
    "nr_lines_total=sum(file_lengths)\n",
    "\n",
    "labels=HL.create_labels(nr_pos_lines,nr_neg_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizing as TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus=TO.preprocess_corpus(original_corpus,stemming=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\\n\"\n",
      "b\"because your logic is so dumb i won't even crop out your name or your photo tsk <url>\"\n"
     ]
    }
   ],
   "source": [
    "print(original_corpus[1])\n",
    "print(new_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_string=DA.corpus_to_strings(original_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "for tweet in corpus_string:\n",
    "    \n",
    "    \n",
    "    words=tknzr.tokenize(tweet) #word_tokenize(tweet)\n",
    "    for i, word in enumerate(words):\n",
    "        if word=='(:':\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='_' and words[i-1]=='^' and words[i+1]=='^':\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='o' and words[i-1]==':':\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='/' and words[i-1]==':':\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='*' and (words[i-1]==':' or words[i-1]==';'):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word==')' and (words[i-1]==':' or words[i-1]==';'):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='(' and (words[i-1]==':' or words[i-1]==';'):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and (words[i-1]=='(' and (word==':' or word==';')):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and words[i-1]==')' and (word==':' or word==';'):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='d' and (words[i-1]==':' or words[i-1]==';'):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "        elif i>0 and word=='p' and (words[i-1]==':' or words[i-1]==';'):\n",
    "            print(words)\n",
    "            print(tweet, '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alt1=nltk.word_tokenize(corpus_string[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "alt2=tknzr.tokenize(original_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(alt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "def replace_words(corpus):\n",
    "    \n",
    "    new_corpus=[]\n",
    "    new_corpus_string=[]\n",
    "    \n",
    "    tknzr = TweetTokenizer()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for line in corpus:\n",
    "        words=tknzr.tokenize(line)\n",
    "        exclamation=False\n",
    "        hashtag=False\n",
    "        number=False\n",
    "        cleaned_tweet=[]\n",
    "        retweet=False\n",
    "        for i, word in enumerate(words):\n",
    "            #If we want to know if tweets are a retweet:\n",
    "            if word=='rt':\n",
    "                retweet=True\n",
    "            \n",
    "           #Want to look for special signs\n",
    "            elif word=='!':\n",
    "                exclamation=True\n",
    "                cleaned_tweet.append(word)\n",
    "            elif word[0]=='#':\n",
    "                hashtag=True\n",
    "                cleaned_tweet.append(word[1:])\n",
    "                \n",
    "            \n",
    "            #want to look for abbreviations that mean not:\n",
    "            elif word[-3:]=='n\\'t':\n",
    "                #cleaned_tweet.append(word[:-3])\n",
    "                cleaned_tweet.append('not')\n",
    "            \n",
    "            #Want to look for numbers\n",
    "            elif word.isdigit():\n",
    "                number=True\n",
    "            \n",
    "            #want to find hearts, hugs, kisses, etc: \n",
    "            elif word == \"<3\":\n",
    "                cleaned_tweet.append('heart')\n",
    "            elif (word == \"xoxo\" or word == \"xo\" or word == \"xoxoxo\" or word == \"xxoo\"):\n",
    "                cleaned_tweet.append('hug')  \n",
    "                cleaned_tweet.append('kiss')\n",
    "            elif (word=='xx' or word=='xxx'):\n",
    "                cleaned_tweet.append('kiss')\n",
    "                \n",
    "            \n",
    "            #looking for different types of smilies that have not been removed from the dataset: \n",
    "            elif i>0 and word=='_' and words[i-1]=='^' and words[i+1]=='^':\n",
    "                cleaned_tweet.append('eyesmiley')\n",
    "            elif i>0 and word=='o' and words[i-1]==':':\n",
    "                cleaned_tweet.append('openmouthface')\n",
    "            elif i>0 and word=='/' and words[i-1]==':':\n",
    "                cleaned_tweet.append('slashsmiely')\n",
    "            elif i>0 and word=='*' and (words[i-1]==':' or words[i-1]==';'):\n",
    "                cleaned_tweet.append('kiss')\n",
    "            elif i>0 and word==')' and (words[i-1]==':' or words[i-1]==';'):\n",
    "                cleaned_tweet.append('possmiley')\n",
    "            elif i>0 and word=='(' and (words[i-1]==':' or words[i-1]==';'):\n",
    "                cleaned_tweet.append('negsmiley')\n",
    "            elif i>0 and words[i-1]=='(' and (word==':' or word==';'):\n",
    "                cleaned_tweet.append('possmiley')\n",
    "            elif i>0 and words[i-1]==')' and (word==':' or word==';'):\n",
    "                cleaned_tweet.append('negsmiley')\n",
    "            elif i>0 and (word=='d' and (words[i-1]==':' or words[i-1]==';')) or word == ':d':\n",
    "                cleaned_tweet.append('possmiley')\n",
    "            elif i>0 and (word=='p' and (words[i-1]==':' or words[i-1]==';')) or word == ':p':\n",
    "                cleaned_tweet.append('possmiley')\n",
    "            elif i>0 and word=='d' and (words[i-1]==':' or words[i-1]==';' or words[i-1]=='x'):\n",
    "                cleaned_tweet.append('possmiley')\n",
    "        \n",
    "        \n",
    "            elif (word!= '^' and word!=',' and word!='.' and word!=':' and word!='-' and word!='Â´' and word!=';'\n",
    "                 and word!=')' and word!='(' and word!='*'):\n",
    "            \n",
    "                cleaned_tweet.append(ps.stem(word))\n",
    "            \n",
    "        if exclamation:\n",
    "            cleaned_tweet.append('exclamation')\n",
    "        if hashtag:\n",
    "            cleaned_tweet.append('hashtag')\n",
    "        if number:\n",
    "            cleaned_tweet.append('thereisanumber')\n",
    "        \n",
    "        #if retweet==False:\n",
    "        \n",
    "        new_words = ' '.join(cleaned_tweet)\n",
    "        new_words = new_words.encode('utf-8')\n",
    "        new_corpus.append(new_words)\n",
    "    return new_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new=replace_words(original_corpus[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(original_corpus[1:10])\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "for j, tweet in enumerate(corpus_string[0:10000]):\n",
    "    words=tknzr.tokenize(tweet)\n",
    "    for i, word in enumerate(words):\n",
    "        \n",
    "        \n",
    "           #Want to look for special signs\n",
    "            if word=='!':\n",
    "                exclamation=True\n",
    "            elif word[0]=='#':\n",
    "                hashtag=True\n",
    "                #cleaned_tweet.append(word[1:])\n",
    "                \n",
    "            \n",
    "            #want to look for abbreviations that mean not:\n",
    "            elif word[-3:]=='n\\'t':\n",
    "                #cleaned_tweet.append(word[:-3])\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            \n",
    "            #Want to look for numbers\n",
    "            elif word.isdigit():\n",
    "                number=True\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            \n",
    "            #want to find hearts, hugs, kisses, etc: \n",
    "            elif word == \"<3\":\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif (word == \"xoxo\" or word == \"xo\" or word == \"xoxoxo\" or word == \"xxoo\"):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif (word=='xx' or word=='xxx'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "                \n",
    "            \n",
    "            #looking for different types of smilies that have not been removed from the dataset: \n",
    "            elif i>0 and word=='_' and words[i-1]=='^' and words[i+1]=='^':\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and word=='o' and words[i-1]==':':\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and word=='/' and words[i-1]==':':\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and word=='*' and (words[i-1]==':' or words[i-1]==';'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and word==')' and (words[i-1]==':' or words[i-1]==';'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and word=='(' and (words[i-1]==':' or words[i-1]==';'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and words[i-1]=='(' and (word==':' or word==';'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and words[i-1]==')' and (word==':' or word==';'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and (word=='d' and (words[i-1]==':' or words[i-1]==';')) or word == ':d':\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and (word=='p' and (words[i-1]==':' or words[i-1]==';')) or word == ':p':\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "            elif i>0 and word=='d' and (words[i-1]==':' or words[i-1]==';' or words[i-1]=='x'):\n",
    "                print(words)\n",
    "                print(new[j])\n",
    "                print(tweet, '\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(new[1])\n",
    "print(corpus_string[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string='<3'\n",
    "string==\"<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new.append('hei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tolken as TO\n",
    "new=TO.replace_words(original_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(new[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hei='ja'\n",
    "\n",
    "if hei!=('nei' and 'hei' and 'h'):\n",
    "    print(hei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
