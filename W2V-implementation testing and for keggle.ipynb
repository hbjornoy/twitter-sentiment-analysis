{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens.split():\n",
    "        try:       \n",
    "            word = word.decode('utf-8')\n",
    "            word_vec = model[word].reshape((1, size))             \n",
    "            idf_weighted_vec = word_vec * tfidf_dict[word]\n",
    "            vec += idf_weighted_vec\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterableCorpus():\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tweet in self.corpus:\n",
    "            tweet_words = tweet.split()\n",
    "            yield [word.decode('utf-8') for word in tweet_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim \n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Conv1D, Embedding, LSTM\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import time\n",
    "\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "\n",
    "\n",
    "\n",
    "# MEANING doc-vecs not necessary with gensim word2vec bcof gensim distance mesaure sim_score = cosing distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables [ choose sub_set or full_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global variables\n",
    "N_DIMENTIONS = 200 \n",
    "test_set_tweets = 10000\n",
    "\n",
    "#FOR TRAINING_SET\n",
    "corpus_filenames = ['train_pos.txt', 'train_neg.txt','test_data.txt'] \n",
    "nr_pos_tweets = 100000\n",
    "nr_neg_tweets = 100000\n",
    "total_training_tweets = 200000\n",
    "\n",
    "#FOR FULL SET\n",
    "#corpus_filenames = ['train_pos_full.txt', 'train_neg_full.txt','test_data.txt'] \n",
    "#nr_pos_tweets = 1250000\n",
    "#nr_neg_tweets = 1250000\n",
    "#total_training_tweets = 2500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length full corpus 210000\n",
      "File lengths: [100000, 100000, 10000]\n"
     ]
    }
   ],
   "source": [
    "full_corpus, corpus_file_lengths = HL.create_corpus(corpus_filenames)\n",
    "\n",
    "print(\"Length full corpus\", len(full_corpus))\n",
    "print(\"File lengths:\", corpus_file_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating clusterizing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len cluster dict: 216856\n"
     ]
    }
   ],
   "source": [
    "cluster_file_path = '50mpaths2.txt'\n",
    "cluster_dict = CL.create_dictionary(cluster_file_path)\n",
    "\n",
    "print(\"Len cluster dict:\", len(cluster_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterizing corpus \n",
    "\n",
    "In practice quite equal to stemming the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len clusterised training corpus:  210000\n"
     ]
    }
   ],
   "source": [
    "clusterised_full_corpus = CL.create_clusterized_corpus(full_corpus, cluster_dict)\n",
    "\n",
    "print(\"Len clusterised training corpus: \", len(clusterised_full_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_corpus = HL.creating_n_grams_cropus(corpus=clusterised_full_corpus, n_gram=2) #2GramsForLife "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating vectorizer to create idf_weighting of each word in the corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True, # scale the term frequency in logarithmic scale\n",
    "        use_idf =True\n",
    "    )\n",
    "\n",
    "corpus_tf_idf = vectorizer.fit_transform(ngram_corpus)\n",
    "\n",
    "tfidf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Creating word2vec for all tweets, train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2v model: Word2Vec(vocab=44487, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#Making the corpus iterable to be able to feed it to word2vec from gensim\n",
    "iterable_full_corpus = IterableCorpus(ngram_corpus)\n",
    "\n",
    "w2v_model = Word2Vec(iterable_full_corpus, size=N_DIMENTIONS, window=5, min_count=10, workers=4)\n",
    "\n",
    "print(\"W2v model:\", w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a different form of the word2vec to use a LOT less RAM when running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labels for the training files. Used to perform validation of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape:  (200000,)\n"
     ]
    }
   ],
   "source": [
    "#Making labels\n",
    "labels = np.zeros(total_training_tweets);\n",
    "labels[0:nr_pos_tweets]=1;\n",
    "labels[nr_pos_tweets:total_training_tweets]=0; \n",
    "\n",
    "print(\"labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the corpus into train and prediction - parts\n",
    "\n",
    "We're done training the word2vec, so all \"common\" operations are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# SPLITTING THE W2V model into training and predict\n",
    "train_clusterised_corpus = clusterised_full_corpus[:total_training_tweets:]\n",
    "predict_clusterised_corpus = clusterised_full_corpus[total_training_tweets::]\n",
    "\n",
    "print(len(train_clusterised_corpus))\n",
    "print(len(predict_clusterised_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating document wecs for training and test-set\n",
    "\n",
    "In other words a vector for each tweet, representing the content of that tweet. Each word in each tweet is weighted by its idf-score found in the 'tfidf_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_document_vecs = np.concatenate([buildWordVector(z, N_DIMENTIONS, word_vectors) for z in train_clusterised_corpus])\n",
    "train_document_vecs = scale(train_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY FOR KAGGLE\n",
    "test_document_vecs = np.concatenate([buildWordVector(z, N_DIMENTIONS, word_vectors) for z in predict_clusterised_corpus])\n",
    "test_document_vecs = scale(test_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train w2v shape: (200000, 200)\n",
      "Train w2v shape: (10000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train w2v shape:\",train_document_vecs.shape)\n",
    "print(\"Train w2v shape:\",test_document_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the neural net classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TING FOR Å SIKRE REPRODUSERBARHET ( ikke alt er nødv. nødvendig )\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "rn.seed(12345) # NO IDEA WHAT THIS DOES\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def basic_model_adam():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def wide_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_1_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_2_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def deep_2_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(60, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, input_dim=N_DIMENTIONS, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def recurrent_model():\n",
    "    # Start neural network\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an embedding layer\n",
    "    model.add(Embedding(input_dim=N_DIMENTIONS, output_dim=128))\n",
    "\n",
    "    # Add a long short-term memory layer with 128 units\n",
    "    model.add(LSTM(units=128))\n",
    "\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile neural network\n",
    "    model.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                optimizer='Adam', # Adam optimization\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "    return model\n",
    "\n",
    "def convolutional_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same', input_shape=(133332, N_DIMENTIONS)))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32,  activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Convolution \n",
    "#Prøve netverk uten noen av pre-processing\n",
    "#Recurrent?\n",
    "#LSTM network? Long short term memory? \n",
    "#Dropout? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Running some nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(models, X, Y, epochs, n_folds, seed):\n",
    "    \n",
    "    for neural_model in models:\n",
    "        \n",
    "        model_name = neural_model.__name__\n",
    "        \n",
    "        model = neural_model()\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train, test in kfold.split(X, Y):\n",
    "            \n",
    "            early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
    "            \n",
    "            model.fit(X[train], Y[train], epochs=epochs, batch_size=1024, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\n",
    "            cv_scores.append(scores[1] * 100)\n",
    "\n",
    "        print(\"Model: \", model_name)\n",
    "        print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "        print(\"Time taken: \", (time.time() - start) / 60, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  basic_model\n",
      "80.45% (+/- 0.49%)\n",
      "Time taken:  0.7064207275708516 \n",
      "\n",
      "Model:  basic_model_adam\n",
      "80.86% (+/- 0.52%)\n",
      "Time taken:  1.143131975332896 \n",
      "\n",
      "Model:  wide_model\n",
      "80.93% (+/- 0.73%)\n",
      "Time taken:  1.0753306984901427 \n",
      "\n",
      "Model:  deep_1_model\n",
      "80.13% (+/- 1.16%)\n",
      "Time taken:  1.1953313549359639 \n",
      "\n",
      "Model:  deep_2_model\n",
      "79.79% (+/- 0.87%)\n",
      "Time taken:  1.1410720268885295 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [basic_model, basic_model_adam, wide_model, deep_1_model, deep_2_model]\n",
    "\n",
    "run_k_fold(models, train_document_vecs, labels, epochs=10, n_folds=3, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NOT WORKING YET\n",
    "#models = [recurrent_model]\n",
    "\n",
    "#run_k_fold(models, train_document_vecs, labels, epochs=3, n_folds=3, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff for making keggle predictions! Probably needs updating, not working yet! Look at old_stuff to see how to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_vecs_w2v)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounded = [-1 if round(x[0])==0 else 1 for x in predictions]\n",
    "\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = list(range(1,test_set_tweets+1))\n",
    "y_pred = rounded\n",
    "name = \"keggle_submission_neural.csv\"\n",
    "\n",
    "HL.create_csv_submission(ids, y_pred, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF, PROBABLY NOT TO BE USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KAN BRUKES TIL NÅR MAN SKAL GJØRE EN MODEL FIT, NÅR MAN HAR FUNNET RIKTIG MODELL Å BRUKE OVER\n",
    "\n",
    "# Smaller batch equalls longer run times, but less epochs for convergence \n",
    "# DENNE GA 80% PÅ KAGGLE\n",
    "\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "#checkpointer = ModelCheckpoint(filepath='tmp/saved_weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "#model_4 = Sequential()\n",
    "#model_4.add(Dropout(0.05, input_shape=(N_DIMENTIONS,)))\n",
    "#model_4.add(Dense(100, activation='relu', input_dim=N_DIMENTIONS))\n",
    "#model_4.add(Dense(60, activation='relu'))\n",
    "#model_4.add(Dense(30, activation='relu'))\n",
    "#model_4.add(Dropout(0, input_shape=(N_DIMENTIONS,)))\n",
    "#model_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "#model_4.compile(optimizer='rmsprop',\n",
    "#      loss='binary_crossentropy',\n",
    "#      metrics=['accuracy'])\n",
    "\n",
    "#model_4.fit(training_vecs_w2v, labels, epochs=30, batch_size=1024, verbose=1, callbacks=[early_stopping, checkpointer], validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#best_model = load_model('tmp/saved_weights.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
