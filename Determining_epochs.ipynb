{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff we actually need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and making corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neural_nets as NN\n",
    "neural_nets=[NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Train on 128000 samples, validate on 40000 samples\n",
      "Epoch 1/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.7289Epoch 00001: val_loss improved from inf to 0.58342, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 60s 472us/step - loss: 0.5030 - acc: 0.7302 - val_loss: 0.5834 - val_acc: 0.7238\n",
      "Epoch 2/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8137Epoch 00002: val_loss improved from 0.58342 to 0.54408, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 6s 47us/step - loss: 0.3939 - acc: 0.8138 - val_loss: 0.5441 - val_acc: 0.7496\n",
      "Epoch 3/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8222Epoch 00003: val_loss improved from 0.54408 to 0.51265, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 5s 41us/step - loss: 0.3788 - acc: 0.8222 - val_loss: 0.5127 - val_acc: 0.7522\n",
      "Epoch 4/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8293Epoch 00004: val_loss improved from 0.51265 to 0.47440, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 5s 40us/step - loss: 0.3662 - acc: 0.8288 - val_loss: 0.4744 - val_acc: 0.7756\n",
      "Epoch 5/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8356Epoch 00005: val_loss did not improve\n",
      "128000/128000 [==============================] - 5s 40us/step - loss: 0.3566 - acc: 0.8356 - val_loss: 0.5376 - val_acc: 0.7481\n",
      "Epoch 6/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8409Epoch 00006: val_loss did not improve\n",
      "128000/128000 [==============================] - 5s 37us/step - loss: 0.3458 - acc: 0.8407 - val_loss: 0.5012 - val_acc: 0.7597\n",
      "Epoch 7/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8455Epoch 00007: val_loss did not improve\n",
      "128000/128000 [==============================] - 5s 38us/step - loss: 0.3389 - acc: 0.8455 - val_loss: 0.4898 - val_acc: 0.7654\n",
      "Epoch 00007: early stopping\n",
      "32000/32000 [==============================] - 5s 160us/step\n",
      "Unseen score: 0.81215625\n",
      "Train on 128000 samples, validate on 40000 samples\n",
      "Epoch 1/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.7234Epoch 00001: val_loss improved from inf to 0.59407, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 11s 85us/step - loss: 0.5071 - acc: 0.7247 - val_loss: 0.5941 - val_acc: 0.7145\n",
      "Epoch 2/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8113Epoch 00002: val_loss improved from 0.59407 to 0.49177, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 5s 37us/step - loss: 0.3975 - acc: 0.8113 - val_loss: 0.4918 - val_acc: 0.7830\n",
      "Epoch 3/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8204Epoch 00003: val_loss did not improve\n",
      "128000/128000 [==============================] - 4s 35us/step - loss: 0.3809 - acc: 0.8206 - val_loss: 0.5365 - val_acc: 0.7327\n",
      "Epoch 4/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8278Epoch 00004: val_loss improved from 0.49177 to 0.46374, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 4s 35us/step - loss: 0.3685 - acc: 0.8274 - val_loss: 0.4637 - val_acc: 0.7784\n",
      "Epoch 5/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8332Epoch 00005: val_loss did not improve\n",
      "128000/128000 [==============================] - 5s 35us/step - loss: 0.3593 - acc: 0.8332 - val_loss: 0.5871 - val_acc: 0.7107\n",
      "Epoch 6/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8399Epoch 00006: val_loss did not improve\n",
      "128000/128000 [==============================] - 4s 34us/step - loss: 0.3491 - acc: 0.8398 - val_loss: 0.5131 - val_acc: 0.7407\n",
      "Epoch 7/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8444Epoch 00007: val_loss did not improve\n",
      "128000/128000 [==============================] - 5s 35us/step - loss: 0.3412 - acc: 0.8444 - val_loss: 0.5091 - val_acc: 0.7495\n",
      "Epoch 00007: early stopping\n",
      "32000/32000 [==============================] - 5s 171us/step\n",
      "Unseen score: 0.8191875\n",
      "Train on 128000 samples, validate on 40000 samples\n",
      "Epoch 1/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.7206Epoch 00001: val_loss improved from inf to 0.57006, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 12s 93us/step - loss: 0.5089 - acc: 0.7220 - val_loss: 0.5701 - val_acc: 0.7364\n",
      "Epoch 2/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8131- ETA: 1s - loss: Epoch 00002: val_loss improved from 0.57006 to 0.52014, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 5s 38us/step - loss: 0.3951 - acc: 0.8131 - val_loss: 0.5201 - val_acc: 0.7721\n",
      "Epoch 3/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8222Epoch 00003: val_loss improved from 0.52014 to 0.50629, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 5s 37us/step - loss: 0.3783 - acc: 0.8222 - val_loss: 0.5063 - val_acc: 0.7530\n",
      "Epoch 4/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8295- ETA: 0s - loss: 0.3667 - acc: 0 - ETA: 0s - loss: 0.3663 - acc:Epoch 00004: val_loss improved from 0.50629 to 0.44691, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 5s 38us/step - loss: 0.3659 - acc: 0.8295 - val_loss: 0.4469 - val_acc: 0.7895\n",
      "Epoch 5/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8347Epoch 00005: val_loss did not improve\n",
      "128000/128000 [==============================] - 5s 39us/step - loss: 0.3566 - acc: 0.8345 - val_loss: 0.5651 - val_acc: 0.7229\n",
      "Epoch 6/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8395Epoch 00006: val_loss did not improve\n",
      "128000/128000 [==============================] - 6s 48us/step - loss: 0.3476 - acc: 0.8393 - val_loss: 0.5060 - val_acc: 0.7452\n",
      "Epoch 7/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8448Epoch 00007: val_loss did not improve\n",
      "128000/128000 [==============================] - 7s 53us/step - loss: 0.3385 - acc: 0.8448 - val_loss: 0.5132 - val_acc: 0.7496\n",
      "Epoch 00007: early stopping\n",
      "32000/32000 [==============================] - 9s 282us/step\n",
      "Unseen score: 0.80784375\n",
      "Train on 128000 samples, validate on 40000 samples\n",
      "Epoch 1/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.7253Epoch 00001: val_loss improved from inf to 0.52623, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 15s 120us/step - loss: 0.5057 - acc: 0.7267 - val_loss: 0.5262 - val_acc: 0.7497\n",
      "Epoch 2/15\n",
      "125952/128000 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8134Epoch 00002: val_loss improved from 0.52623 to 0.52612, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 7s 52us/step - loss: 0.3943 - acc: 0.8135 - val_loss: 0.5261 - val_acc: 0.7676\n",
      "Epoch 3/15\n",
      "126976/128000 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8223Epoch 00003: val_loss improved from 0.52612 to 0.51182, saving model to best_neural_model_save.hdf5\n",
      "128000/128000 [==============================] - 7s 54us/step - loss: 0.3781 - acc: 0.8222 - val_loss: 0.5118 - val_acc: 0.7536\n",
      "Epoch 4/15\n",
      " 55296/128000 [===========>..................] - ETA: 3s - loss: 0.3655 - acc: 0.8300"
     ]
    }
   ],
   "source": [
    "accuracies_E=[]\n",
    "stds_E=[]\n",
    "\n",
    "for epochs_ in range(15,16):\n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, epochs=epochs_, n_folds=5)\n",
    "    accuracies_E.append(model_score[0][0])\n",
    "    stds_E.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracies_E) # Y\n",
    "print(stds_E) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the example exercise dataset\n",
    "\n",
    "print(type(epoch_values))\n",
    "print(type(accuracies_E))\n",
    "print(type(stds_E))\n",
    "\n",
    "index = range(0,len(epoch_values))\n",
    "#s = pd.Series(data, index=index)\n",
    "df = pd.DataFrame({'epoch_values' : pd.Series(epoch_values, index=epoch_values),\n",
    "      'accuracies_E' : pd.Series(accuracies_E, index=epoch_values),\n",
    "      'stds_E' : pd.Series(stds_E, index=epoch_values)})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sb.set(style=\"whitegrid\")\n",
    "\n",
    "# Draw a pointplot to show pulse as a function of three categorical factors\n",
    "g = sb.factorplot(x=\"epoch_values\", y=\"accuracies_E\", data=df, ) # , capsize=.2, size=6, aspect=.75\n",
    "#g.despine(left=True)\n",
    "g.map(plt.errorbar, \"epoch_values\", \"accuracies_E\", \"stds_E\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
