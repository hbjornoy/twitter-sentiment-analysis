{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff we actually need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and making corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neural_nets as NN\n",
    "neural_nets=[NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 13s 132us/step - loss: 0.5144 - acc: 0.7393 - val_loss: 0.4187 - val_acc: 0.8023\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 5s 47us/step - loss: 0.4039 - acc: 0.8096 - val_loss: 0.4060 - val_acc: 0.8075\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3866 - acc: 0.8196 - val_loss: 0.3938 - val_acc: 0.8152\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3763 - acc: 0.8256 - val_loss: 0.3889 - val_acc: 0.8177\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 5s 49us/step - loss: 0.3654 - acc: 0.8319 - val_loss: 0.3895 - val_acc: 0.8183\n",
      "Epoch 6/100\n",
      "100000/100000 [==============================] - 5s 50us/step - loss: 0.3579 - acc: 0.8352 - val_loss: 0.3910 - val_acc: 0.8173\n",
      "Epoch 7/100\n",
      "100000/100000 [==============================] - 5s 49us/step - loss: 0.3492 - acc: 0.8404 - val_loss: 0.3865 - val_acc: 0.8194\n",
      "Epoch 8/100\n",
      "100000/100000 [==============================] - 5s 46us/step - loss: 0.3418 - acc: 0.8444 - val_loss: 0.3867 - val_acc: 0.8182\n",
      "Epoch 9/100\n",
      "100000/100000 [==============================] - 5s 47us/step - loss: 0.3325 - acc: 0.8499 - val_loss: 0.3935 - val_acc: 0.8188\n",
      "Epoch 10/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3257 - acc: 0.8528 - val_loss: 0.3918 - val_acc: 0.8182\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 7s 66us/step - loss: 0.5169 - acc: 0.7376 - val_loss: 0.4184 - val_acc: 0.8013\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.4035 - acc: 0.8103 - val_loss: 0.3994 - val_acc: 0.8117\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 5s 51us/step - loss: 0.3881 - acc: 0.8181 - val_loss: 0.3926 - val_acc: 0.8148\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3788 - acc: 0.8241 - val_loss: 0.3872 - val_acc: 0.8187\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3679 - acc: 0.8305 - val_loss: 0.3853 - val_acc: 0.8194\n",
      "Epoch 6/100\n",
      "100000/100000 [==============================] - 5s 47us/step - loss: 0.3594 - acc: 0.8340 - val_loss: 0.3902 - val_acc: 0.8168\n",
      "Epoch 7/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3521 - acc: 0.8388 - val_loss: 0.3842 - val_acc: 0.8206\n",
      "Epoch 8/100\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3436 - acc: 0.8441 - val_loss: 0.3845 - val_acc: 0.8212\n",
      "Epoch 9/100\n",
      "100000/100000 [==============================] - 5s 49us/step - loss: 0.3363 - acc: 0.8476 - val_loss: 0.3864 - val_acc: 0.8193\n",
      "Epoch 10/100\n",
      "100000/100000 [==============================] - 5s 47us/step - loss: 0.3290 - acc: 0.8520 - val_loss: 0.3987 - val_acc: 0.8137\n",
      "Model:  deep_HB\n",
      "0.61% (+/- 0.21%)\n",
      "Negative sentiment: 81.58%  Positive sentiment: 81.61%\n",
      "Percentage of positive classifications (should be 50%ish): 50.0135\n",
      "Time taken:  2.314208745956421 \n",
      "\n",
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/101\n",
      "100000/100000 [==============================] - 15s 146us/step - loss: 0.5196 - acc: 0.7358 - val_loss: 0.4146 - val_acc: 0.8035\n",
      "Epoch 2/101\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.4076 - acc: 0.8083 - val_loss: 0.4027 - val_acc: 0.8108\n",
      "Epoch 3/101\n",
      "100000/100000 [==============================] - 5s 51us/step - loss: 0.3915 - acc: 0.8170 - val_loss: 0.3904 - val_acc: 0.8176\n",
      "Epoch 4/101\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3808 - acc: 0.8233 - val_loss: 0.3855 - val_acc: 0.8195\n",
      "Epoch 5/101\n",
      "100000/100000 [==============================] - 5s 51us/step - loss: 0.3707 - acc: 0.8292 - val_loss: 0.3864 - val_acc: 0.8192\n",
      "Epoch 6/101\n",
      "100000/100000 [==============================] - 5s 48us/step - loss: 0.3618 - acc: 0.8340 - val_loss: 0.3821 - val_acc: 0.8192\n",
      "Epoch 7/101\n",
      "100000/100000 [==============================] - 4s 44us/step - loss: 0.3548 - acc: 0.8378 - val_loss: 0.3823 - val_acc: 0.8212\n",
      "Epoch 8/101\n",
      "100000/100000 [==============================] - 4s 43us/step - loss: 0.3451 - acc: 0.8437 - val_loss: 0.3912 - val_acc: 0.8169\n",
      "Epoch 9/101\n",
      "100000/100000 [==============================] - 4s 41us/step - loss: 0.3364 - acc: 0.8486 - val_loss: 0.3857 - val_acc: 0.8207\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/101\n",
      "100000/100000 [==============================] - 6s 62us/step - loss: 0.5109 - acc: 0.7404 - val_loss: 0.4165 - val_acc: 0.8022\n",
      "Epoch 2/101\n",
      "100000/100000 [==============================] - 4s 43us/step - loss: 0.3998 - acc: 0.8124 - val_loss: 0.4028 - val_acc: 0.8086\n",
      "Epoch 3/101\n",
      "100000/100000 [==============================] - 4s 42us/step - loss: 0.3825 - acc: 0.8215 - val_loss: 0.3951 - val_acc: 0.8149\n",
      "Epoch 4/101\n",
      "100000/100000 [==============================] - 4s 44us/step - loss: 0.3710 - acc: 0.8285 - val_loss: 0.3924 - val_acc: 0.8164\n",
      "Epoch 5/101\n",
      "100000/100000 [==============================] - 5s 49us/step - loss: 0.3614 - acc: 0.8333 - val_loss: 0.3904 - val_acc: 0.8180\n",
      "Epoch 6/101\n",
      "100000/100000 [==============================] - 5s 49us/step - loss: 0.3517 - acc: 0.8393 - val_loss: 0.3942 - val_acc: 0.8178\n",
      "Epoch 7/101\n",
      "100000/100000 [==============================] - 5s 46us/step - loss: 0.3447 - acc: 0.8417 - val_loss: 0.3930 - val_acc: 0.8166\n",
      "Epoch 8/101\n",
      "100000/100000 [==============================] - 5s 47us/step - loss: 0.3352 - acc: 0.8480 - val_loss: 0.3967 - val_acc: 0.8176\n",
      "Model:  deep_HB\n",
      "0.61% (+/- 0.21%)\n",
      "Negative sentiment: 78.34%  Positive sentiment: 85.49%\n",
      "Percentage of positive classifications (should be 50%ish): 53.5745\n",
      "Time taken:  2.053626370429993 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_E=[]\n",
    "stds_E=[]\n",
    "\n",
    "for epochs_ in range(100,102):\n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, epochs=epochs_, n_folds=2)\n",
    "    accuracies_E.append(model_score[0][0])\n",
    "    stds_E.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracies_E) # Y\n",
    "print(stds_E) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the example exercise dataset\n",
    "\n",
    "print(type(epoch_values))\n",
    "print(type(accuracies_E))\n",
    "print(type(stds_E))\n",
    "\n",
    "index = range(0,len(epoch_values))\n",
    "#s = pd.Series(data, index=index)\n",
    "df = pd.DataFrame({'epoch_values' : pd.Series(epoch_values, index=epoch_values),\n",
    "      'accuracies_E' : pd.Series(accuracies_E, index=epoch_values),\n",
    "      'stds_E' : pd.Series(stds_E, index=epoch_values)})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sb.set(style=\"whitegrid\")\n",
    "\n",
    "# Draw a pointplot to show pulse as a function of three categorical factors\n",
    "g = sb.factorplot(x=\"epoch_values\", y=\"accuracies_E\", data=df, ) # , capsize=.2, size=6, aspect=.75\n",
    "#g.despine(left=True)\n",
    "g.map(plt.errorbar, \"epoch_values\", \"accuracies_E\", \"stds_E\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
