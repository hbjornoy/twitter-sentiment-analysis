{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff we actually need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and making corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neural_nets as NN\n",
    "neural_nets=[NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.6772Epoch 00001: val_loss improved from inf to 0.50755, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 4s 39us/step - loss: 0.5798 - acc: 0.6788 - val_loss: 0.5075 - val_acc: 0.7301\n",
      "Epoch 2/15\n",
      " 96256/100000 [===========================>..] - ETA: 0s - loss: 0.5011 - acc: 0.7357Epoch 00002: val_loss improved from 0.50755 to 0.50088, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.5012 - acc: 0.7358 - val_loss: 0.5009 - val_acc: 0.7358\n",
      "Epoch 3/15\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.7415Epoch 00003: val_loss improved from 0.50088 to 0.49151, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.4915 - acc: 0.7414 - val_loss: 0.4915 - val_acc: 0.7427\n",
      "Epoch 4/15\n",
      " 96256/100000 [===========================>..] - ETA: 0s - loss: 0.4859 - acc: 0.7455Epoch 00004: val_loss improved from 0.49151 to 0.48621, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.4864 - acc: 0.7450 - val_loss: 0.4862 - val_acc: 0.7467\n",
      "Epoch 5/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.7485Epoch 00005: val_loss improved from 0.48621 to 0.48428, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 19us/step - loss: 0.4827 - acc: 0.7489 - val_loss: 0.4843 - val_acc: 0.7474\n",
      "Epoch 6/15\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.7511Epoch 00006: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.4784 - acc: 0.7509 - val_loss: 0.4854 - val_acc: 0.7469\n",
      "Epoch 7/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4768 - acc: 0.7526Epoch 00007: val_loss improved from 0.48428 to 0.48043, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.4762 - acc: 0.7529 - val_loss: 0.4804 - val_acc: 0.7508\n",
      "Epoch 8/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4741 - acc: 0.7551Epoch 00008: val_loss improved from 0.48043 to 0.48040, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.4741 - acc: 0.7551 - val_loss: 0.4804 - val_acc: 0.7491\n",
      "Epoch 9/15\n",
      " 96256/100000 [===========================>..] - ETA: 0s - loss: 0.4716 - acc: 0.7568Epoch 00009: val_loss improved from 0.48040 to 0.47903, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.4717 - acc: 0.7569 - val_loss: 0.4790 - val_acc: 0.7509\n",
      "Epoch 10/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.7584Epoch 00010: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 19us/step - loss: 0.4691 - acc: 0.7585 - val_loss: 0.4855 - val_acc: 0.7461\n",
      "Epoch 11/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.7578Epoch 00011: val_loss improved from 0.47903 to 0.47627, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.4679 - acc: 0.7583 - val_loss: 0.4763 - val_acc: 0.7542\n",
      "Epoch 12/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.7602Epoch 00012: val_loss improved from 0.47627 to 0.47494, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 19us/step - loss: 0.4643 - acc: 0.7601 - val_loss: 0.4749 - val_acc: 0.7538\n",
      "Epoch 13/15\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.7614Epoch 00013: val_loss improved from 0.47494 to 0.47465, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.4628 - acc: 0.7614 - val_loss: 0.4747 - val_acc: 0.7545\n",
      "Epoch 14/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.7640Epoch 00014: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.4608 - acc: 0.7642 - val_loss: 0.4785 - val_acc: 0.7516\n",
      "Epoch 15/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4603 - acc: 0.7631Epoch 00015: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.4609 - acc: 0.7628 - val_loss: 0.4804 - val_acc: 0.7511\n",
      "100000/100000 [==============================] - 4s 36us/step\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.5819 - acc: 0.6779Epoch 00001: val_loss improved from inf to 0.50991, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 3s 28us/step - loss: 0.5804 - acc: 0.6789 - val_loss: 0.5099 - val_acc: 0.7286\n",
      "Epoch 2/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.7360Epoch 00002: val_loss improved from 0.50991 to 0.49541, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.5007 - acc: 0.7360 - val_loss: 0.4954 - val_acc: 0.7393\n",
      "Epoch 3/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4913 - acc: 0.7423Epoch 00003: val_loss improved from 0.49541 to 0.49063, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.4910 - acc: 0.7426 - val_loss: 0.4906 - val_acc: 0.7425\n",
      "Epoch 4/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.7462Epoch 00004: val_loss improved from 0.49063 to 0.48611, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 22us/step - loss: 0.4859 - acc: 0.7464 - val_loss: 0.4861 - val_acc: 0.7468\n",
      "Epoch 5/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.7491Epoch 00005: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 22us/step - loss: 0.4807 - acc: 0.7491 - val_loss: 0.4883 - val_acc: 0.7443\n",
      "Epoch 6/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.7508Epoch 00006: val_loss improved from 0.48611 to 0.48377, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 22us/step - loss: 0.4782 - acc: 0.7505 - val_loss: 0.4838 - val_acc: 0.7487\n",
      "Epoch 7/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.7531Epoch 00007: val_loss improved from 0.48377 to 0.48185, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.4759 - acc: 0.7530 - val_loss: 0.4819 - val_acc: 0.7497\n",
      "Epoch 8/15\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.7543Epoch 00008: val_loss improved from 0.48185 to 0.48008, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 22us/step - loss: 0.4736 - acc: 0.7543 - val_loss: 0.4801 - val_acc: 0.7507\n",
      "Epoch 9/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.7556Epoch 00009: val_loss improved from 0.48008 to 0.47973, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 21us/step - loss: 0.4722 - acc: 0.7554 - val_loss: 0.4797 - val_acc: 0.7519\n",
      "Epoch 10/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4696 - acc: 0.7566Epoch 00010: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.4695 - acc: 0.7567 - val_loss: 0.4848 - val_acc: 0.7483\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.7591Epoch 00011: val_loss improved from 0.47973 to 0.47860, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 20us/step - loss: 0.4671 - acc: 0.7591 - val_loss: 0.4786 - val_acc: 0.7518\n",
      "Epoch 12/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.7610Epoch 00012: val_loss improved from 0.47860 to 0.47742, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 23us/step - loss: 0.4655 - acc: 0.7609 - val_loss: 0.4774 - val_acc: 0.7525\n",
      "Epoch 13/15\n",
      " 98304/100000 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.7610Epoch 00013: val_loss improved from 0.47742 to 0.47718, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 0.4644 - acc: 0.7610 - val_loss: 0.4772 - val_acc: 0.7521\n",
      "Epoch 14/15\n",
      " 96256/100000 [===========================>..] - ETA: 0s - loss: 0.4628 - acc: 0.7616Epoch 00014: val_loss improved from 0.47718 to 0.47706, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 2s 19us/step - loss: 0.4629 - acc: 0.7619 - val_loss: 0.4771 - val_acc: 0.7520\n",
      "Epoch 15/15\n",
      " 97280/100000 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.7635Epoch 00015: val_loss did not improve\n",
      "100000/100000 [==============================] - 2s 18us/step - loss: 0.4613 - acc: 0.7638 - val_loss: 0.4788 - val_acc: 0.7513\n",
      "100000/100000 [==============================] - 4s 41us/step\n",
      "Model:  deep_HB\n",
      "<keras.callbacks.History object at 0x00000258A1B50D68>\n",
      "[0.75446999999999997, 0.75202000000000002]\n",
      "0.75% (+/- 0.00%)\n",
      "Negative sentiment: 71.63%  Positive sentiment: 79.01%\n",
      "Percentage of positive classifications (should be 50%ish): 53.6905\n",
      "Time taken:  1.353650152683258 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_E=[]\n",
    "stds_E=[]\n",
    "\n",
    "for epochs_ in range(15,16):\n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, epochs=epochs_, n_folds=2)\n",
    "    accuracies_E.append(model_score[0][0])\n",
    "    stds_E.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracies_E) # Y\n",
    "print(stds_E) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the example exercise dataset\n",
    "\n",
    "print(type(epoch_values))\n",
    "print(type(accuracies_E))\n",
    "print(type(stds_E))\n",
    "\n",
    "index = range(0,len(epoch_values))\n",
    "#s = pd.Series(data, index=index)\n",
    "df = pd.DataFrame({'epoch_values' : pd.Series(epoch_values, index=epoch_values),\n",
    "      'accuracies_E' : pd.Series(accuracies_E, index=epoch_values),\n",
    "      'stds_E' : pd.Series(stds_E, index=epoch_values)})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sb.set(style=\"whitegrid\")\n",
    "\n",
    "# Draw a pointplot to show pulse as a function of three categorical factors\n",
    "g = sb.factorplot(x=\"epoch_values\", y=\"accuracies_E\", data=df, ) # , capsize=.2, size=6, aspect=.75\n",
    "#g.despine(left=True)\n",
    "g.map(plt.errorbar, \"epoch_values\", \"accuracies_E\", \"stds_E\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
