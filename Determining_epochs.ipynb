{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff we actually need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import helpers as HL\n",
    "import glove_module as GV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and making corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neural_nets as NN\n",
    "neural_nets=[NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.7369Epoch 00001: val_loss improved from inf to 0.41608, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 116s 1ms/step - loss: 0.5204 - acc: 0.7373 - val_loss: 0.4161 - val_acc: 0.8027\n",
      "Epoch 2/2\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8074Epoch 00002: val_loss improved from 0.41608 to 0.40195, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 9s 90us/step - loss: 0.4076 - acc: 0.8074 - val_loss: 0.4019 - val_acc: 0.8092\n",
      "100000/100000 [==============================] - 27s 269us/step\n",
      "CV-score: 0.80925\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.7399Epoch 00001: val_loss improved from inf to 0.43106, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 17s 165us/step - loss: 0.5137 - acc: 0.7402 - val_loss: 0.4311 - val_acc: 0.7938\n",
      "Epoch 2/2\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8137Epoch 00002: val_loss improved from 0.43106 to 0.41724, saving model to best_neural_model_save.hdf5\n",
      "100000/100000 [==============================] - 7s 71us/step - loss: 0.3977 - acc: 0.8138 - val_loss: 0.4172 - val_acc: 0.7996\n",
      "100000/100000 [==============================] - 28s 277us/step\n",
      "CV-score: 0.79955\n",
      "Model:  deep_HB\n",
      "CV_SCORES: [0.80925000000000002, 0.79954999999999998]\n",
      "CV_SCORES MEAN/STD: 0.80% (+/- 0.00%)\n",
      "Negative sentiment: 80.08%  Positive sentiment: 80.80%\n",
      "Percentage of positive classifications (should be 50%ish): 50.365\n",
      "Time taken:  4.5226789553960165 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_E=[]\n",
    "stds_E=[]\n",
    "\n",
    "for epochs_ in range(2,3):\n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, full_corpus, total_training_tweets, nr_pos_tweets, epochs=epochs_, n_folds=2)\n",
    "    accuracies_E.append(model_score[0][0])\n",
    "    stds_E.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracies_E) # Y\n",
    "print(stds_E) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the example exercise dataset\n",
    "\n",
    "print(type(epoch_values))\n",
    "print(type(accuracies_E))\n",
    "print(type(stds_E))\n",
    "\n",
    "index = range(0,len(epoch_values))\n",
    "#s = pd.Series(data, index=index)\n",
    "df = pd.DataFrame({'epoch_values' : pd.Series(epoch_values, index=epoch_values),\n",
    "      'accuracies_E' : pd.Series(accuracies_E, index=epoch_values),\n",
    "      'stds_E' : pd.Series(stds_E, index=epoch_values)})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sb.set(style=\"whitegrid\")\n",
    "\n",
    "# Draw a pointplot to show pulse as a function of three categorical factors\n",
    "g = sb.factorplot(x=\"epoch_values\", y=\"accuracies_E\", data=df, ) # , capsize=.2, size=6, aspect=.75\n",
    "#g.despine(left=True)\n",
    "g.map(plt.errorbar, \"epoch_values\", \"accuracies_E\", \"stds_E\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
