{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove with preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "import tokenizing_ekphrasis as TE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files used to create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets=[NN.deep_HB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing vectors:\n",
    "\n",
    "accuracies=[]\n",
    "stds=[]\n",
    "names=[]\n",
    "corpuses=[]\n",
    "corpuses.append(full_corpus)\n",
    "names.append('full_corpus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing unprocessed corpus : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best combo: seg_hash hash mention, set_not, elongment! (to første sammen, deretter de to andre gir 81.67% (+/- 0.58%) med 100) \n",
    "gjøre alt samtidig: 81.68% (+/- 0.52%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets using costumized tokenizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing to see that we get same result when all parameters are set to false: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(test_corpus)\n",
    "names.append('test_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_hash_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=True, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= True, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(seg_hash_corpus)\n",
    "names.append('seg_hash_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seg hash and hash mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_hash_m_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=True, hashtag_mention=True, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= True, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(seg_hash_m_corpus)\n",
    "names.append('seg_hash_m_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_s_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=True, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(pos_s_corpus)\n",
    "names.append('pos_s_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neg smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_s_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=True, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(neg_s_corpus)\n",
    "names.append('neg_s_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Smilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_s_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=True,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(other_s_corpus)\n",
    "names.append('other_s_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugs and kisses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_k_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=True,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(h_k_corpus)\n",
    "names.append('h_k_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hearts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=True,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(h_corpus)\n",
    "names.append('h_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=True, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(number_corpus)\n",
    "names.append('number_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers and numbermentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_m_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=True, number_mention=True, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(number_m_corpus)\n",
    "names.append('number_m_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=True,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(ex_corpus)\n",
    "names.append('ex_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set to not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=True, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(not_corpus)\n",
    "names.append('not_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spelling_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=True,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(spelling_corpus)\n",
    "names.append('spelling_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elongation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=True,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(en_corpus)\n",
    "names.append('en_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sign_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= False, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(sign_corpus)\n",
    "names.append('sign_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=2\n",
    "n_grams_corpus2=HL.creating_n_grams_cropus(n_grams,full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(n_grams_corpus2)\n",
    "names.append('n_grams_corpus2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=3\n",
    "n_grams_corpus3=HL.creating_n_grams_cropus(n_grams,full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(n_grams_corpus3)\n",
    "names.append('n_grams_corpus3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_grams=4\n",
    "n_grams_corpus4=HL.creating_n_grams_cropus(n_grams,full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(n_grams_corpus4)\n",
    "names.append('n_grams_corpus4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords= CL.get_dynamic_stopwords(full_corpus, MinDf=0.01, MaxDf=0.99,sublinearTF=True,useIDF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_corpus=CL.remove_stopwords(full_corpus, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(stopword_corpus)\n",
    "names.append('stopword_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for corpus in corpuses: \n",
    "    acc,std=GV.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets,epochs=5, n_folds=2)\n",
    "    accuracies.append(acc)\n",
    "    stds.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good=0\n",
    "bad=0\n",
    "good_corpuses=[]\n",
    "good_names=[]\n",
    "for i in range(len(accuracies)):\n",
    "    if i>0 and  accuracies[i]>accuracies[0]:\n",
    "        print('Method',names[i], 'improves, with acc=', accuracies[i],'and std:', stds[i],'\\n')\n",
    "        good+=1\n",
    "        good_corpuses.append(corpuses[i])\n",
    "        good_names.append(names[i])\n",
    "    else:\n",
    "        #print('Method',names[i], 'have acc=', accuracies[i],'and std:', stds[i],'\\n')\n",
    "        bad+=1\n",
    "    \n",
    "print(good/bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(good_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_has_combos=[]\n",
    "seg_has_combos_names=[]\n",
    "i=2\n",
    "seg_has_combo_accs=[]\n",
    "seg_has_combo_stds=[]\n",
    "for corpus in good_corpuses[2:]:\n",
    "    seg_hash_combo_corpus=TO.preprocess_corpus(corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=True, hashtag_mention=True, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= True, \n",
    "                      spelling=False,\n",
    "                      elongation=False,\n",
    "                      remove_signs=False\n",
    "                      )\n",
    "    seg_has_combos.append(seg_hash_combo_corpus)\n",
    "    seg_has_combos_names.append(good_names[i])\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "    acc,std=GV.classify_with_neural_networks(neural_nets, global_vectors, seg_hash_combo_corpus, total_training_tweets, nr_pos_tweets,epochs=5, n_folds=2)\n",
    "    seg_has_combo_accs.append(acc)\n",
    "    seg_has_combo_stds.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good=0\n",
    "bad=0\n",
    "good_corpuses2=[]\n",
    "good_names2=[]\n",
    "for i in range(len(seg_has_combo_accs)):\n",
    "    if i>0 and  seg_has_combo_accs[i]>seg_has_combo_accs[0]:\n",
    "        print('Method',seg_has_combos_names[i], 'improves, with acc=', seg_has_combo_accs[i],'and std:', seg_has_combo_stds[i],'\\n')\n",
    "        good+=1\n",
    "        good_corpuses2.append(seg_has_combos[i])\n",
    "        good_names2.append(seg_has_combos_names[i])\n",
    "    else:\n",
    "        #print('Method',names[i], 'have acc=', accuracies[i],'and std:', stds[i],'\\n')\n",
    "        bad+=1\n",
    "    \n",
    "print(good/bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_corpus=TO.preprocess_corpus(good_corpuses2[0],stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=False, hashtag_mention=False, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=False, \n",
    "                      segmentation_hash= True, \n",
    "                      spelling=False,\n",
    "                      elongation=True,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc,std=GV.classify_with_neural_networks(neural_nets, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets,epochs=5, n_folds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2_corpus=TO.preprocess_corpus(full_corpus,stemming=False,\n",
    "                      all_smilies=False, pos_smilies=False, neg_smilies=False, other_smilies=False,\n",
    "                      hugs_and_kisses=False,hearts=False,\n",
    "                      hashtag=True, hashtag_mention=True, \n",
    "                      numbers=False, number_mention=False, \n",
    "                      exclamation=False,\n",
    "                      set_to_not=True, \n",
    "                      segmentation_hash= True, \n",
    "                      spelling=False,\n",
    "                      elongation=True,\n",
    "                      remove_signs=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,std=GV.classify_with_neural_networks(neural_nets, global_vectors, final2_corpus, total_training_tweets, nr_pos_tweets,epochs=5, n_folds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teting n grams once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[2,3,4]\n",
    "\n",
    "n_accs=[]\n",
    "n_stds=[]\n",
    "n_names=[]\n",
    "n_ns=[]\n",
    "for i,corpus in enumerate(corpuses):\n",
    "    for n in ns:\n",
    "        ngram_corpus=HL.creating_n_grams_cropus(n,corpus)\n",
    "        acc,std=GV.classify_with_neural_networks(neural_nets, global_vectors, ngram_corpus, total_training_tweets, nr_pos_tweets,epochs=5, n_folds=2)\n",
    "        n_accs.append(acc)\n",
    "        n_stds.append(std)\n",
    "        n_names.append(names[i])\n",
    "        n_ns.append(n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Kaggle submission\n",
    "\n",
    "Som før for å lage en keggle! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_12_12.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.deep_HB, global_vectors, final2_corpus, total_training_tweets, nr_pos_tweets,kaggle_name, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
