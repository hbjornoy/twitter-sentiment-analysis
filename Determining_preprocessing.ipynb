{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining best combination of preprocessing techniques  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f3a474f1ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglove_module\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mGV\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mneural_nets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtokenizing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mTO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenizing_ekphrasis\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Thomas\\epfl\\Machine learning CS-433\\Project 2\\CD-433-Project-2\\tokenizing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#from nltk.tokenize import TweetTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "\n",
    "#MÅ GÅ NØYE IGJENNOM Å SJEKKE HVA SOM FAKTISK BRUKES\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "\n",
    "# internal imports\n",
    "import helpers as HL\n",
    "import cleaning as CL\n",
    "import glove_module as GV\n",
    "import neural_nets as NN\n",
    "import tokenizing as TO\n",
    "import tokenizing_ekphrasis as TE\n",
    "\n",
    "import maketextfile as MT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files used to create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "#BRUKER VI DET HER??\n",
    "DATA_FOLDER = os.path.join(\"glove.twitter.27B\") \n",
    "DATA_25DIM = DATA_FOLDER + \"/glove.twitter.27B.25d.txt\"\n",
    "DATA_50DIM = DATA_FOLDER + \"/glove.twitter.27B.50d.txt\"\n",
    "DATA_100DIM = DATA_FOLDER + \"/glove.twitter.27B.100d.txt\"\n",
    "DATA_200DIM = DATA_FOLDER + \"/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained GloVe with gensim\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word embeddings using the created gensim-.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pick one, the higher dimension, the better result and longer computational time. \n",
    "\n",
    "#global_vectors=GV.make_glove(\"data/gensim_global_vectors_25dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "global_vectors=GV.make_glove(\"gensim_global_vectors_200dim.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_vectors=GV.make_glove(\"global_vectors.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_nets=[NN.deep_HB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables to apply all preprocessing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing vectors:\n",
    "\n",
    "corpuses=[]\n",
    "corpuses.append(full_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining names of corpuses: \n",
    "names=['original_corpus','SH_corpus','SHM_corpus','H_corpus','HK_corpus','PS_corpus','NS__corpus','OS_corpus','N_corpus','NM_corpus','ST_corpus','SP_corpus','E_corpus','SN_corpus','RS_corpus','N-2_corpus','N-3_corpus','N-4_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining inputs to preprocessing function: \n",
    "inputs=[{'hashtag': True, 'segmentation_hash': True},\n",
    "        {'hashtag':True,'segmentation_hash': True,'hashtag_mention':True},\n",
    "        {'hearts':True},\n",
    "        {'hugs_and_kisses':True},\n",
    "        {'pos_smilies':True},\n",
    "        {'neg_smilies':True},\n",
    "        {'other_smilies':True},\n",
    "        {'numbers':True},\n",
    "        {'numbers':True,'number_mention':True},\n",
    "        {'stemming':True},\n",
    "        {'spelling':False},#Warning: When True, it takes forever. Recomended to always have as false \n",
    "        {'elongation':True},\n",
    "        {'set_to_not':True},\n",
    "        {'remove_signs':True}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying all preprocessing techniques to the original corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for input_ in inputs: \n",
    "        corpus=TO.preprocess_corpus(full_corpus, **input_)\n",
    "        corpuses.append(corpus)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[2,3,4]\n",
    "for n in ns: \n",
    "    corpus=HL.creating_n_grams_corpus(n,full_corpus)\n",
    "    corpuses.append(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1: Testing all preprocessing techniques: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "stds=[]\n",
    "\n",
    "for corpus in corpuses: \n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, corpus, total_training_tweets, nr_pos_tweets, epochs=5, n_folds=3)\n",
    "    accuracies.append(model_score[0][0])\n",
    "    stds.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to determine which preprocessing techniques that improved the accuracy, and keep them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses_1=[]\n",
    "names_1=[]\n",
    "stds_1=[]\n",
    "acc_1=[]\n",
    "print('The original corpus gave accuracy of: ',accuracies[0],'\\n')\n",
    "for i in range(1,len(accuracies)):\n",
    "    if accuracies[i]>=accuracies[0]:\n",
    "        corpuses_1.append(corpuses[i])\n",
    "        names_1.append(names[i])\n",
    "        stds_1.append(stds[i])\n",
    "        acc_1.append(accuracies[i])\n",
    "        print('IMPROVED:  ',names[i],', score:',accuracies[i],'std:',stds[i])\n",
    "    else:\n",
    "        print('Not better:',names[i],', score:',accuracies[i],'std:',stds[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(names_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2: apply best preprocessing technique to all other techniques that imporved the accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to determine which preprocessing technique-combinations that improved the accuracy, and keep them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 3: apply best preprocessing technique-combination to all other techniques that imporved the accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords= CL.get_dynamic_stopwords(full_corpus, MinDf=0.01, MaxDf=0.99,sublinearTF=True,useIDF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_corpus=CL.remove_stopwords(full_corpus, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpuses.append(stopword_corpus)\n",
    "names.append('stopword_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing \"best preprocessing\" with full dataset: \n",
    "\n",
    "Som før for å lage en keggle! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_corpus=TO.preprocess_corpus(full_corpus, segmentation_hash=True, hashtag=True, hashtag_mention=True, set_to_not=True,elongation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_score=GV.classify_with_neural_networks(neural_nets, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets, epochs=6, n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making kaggle submission: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_name=\"keggle_glove_13_12_full.csv\"\n",
    "#final_corpus=n_grams_corpus\n",
    "\n",
    "pred= GV.get_prediction(NN.deep_HB, global_vectors, final_corpus, total_training_tweets, nr_pos_tweets,kaggle_name, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
