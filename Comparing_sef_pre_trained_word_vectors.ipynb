{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Self- and Pre-trained Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import glove_solution as GS\n",
    "import glove_module as GV\n",
    "import helpers as HL\n",
    "import neural_nets as NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and creating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_pos = \"train_pos.txt\" \n",
    "training_set_neg = \"train_neg.txt\"\n",
    "training_set_pos_full = \"train_pos_full.txt\"\n",
    "training_set_neg_full = \"train_neg_full.txt\"\n",
    "test_set = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#When testing\n",
    "inputfiles=[training_set_pos,training_set_neg,test_set]\n",
    "\n",
    "#when using full data set:\n",
    "#inputfiles=[training_set_pos_full,training_set_neg_full,test_set]\n",
    "\n",
    "full_corpus, file_lengths=HL.create_corpus(inputfiles)\n",
    "nr_pos_tweets = file_lengths[0]\n",
    "nr_neg_tweets = file_lengths[1]\n",
    "total_training_tweets =file_lengths [0]+file_lengths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the neural net used to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_nets=[NN.basic_model_adam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating global vectors from the co-occurence matrix \n",
    "Only have to be done once. Assumes that the cooc_full.pkl file is created and availiable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GS.glove('cooc_full.pkl',100, 'embeddings_full50')\n",
    "GS.glove('cooc_full.pkl',100, 'embeddings_full100')\n",
    "GS.glove('cooc_full.pkl',100, 'embeddings_full200')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datafiles of the embeddings: \n",
    "Only have to be done once, assumes that the embedding files and vocab_full.pkl file are availiable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MT.make_file('embeddings_full50.npy','vocab_full.pkl', 'global_vectors_full_unprepro50.txt')\n",
    "MT.make_file('embeddings_full100.npy','vocab_full.pkl', 'global_vectors_full_unprepro100.txt')\n",
    "MT.make_file('embeddings_full200.npy','vocab_full.pkl', 'global_vectors_full_unprepro200.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make global vectors of all embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_vectors_pre_50=GV.make_glove(\"gensim_global_vectors_50dim.txt\")\n",
    "#global_vectors_pre_100=GV.make_glove(\"gensim_global_vectors_100dim.txt\")\n",
    "#global_vectors_pre_200=GV.make_glove(\"gensim_global_vectors_200dim.txt\")\n",
    "\n",
    "#global_vectors_self_50=GV.make_glove('global_vectors_full_unprepro50.txt')\n",
    "#global_vectors_self_100=GV.make_glove('global_vectors_full_unprepro100.txt')\n",
    "#global_vectors_self_200=GV.make_glove('global_vectors_full_unprepro200.txt')\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_vectors=[global_vectors_pre_50]\n",
    "#global_vectors=[global_vectors_pre_50,global_vectors_pre_100,global_vectors_pre_200,global_vectors_self_50, global_vectors_self_100, global_vectors_self_200 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=['pre50','pre100','pre200','self50','self100','self200']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do cross validation using all global vectors, to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets processed: 0  of total number of tweets: 200000\n",
      "tweets processed: 50000  of total number of tweets: 200000\n",
      "tweets processed: 100000  of total number of tweets: 200000\n",
      "tweets processed: 150000  of total number of tweets: 200000\n",
      "Epoch 1/20\n",
      "133332/133332 [==============================] - 1s 11us/step - loss: 0.5292 - acc: 0.7187\n",
      "Epoch 2/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4752 - acc: 0.7599\n",
      "Epoch 3/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4634 - acc: 0.7673\n",
      "Epoch 4/20\n",
      "133332/133332 [==============================] - 1s 6us/step - loss: 0.4566 - acc: 0.7716\n",
      "Epoch 5/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4520 - acc: 0.7751\n",
      "Epoch 6/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4481 - acc: 0.7777\n",
      "Epoch 7/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4446 - acc: 0.7798\n",
      "Epoch 8/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4418 - acc: 0.7816\n",
      "Epoch 9/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4396 - acc: 0.7830\n",
      "Epoch 10/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4375 - acc: 0.7840\n",
      "Epoch 11/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4356 - acc: 0.7850\n",
      "Epoch 12/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4339 - acc: 0.7862\n",
      "Epoch 13/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4324 - acc: 0.7870\n",
      "Epoch 14/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4311 - acc: 0.7879\n",
      "Epoch 15/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4299 - acc: 0.7892\n",
      "Epoch 16/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4285 - acc: 0.7900\n",
      "Epoch 17/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4274 - acc: 0.7900\n",
      "Epoch 18/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4264 - acc: 0.7912\n",
      "Epoch 19/20\n",
      "133332/133332 [==============================] - 1s 4us/step - loss: 0.4253 - acc: 0.7912\n",
      "Epoch 20/20\n",
      "133332/133332 [==============================] - 1s 5us/step - loss: 0.4248 - acc: 0.7914\n",
      "Epoch 1/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4310 - acc: 0.7881\n",
      "Epoch 2/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4288 - acc: 0.7898\n",
      "Epoch 3/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4269 - acc: 0.7904\n",
      "Epoch 4/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4258 - acc: 0.7911\n",
      "Epoch 5/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4249 - acc: 0.7923\n",
      "Epoch 6/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4238 - acc: 0.7924\n",
      "Epoch 7/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4229 - acc: 0.7939\n",
      "Epoch 8/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4222 - acc: 0.7941\n",
      "Epoch 9/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4213 - acc: 0.7949\n",
      "Epoch 10/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4204 - acc: 0.7948\n",
      "Epoch 11/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4198 - acc: 0.7956\n",
      "Epoch 12/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4193 - acc: 0.7963\n",
      "Epoch 13/20\n",
      "133334/133334 [==============================] - 1s 6us/step - loss: 0.4183 - acc: 0.7961\n",
      "Epoch 14/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4176 - acc: 0.7970\n",
      "Epoch 15/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4170 - acc: 0.7974\n",
      "Epoch 16/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4166 - acc: 0.7972\n",
      "Epoch 17/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4162 - acc: 0.7970\n",
      "Epoch 18/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4154 - acc: 0.7981\n",
      "Epoch 19/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4149 - acc: 0.7983\n",
      "Epoch 20/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4148 - acc: 0.7987\n",
      "Epoch 1/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4237 - acc: 0.7926\n",
      "Epoch 2/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4216 - acc: 0.7932\n",
      "Epoch 3/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4202 - acc: 0.7941\n",
      "Epoch 4/20\n",
      "133334/133334 [==============================] - 1s 4us/step - loss: 0.4196 - acc: 0.7949\n",
      "Epoch 5/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4186 - acc: 0.7954\n",
      "Epoch 6/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4176 - acc: 0.7965\n",
      "Epoch 7/20\n",
      "133334/133334 [==============================] - 1s 6us/step - loss: 0.4173 - acc: 0.7963\n",
      "Epoch 8/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4164 - acc: 0.7972\n",
      "Epoch 9/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4162 - acc: 0.7962\n",
      "Epoch 10/20\n",
      "133334/133334 [==============================] - 1s 6us/step - loss: 0.4151 - acc: 0.7983\n",
      "Epoch 11/20\n",
      "133334/133334 [==============================] - 1s 6us/step - loss: 0.4149 - acc: 0.7975\n",
      "Epoch 12/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4139 - acc: 0.7985\n",
      "Epoch 13/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4137 - acc: 0.7987\n",
      "Epoch 14/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4136 - acc: 0.7986\n",
      "Epoch 15/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4130 - acc: 0.7988\n",
      "Epoch 16/20\n",
      "133334/133334 [==============================] - 1s 6us/step - loss: 0.4124 - acc: 0.7998\n",
      "Epoch 17/20\n",
      "133334/133334 [==============================] - 1s 6us/step - loss: 0.4119 - acc: 0.7996\n",
      "Epoch 18/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4115 - acc: 0.8007\n",
      "Epoch 19/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4115 - acc: 0.8002\n",
      "Epoch 20/20\n",
      "133334/133334 [==============================] - 1s 5us/step - loss: 0.4106 - acc: 0.8005\n",
      "Model:  basic_model_adam\n",
      "0.61% (+/- 0.18%)\n",
      "Negative sentiment: 75.72%  Positive sentiment: 81.92%\n",
      "Percentage of positive classifications (should be 50%ish): 53.1019954257\n",
      "Time taken:  0.8042745272318522 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stds=[]\n",
    "accs=[]\n",
    "\n",
    "for global_vector in global_vectors: \n",
    "    model_score=GV.classify_with_neural_networks(neural_nets, global_vector, full_corpus, total_training_tweets, nr_pos_tweets, epochs=20, n_folds=3)\n",
    "    accs.append(model_score[0][0])\n",
    "    stds.append(model_score[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in rang(len(accs)):\n",
    "    print('Global vectors: ', names(i), 'Accuracy: ', accs[i], '+- :', stds[i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
